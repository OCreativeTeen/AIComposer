import json
import base64
import requests
from PIL import Image
from io import BytesIO
from rembg import remove
import os
import config
from pathlib import Path
import threading
# VIDEO_WIDTH and VIDEO_HEIGHT are now obtained from project config via ffmpeg_processor
from typing import Dict, Any
import config_prompt
from . import llm_api
from .file_util import get_file_path, build_scene_media_prefix, safe_copy_overwrite
from project_manager import refresh_scene_media
import subprocess


GEN_CONFIG = {
        #"Story":{"url": "http://10.0.0.179:8188", "model": "banana", "seed": 1234567890, "steps": 4, "cfg": 1.0, "workflow":"\\\\10.0.0.179\\wan22\\ComfyUI\\user\\default\\workflows\\nano_banana.json"},
        #"Story":{"url": "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image-preview:generateContent", "model": "banana", "seed": 1234567890, "steps": 4, "cfg": 1.0},
        "Story":{"url": "http://10.0.0.x:8188",                        "face": "flux","seed": 1234567890, "steps": 4, "cfg": 1.0, "workflow":"\\\\10.0.0.179\\wan22\\ComfyUI\\user\\default\\workflows\\flux_workflow.json"},
        "Host": {"url": "http://10.0.0.x:8188",                        "face": "flux","seed": 1234567890, "steps": 4, "cfg": 1.0, "workflow":"\\\\10.0.0.179\\wan22\\ComfyUI\\user\\default\\workflows\\flux_workflow_figure.json"},
        "SD":   {"url": "http://10.0.0.x:7860/sdapi/v1/txt2img",       "face": "sd",  "seed": 1234567890, "steps": 30,"cfg": 7.0},

        "I2V":    {"url": "http://10.0.0.231:9001/wan/image2video",    "face": "66", "seed": 1234567890, "steps": 4, "cfg": 0.5, "motion_frame":16, "frame_rate":15, "max_frames":91,  "image_width":704, "image_height":400},
        "2I2V":   {"url": "http://10.0.0.231:9001/wan/imagesss2video", "face": "66", "seed": 1234567890, "steps": 4, "cfg": 0.5, "motion_frame":4,  "frame_rate":15, "max_frames":91,  "image_width":704, "image_height":400},

        "HS2V":   {"url": "http://10.0.0.222:9001/wan/infinite_s2v",   "face": "66", "seed": 1234567890, "steps": 4, "cfg": 0.5, "motion_frame":5,  "frame_rate":15, "max_frames":196, "image_width":720, "image_height":405},
        "S2V":    {"url": "http://10.0.0.222:9001/wan/infinite_s2v",   "face": "66", "seed": 1234567890, "steps": 4, "cfg": 0.5, "motion_frame":5,  "frame_rate":15, "max_frames":226, "image_width":672, "image_height":378},
        "FS2V":   {"url": "http://10.0.0.222:9001/wan/infinite_s2v",   "face": "66", "seed": 1234567890, "steps": 4, "cfg": 0.5, "motion_frame":4,  "frame_rate":15, "max_frames":316, "image_width":570, "image_height":321},
        "WS2V":   {"url": "http://10.0.0.222:9001/wan/infinite_s2v",   "face": "66", "seed": 1234567890, "steps": 4, "cfg": 0.5, "motion_frame":5,  "frame_rate":15, "max_frames":91,  "image_width":468, "image_height":480},
        #"FS2V": {"url": "http://10.0.0.222:9001/wan/infinite_s2v",   "model": "wan", "seed": 1234567890, "steps": 4, "cfg": 1.0, "motion_frame":5,  "frame_rate":15, "max_frames":121, "image_width":683, "image_height":384},
        "AI2V":   {"url": "http://10.0.0.231:9001/wan/action_transfer","face": "66", "seed": 1234567890, "steps": 4, "cfg": 0.5, "motion_frame":5,  "frame_rate":15, "max_frames":121, "image_width":853, "image_height":480},
        "INTP":   {"url": "http://10.0.0.235:9001/interpolate",        "face": "66", "seed": 1234567890, "steps": 4, "cfg": 0.5, "motion_frame":5,  "frame_rate":60, "max_frames":121, "image_width":853, "image_height":480}
}


class SDProcessor:


    """
    å›¾åƒå¤„ç†æµæ°´çº¿ç±»ï¼Œæä¾›å›¾åƒå¡é€šåŒ–ã€èƒŒæ™¯ç§»é™¤ç­‰åŠŸèƒ½
    """
    def __init__(self, workflow):
        self.prompt_url = ""
        self.prompt_model = ""


        self.workflow = workflow
        # Get video dimensions from ffmpeg_processor (will be set after workflow initialization)
        # For now, use default values - they will be updated when workflow is created
        
        self.llm_api = llm_api.LLMApi()

        # Set default image dimensions to match video dimensions
        self.wan_vidoe_count = 0
        self.infinite_vidoe_count = 0


    def resize_image(self, image, width, height):
        """è°ƒæ•´å›¾åƒå¤§å°å¹¶å¤„ç†EXIFæ–¹å‘"""
        # é¦–å…ˆå°†base64å­—ç¬¦ä¸²è§£ç ä¸ºäºŒè¿›åˆ¶æ•°æ®
        if isinstance(image, str):
            # å¦‚æžœæ˜¯base64ç¼–ç çš„å­—ç¬¦ä¸²ï¼Œå…ˆè§£ç 
            image_data = base64.b64decode(image)
        else:
            image_data = image
            
        # æ‰“å¼€å›¾åƒ
        img = Image.open(BytesIO(image_data))
        
        try:
            exif = img._getexif()
            if exif:
                # EXIFæ–¹å‘æ ‡ç­¾
                orientation_tag = 274  # 0x0112
                if orientation_tag in exif:
                    orientation = exif[orientation_tag]
                    # æ ¹æ®ä¸åŒçš„æ–¹å‘å€¼æ—‹è½¬å›¾åƒ
                    if orientation == 3:
                        img = img.rotate(180, expand=True)
                    elif orientation == 6:
                        img = img.rotate(270, expand=True)
                    elif orientation == 8:
                        img = img.rotate(90, expand=True)
        except (AttributeError, KeyError, IndexError):
            pass
        
        # è½¬æ¢ä¸ºRGBAå¹¶è°ƒæ•´å¤§å°
        img = img.convert("RGBA")
        resized_img = img.resize(
            (int(width), int(height)),
            Image.LANCZOS
        )
        return resized_img


    def cartoonizeImage(self, image_b64, image_dimen, description, denoising):
        """å°†å›¾åƒå¡é€šåŒ–å¤„ç†"""
        if image_dimen[0] > image_dimen[1]:
            width = 512
            height = 512*image_dimen[1]/image_dimen[0]
        else:
            width = 512*image_dimen[0]/image_dimen[1]
            height = 512

        positive = "pixar art style \n\n---------------\n\n" + description
        negative = "worst quality, low quality, normal quality, lowres, low details, oversaturated, undersaturated, overexposed, underexposed, grayscale, bw, bad photo, bad photography, bad art:1.4), (watermark, signature, text font, username, error, logo, words, letters, digits, autograph, trademark, name)"
        payload = {
            "init_images": [image_b64],
            "prompt": positive,
            "negative_prompt": negative,
            "steps": 30,
            "denoising_strength": denoising,  # 0.3â€“0.7 is typical
            "cfg_scale": 7.0,
            "width": width,
            "height": height,
            "seed": 1234567890,
            "sampler_name": "Euler a"  #"DPM++ 2M" "DPM++ 2M SDE Heun" 
            #"sd_model_checkpoint": "cartoon_model.ckpt",  # Optional if already loaded
        }

        # Generate curl command for debugging
        self._save_curl_command(GEN_CONFIG['SD']['url'], payload, "img2img")
        
        # Send request to AUTOMATIC1111 API
        response = requests.post(GEN_CONFIG['SD']['url'], json=payload, timeout=60)

        # Get and decode result
        r = response.json()

        resized_img = self.resize_image(r['images'][0], image_dimen[0], image_dimen[1])

        buffer = BytesIO()
        resized_img.save(buffer, format="PNG")
        return buffer.getvalue()


    def text2Image_sd(self, positive, negative, url, cfg, seed, steps, width, height):
        print(f"ðŸ–¼ï¸ å‡†å¤‡å‘é€åˆ°IMAGEæœåŠ¡å™¨{url}çš„å›¾åƒå°ºå¯¸: {width}x{height}")
        
        # å¦‚æžœ prompt æ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸º JSON å­—ç¬¦ä¸²
        if isinstance(positive, dict):
            import json
            positive = json.dumps(positive, ensure_ascii=False)
        if isinstance(negative, dict):
            import json
            negative = json.dumps(negative, ensure_ascii=False)
        
        payload = {
            "prompt": positive,
            "negative_prompt": negative,
            "cfg_scale": cfg,
            "width": width,
            "height": height,
            "steps": steps,
            "seed": seed,
            "sampler_name": "Euler a"
        }
        # Generate curl command for debugging
        self._save_curl_command(url, payload, "txt2img")

        try:
            response = requests.post(url, json=payload, timeout=90)
            # Get and decode result
            r = response.json()
            image_b64 = r['images'][0]
            # è§£ç base64å›¾åƒæ•°æ®
            print(f"ðŸ” å¼€å§‹è§£ç base64å›¾åƒæ•°æ®ï¼Œé•¿åº¦: {len(image_b64)}")
            return base64.b64decode(image_b64)
        except Exception as e:
            print(f"âŒ å›¾åƒç¼©æ”¾å¤±è´¥: {str(e)}")
            return None

    
    def text2Image_banana(self, url, workflow, positive, negative, image_list=None, width=None, height=None, cfg=None, seed=None, steps=None):
        """ä½¿ç”¨ Banana æ¨¡åž‹ç”Ÿæˆæ–‡æœ¬åˆ°å›¾åƒ
        Returns:
            bytes: ç”Ÿæˆçš„å›¾åƒæ•°æ®ï¼Œå¤±è´¥æ—¶è¿”å›ž None
        """
        pass


    def png_to_base64(self, image_path):
        with open(image_path, "rb") as image_file:
            encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
            return f"data:image/png;base64,{encoded_string}"


    def remove_background(self, image_data):
        """åŽ»é™¤å›¾åƒèƒŒæ™¯"""
        removed = remove(image_data)
        # reduce the size to get ride of empty top bottom left right 
        
        # è½¬æ¢ä¸ºPILå›¾åƒ
        img = Image.open(BytesIO(removed))
        
        # èŽ·å–éžé€æ˜ŽåŒºåŸŸçš„è¾¹ç•Œ
        # åˆ›å»ºå›¾åƒçš„alphaé€šé“
        alpha = img.getchannel('A')
        
        # æ‰¾åˆ°éžé€æ˜Žåƒç´ çš„è¾¹ç•Œ
        bbox = alpha.getbbox()
        
        if bbox:
            # è£å‰ªå›¾åƒï¼Œåªä¿ç•™éžé€æ˜Žéƒ¨åˆ†
            cropped_img = img.crop(bbox)
            
            # å°†è£å‰ªåŽçš„å›¾åƒä¿å­˜åˆ°å†…å­˜ä¸­
            output = BytesIO()
            cropped_img.save(output, format='PNG')
            output.seek(0)
            
            return output.getvalue()
        
        # å¦‚æžœæ²¡æœ‰æ‰¾åˆ°éžé€æ˜ŽåŒºåŸŸï¼Œåˆ™è¿”å›žåŽŸå§‹å›¾åƒ
        return removed


    def save_image(self, image, filename, format="WEBP"):
        if os.path.exists(filename):
            os.remove(filename)
        # if image instanceof Image, then save it
        if isinstance(image, Image.Image):
            image.save(filename, format=format)
            return filename
        
        img = Image.open(BytesIO(image))
        img.save(filename, format=format)
        return filename


    def read_image(self, image_path):
        with open(image_path, "rb") as input_file:
            return input_file.read()


    def add_image_to_image(self, top_image, background_image, position):
        try:
            from PIL import Image
            from io import BytesIO
            
            # Load both images
            bg_img = Image.open(BytesIO(background_image))
            top_img = Image.open(BytesIO(top_image))
            
            # Get background image dimensions
            bg_width, bg_height = bg_img.size
            
            # Calculate target height for top image (3/4 of background height)
            target_height = int(bg_height * 5 / 6)
            
            # Calculate target width to maintain aspect ratio
            top_original_width, top_original_height = top_img.size
            aspect_ratio = top_original_width / top_original_height
            target_width = int(target_height * aspect_ratio)
            
            # Convert top image to binary data for resize_image method
            top_buffer = BytesIO()
            top_img.save(top_buffer, format='PNG')
            top_buffer.seek(0)
            top_binary = top_buffer.getvalue()
            
            # Resize top image to 3/4 of background height while keeping aspect ratio
            top_img = self.resize_image(top_binary, target_width, target_height)
            
            # Convert background to RGBA if it's not already
            if bg_img.mode != 'RGBA':
                bg_img = bg_img.convert('RGBA')
            
            # Convert top image to RGBA if it's not already
            if top_img.mode != 'RGBA':
                top_img = top_img.convert('RGBA')
            
            # Get dimensions (top_img has already been resized)
            top_width, top_height = top_img.size
            
            # Calculate position
            if position.lower() == "right":
                x = bg_width - top_width - 20  # 20px margin from right edge
                y = (bg_height - top_height)  # Center vertically
            elif position.lower() == "left":
                x = 20  # 20px margin from left edge
                y = (bg_height - top_height)  # Center vertically
            elif position.lower() == "center":
                x = (bg_width - top_width) // 2  # Center horizontally
                y = (bg_height - top_height)  # Center vertically
            else:
                # Default to center if position is not recognized
                x = (bg_width - top_width) // 2
                y = (bg_height - top_height) // 2
            
            # Ensure the top image doesn't go outside the background bounds
            x = max(0, min(x, bg_width - top_width))
            y = max(0, min(y, bg_height - top_height))
            
            # Create a copy of the background image
            result_img = bg_img.copy()
            
            # Paste the top image onto the background
            result_img.paste(top_img, (x, y), top_img)
            
            # Convert to binary data
            output = BytesIO()
            result_img.save(output, format='PNG')
            output.seek(0)
            
            return output.getvalue()
            
        except Exception as e:
            print(f"âŒ Error adding image to image: {str(e)}")
            # Return the background image if there's an error
            return background_image


    def _save_curl_command(self, url, payload, api_type):
        """ç”Ÿæˆå¹¶ä¿å­˜curlå‘½ä»¤ç”¨äºŽè°ƒè¯•"""
        try:
            debug_dir = os.path.join(config.get_temp_path(self.workflow.pid), "debug_curls")
            os.makedirs(debug_dir, exist_ok=True)
            
            # ç”Ÿæˆæ—¶é—´æˆ³ç”¨äºŽæ–‡ä»¶å
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]  # æ¯«ç§’ç²¾åº¦
            
            # ç”Ÿæˆcurlå‘½ä»¤ - æ ¹æ®APIç±»åž‹é€‰æ‹©æ ¼å¼
            if api_type == "txt2img" and isinstance(payload, dict) and "prompt" in payload:
                # flux APIä½¿ç”¨multipart/form-dataæ ¼å¼
                curl_command = f'curl --location "{url}" \\\n'
                for key, value in payload.items():
                    curl_command += f'  --form \'{key}="{value}"\' \\\n'
                curl_command = curl_command.rstrip(' \\\n')  # ç§»é™¤æœ€åŽçš„åæ–œæ 
            else:
                # å…¶ä»–APIä½¿ç”¨JSONæ ¼å¼
                curl_command = f'curl -X POST "{url}" \\\n'
                curl_command += '  -H "Content-Type: application/json" \\\n'
                curl_command += f'  -d \'{json.dumps(payload, indent=2, ensure_ascii=False)}\''
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            filename = f"{api_type}_{timestamp}.txt"
            filepath = os.path.join(debug_dir, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(f"# Stable Diffusion API Debug - {api_type.upper()}\n")
                f.write(f"# Generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"# URL: {url}\n\n")
                f.write("# CURL Command:\n")
                f.write(curl_command)
                if api_type == "txt2img" and isinstance(payload, dict) and "prompt" in payload:
                    f.write("\n\n# Form Data:\n")
                    for key, value in payload.items():
                        f.write(f"{key}: {value}\n")
                else:
                    f.write("\n\n# Payload JSON (formatted):\n")
                    f.write(json.dumps(payload, indent=2, ensure_ascii=False))
                
            print(f"ðŸ”§ è°ƒè¯•curlå‘½ä»¤å·²ä¿å­˜åˆ°: {filepath}")
            
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜curlå‘½ä»¤æ—¶å‡ºé”™: {str(e)}")


    def open_image_as_base64(self, image_path):
        import base64
        from io import BytesIO
        
        # æ‰“å¼€å¹¶è½¬æ¢å›¾ç‰‡ä¸º base64
        image = Image.open(image_path)
        
        # å¦‚æžœå›¾ç‰‡å¤ªå¤§ï¼Œå…ˆè°ƒæ•´å¤§å°ä»¥èŠ‚çœ token
        max_size = 1024
        if max(image.size) > max_size:
            image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)
        
        # å¤„ç† RGBA æ¨¡å¼ï¼ˆPNG é€æ˜Žåº¦ï¼‰ï¼šåˆæˆåˆ°ç™½è‰²èƒŒæ™¯
        if image.mode == 'RGBA':
            background = Image.new('RGB', image.size, (255, 255, 255))
            background.paste(image, mask=image.split()[-1])  # ä½¿ç”¨ alpha é€šé“ä½œä¸º mask
            image = background
        elif image.mode not in ('RGB',):
            # å…¶ä»–æ¨¡å¼è½¬æ¢ä¸º RGB
            image = image.convert('RGB')
        
        # è½¬æ¢ä¸º base64ï¼ˆJPEG æ ¼å¼ï¼‰
        buffered = BytesIO()
        image.save(buffered, format="JPEG", quality=85)
        img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
        return img_base64


    def describe_image(self, img_input):
        """
        ä½¿ç”¨ OpenAI/Gemini API æè¿°å›¾ç‰‡
        
        Args:
            img_input: å¯ä»¥æ˜¯æ–‡ä»¶è·¯å¾„ï¼ˆstrï¼‰æˆ– base64 å­—ç¬¦ä¸²ï¼ˆstrï¼‰
        
        Returns:
            dict: å›¾ç‰‡æè¿°æ•°æ®ï¼Œå¤±è´¥æ—¶è¿”å›ž None
        """
        try:
            # å¦‚æžœè¾“å…¥æ˜¯æ–‡ä»¶è·¯å¾„ï¼Œå…ˆè½¬æ¢ä¸º base64
            if isinstance(img_input, str):
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡ä»¶è·¯å¾„ï¼ˆæ–‡ä»¶å­˜åœ¨ï¼Œæˆ–åŒ…å«è·¯å¾„åˆ†éš”ç¬¦ï¼‰
                if os.path.exists(img_input):
                    # ç¡®è®¤æ˜¯æ–‡ä»¶è·¯å¾„ï¼Œè½¬æ¢ä¸º base64
                    img_base64 = self.open_image_as_base64(img_input)
                elif '/' in img_input or '\\' in img_input:
                    # åŒ…å«è·¯å¾„åˆ†éš”ç¬¦ä½†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä½œä¸ºæ–‡ä»¶è·¯å¾„å¤„ç†
                    # å¦‚æžœå¤±è´¥ä¼šåœ¨ open_image_as_base64 ä¸­æŠ›å‡ºå¼‚å¸¸
                    img_base64 = self.open_image_as_base64(img_input)
                else:
                    # å‡è®¾æ˜¯ base64 å­—ç¬¦ä¸²
                    img_base64 = img_input
            else:
                raise ValueError("img_input å¿…é¡»æ˜¯æ–‡ä»¶è·¯å¾„ï¼ˆstrï¼‰æˆ– base64 å­—ç¬¦ä¸²ï¼ˆstrï¼‰")
            
            # æ¸…ç† base64 å­—ç¬¦ä¸²ï¼ˆç§»é™¤å¯èƒ½çš„æ¢è¡Œç¬¦ã€ç©ºæ ¼å’Œ data URI å‰ç¼€ï¼‰
            img_base64 = img_base64.strip()
            # å¦‚æžœåŒ…å« data URI å‰ç¼€ï¼Œç§»é™¤å®ƒ
            if img_base64.startswith('data:image'):
                img_base64 = img_base64.split(',', 1)[1] if ',' in img_base64 else img_base64
            # ç§»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦
            img_base64 = img_base64.replace('\n', '').replace('\r', '').replace(' ', '').replace('\t', '')
            
            system_prompt = config_prompt.IMAGE_DESCRIPTION_SYSTEM_PROMPT
            user_prompt = [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{img_base64}"
                    }
                }
            ]
            return self.llm_api.generate_json(system_prompt, user_prompt, "describe_image_response.txt", False)

        except Exception as e:
            print(f"âŒ å›¾ç‰‡æè¿°å¤±è´¥: {str(e)}")
            return None


    def two_image_to_video(self, prompt, file_prefix, first_frame, last_frame, sound_path) :
        server_config = GEN_CONFIG["2I2V"]
        num_frames = server_config["frame_rate"] * self.workflow.ffmpeg_audio_processor.get_duration(sound_path) + 1
        if num_frames > server_config["max_frames"]:
            num_frames = server_config["max_frames"]

        # å¦‚æžœ prompt æ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸º JSON å­—ç¬¦ä¸²
        if isinstance(prompt, dict):
            import json
            prompt = json.dumps(prompt, ensure_ascii=False)

        data = {
            'prompt': prompt,
            "negative_prompt": "",
            'filename_prefix': file_prefix,

            'image_width': server_config["image_width"] if self.workflow.ffmpeg_processor.width > self.workflow.ffmpeg_processor.height else server_config["image_height"],
            'image_height': server_config["image_height"] if self.workflow.ffmpeg_processor.width > self.workflow.ffmpeg_processor.height else server_config["image_width"],

            "cfg_scale": server_config["cfg"],
            "steps": server_config["steps"],
            "seed": server_config["seed"],

            'motion_frame': server_config["motion_frame"],
            'frame_rate': server_config["frame_rate"],
            'num_frames': int(num_frames)
        }

        files = {
            'first_frame': first_frame,
            'last_frame': last_frame
        }
        self.post_multipart(
            data=data,
            files=files,
            full_url=server_config["url"]
        )
        self.wan_vidoe_count += 1


    def action_transfer_video(self, prompt, file_prefix, image_path, sound_path, action_path) :
        duration = self.workflow.ffmpeg_audio_processor.get_duration(sound_path)
        if duration <= 0.0:
            print(f"ðŸ”´ éŸ³é¢‘æ—¶é•¿ä¸º0")
            return

        if isinstance(prompt, dict):
            import json
            prompt = json.dumps(prompt, ensure_ascii=False)

        server_config = GEN_CONFIG["AI2V"]
        fps = server_config["frame_rate"]

        action_path = self.workflow.ffmpeg_processor.refps_video(action_path, fps)

        num_frames = int(duration * fps)
        max_frames = server_config["max_frames"]
        if num_frames > max_frames:
            num_frames = max_frames

        data = {
            'prompt': prompt,
            "negative_prompt": "",
            'filename_prefix': file_prefix,

            'image_width': server_config["image_width"] if self.workflow.ffmpeg_processor.width > self.workflow.ffmpeg_processor.height else server_config["image_height"],
            'image_height': server_config["image_height"] if self.workflow.ffmpeg_processor.width > self.workflow.ffmpeg_processor.height else server_config["image_width"],

            "cfg_scale": server_config["cfg"],
            "steps": server_config["steps"],
            "seed": server_config["seed"],

            'motion_frame': server_config["motion_frame"],
            'frame_rate': fps,
            'num_frames': int(num_frames)
        }

        files = {
            'image': image_path,
            'action': action_path
        }
        self.post_multipart(
            data=data,
            files=files,
            full_url=server_config["url"]
        )
        self.infinite_vidoe_count += 1


    def sound_to_video(self, prompt, file_prefix, image_path, sound_path, next_sound_path, animate_mode, silence=False) :
        # å¦‚æžœ prompt æ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸º JSON å­—ç¬¦ä¸²
        if isinstance(prompt, dict):
            import json
            prompt = json.dumps(prompt, ensure_ascii=False)

        server_config = GEN_CONFIG[animate_mode]

        if not silence:
            fps = server_config["frame_rate"]
            max_frames = server_config["max_frames"]
            duration = self.workflow.ffmpeg_audio_processor.get_duration(sound_path)

            if next_sound_path:
                batch_sec = (max_frames-3.0)/fps
                batches = int(duration/batch_sec)
                remind_sec = duration - batches*batch_sec
                add_sec = batch_sec - remind_sec - 0.15
                if add_sec > 0.0:
                    next_sound_path = self.workflow.ffmpeg_audio_processor.audio_cut_fade(next_sound_path, 0, add_sec)
                    sound_path = self.workflow.ffmpeg_audio_processor.concat_audios([sound_path, next_sound_path])
                    duration = self.workflow.ffmpeg_audio_processor.get_duration(sound_path)
            # if int(duration*fps)+1 <= max_frames:
            #    max_frames = int(duration*fps)+1
            #    if max_frames % fps == 0:
            #        max_frames += 1
        else:    
            fps = server_config["frame_rate"]//2
            max_frames = int(server_config["max_frames"]//2)
            if max_frames % 2 == 0:
                max_frames += 1
            duration = self.workflow.ffmpeg_audio_processor.get_duration(sound_path)
            sound_path = self.workflow.ffmpeg_audio_processor.make_silence(duration)

        #num_frames = int(duration * fps)
        #if num_frames % 2 == 0:
        #    num_frames += 1
        #if num_frames > max_frames:
        #    num_frames = max_frames

        data = {
            'prompt': prompt,
            "negative_prompt": "",
            'filename_prefix': file_prefix,

            'image_width': server_config["image_width"] if self.workflow.ffmpeg_processor.width > self.workflow.ffmpeg_processor.height else server_config["image_height"],
            'image_height': server_config["image_height"] if self.workflow.ffmpeg_processor.width > self.workflow.ffmpeg_processor.height else server_config["image_width"],

            "cfg_scale": server_config["cfg"],
            "steps": server_config["steps"],
            "seed": server_config["seed"],

            'motion_frame': server_config["motion_frame"],
            'frame_rate': fps,
            'num_frames': int(max_frames)
        }

        files = {
            'image': image_path,
            'sound': sound_path
        }
        self.post_multipart(
            data=data,
            files=files,
            full_url=server_config["url"]
        )
        self.infinite_vidoe_count += 1


    def image_to_video(self, prompt, file_prefix, image_path, sound_path) :
        server_config = GEN_CONFIG["I2V"]
        num_frames = server_config["frame_rate"] * self.workflow.ffmpeg_audio_processor.get_duration(sound_path) + 1
        if num_frames > server_config["max_frames"]:
            num_frames = server_config["max_frames"]
        
        # å¦‚æžœ prompt æ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸º JSON å­—ç¬¦ä¸²
        if isinstance(prompt, dict):
            import json
            prompt = json.dumps(prompt, ensure_ascii=False)
        
        data = {
            'prompt': prompt,
            "negative_prompt": "",
            'filename_prefix': file_prefix,

            'image_width': server_config["image_width"] if self.workflow.ffmpeg_processor.width > self.workflow.ffmpeg_processor.height else server_config["image_height"],
            'image_height': server_config["image_height"] if self.workflow.ffmpeg_processor.width > self.workflow.ffmpeg_processor.height else server_config["image_width"],
            
            "cfg_scale": server_config["cfg"],
            "steps": server_config["steps"],
            "seed": server_config["seed"],
            
            'motion_frame': server_config["motion_frame"],
            'frame_rate': server_config["frame_rate"],
            'num_frames': int(num_frames)
        }
        files = {
            'first_frame': image_path
        }

        self.post_multipart(
            data=data,
            files=files,
            full_url=server_config["url"]
        )
        self.wan_vidoe_count += 1


    def enhance_clip(self, pid, scene, track, level:str):
        status = scene.get(track + "_status", "")
        print(f"enhance_clip {track} status: {status}")
        if status == "ENH2":
            return

        fps = scene.get(track + "_fps", 15)
        if fps > 24:
            fps = 15

        animate_mode = scene.get(track + "_animation", "")

        if animate_mode in config_prompt.ANIMATE_WS2V:
            left_input = get_file_path(scene, track + "_left")
            right_input = get_file_path(scene, track + "_right")
            if left_input and right_input:
                left_input = self.workflow.ffmpeg_processor.refps_video(left_input, fps)
                right_input = self.workflow.ffmpeg_processor.refps_video(right_input, fps)
                enhance_left_input = build_scene_media_prefix(pid, scene["id"], track, "ENH", True)
                enhance_left_input = config.get_temp_file(self.pid, "mp4", enhance_left_input + "_" + level + "_")
                safe_copy_overwrite(left_input, enhance_left_input)
                self._enhance_video(enhance_left_input)

                enhance_right_input = build_scene_media_prefix(pid, scene["id"], track, "ENH", True)
                enhance_right_input = config.get_temp_file(self.pid, "mp4", enhance_right_input + "_" + level + "_")
                safe_copy_overwrite(right_input, enhance_right_input)
                self._enhance_video(enhance_right_input)
                scene[track + "_status"] = "ENH1"
        else:
            input = get_file_path(scene, track)
            if input:
                input = self.workflow.ffmpeg_processor.refps_video(input, fps)
                enhance_input = build_scene_media_prefix(pid, scene["id"], track, "ENH", True)
                enhance_input = config.get_temp_file(pid, "mp4", enhance_input+"_"+level+"_")
                safe_copy_overwrite(input, enhance_input)
                self._enhance_video(enhance_input)
                scene[track + "_status"] = "ENH1"

 
    ENHANCE_SERVERS = ["http://10.0.0.210:5000", "http://10.0.0.235:5000", "http://10.0.0.210:5000", "http://10.0.0.235:5000", "http://10.0.0.235:5000",]
    current_enhance_server = 0

    def _enhance_video(self, video_path):
        """è°ƒç”¨ REST API å¢žå¼ºå•ä¸ªè§†é¢‘"""
        try: # clip_project_20251208_1710_10708_S2V_13225050_original.mp4
            url = self.ENHANCE_SERVERS[self.current_enhance_server] + "/enhance"
            self.current_enhance_server = (self.current_enhance_server + 1) % len(self.ENHANCE_SERVERS)
            
            with open(video_path, 'rb') as video_file:
                files = {'video': video_file}
                
                print(f"ðŸš€ æ­£åœ¨è°ƒç”¨è§†é¢‘å¢žå¼ºAPI: {url}")
                response = requests.post(url, files=files, timeout=300)
                
                if response.status_code >= 200 and response.status_code < 300:
                    print("âœ… å•è§†é¢‘å¢žå¼ºæˆåŠŸ")
                    print(f"ðŸ“„ å“åº”: {response.text}")
                else:
                    print(f"âŒ å•è§†é¢‘å¢žå¼ºå¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                    print(f"ðŸ“„ é”™è¯¯ä¿¡æ¯: {response.text}")
                    
        except requests.exceptions.RequestException as e:
            print(f"âŒ REST API è°ƒç”¨å¤±è´¥: {str(e)}")
        except Exception as e:
            print(f"âŒ å¢žå¼ºå•è§†é¢‘æ—¶å‡ºé”™: {str(e)}")


    def _enhance_image_in_bat(self, image_path, face_weight=0.3):
        # èŽ·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•çš„çˆ¶ç›®å½•ï¼ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰ï¼Œç„¶åŽæ‹¼æŽ¥ Real-ESRGAN å­æ–‡ä»¶å¤¹
        #project_root = Path("__file__").parent.parent
        enhancer_root = Path("/ImageEnhance")
        realesrgan_python = enhancer_root / "venv" / "Scripts" / "python.exe"

        temp_path = config.get_temp_path(self.workflow.pid)
        face_args = f"--face_enhance --face_enhance_weight {face_weight}"  if face_weight > 0.09 else ""
        # Use explicit Python path to ensure we use the correct version
        batch_content = f"""
cd {enhancer_root}
call venv\\Scripts\\activate.bat
{str(realesrgan_python)} inference_realesrgan.py -n RealESRGAN_x2plus -i {image_path} -o {temp_path} --suffix out  {face_args}
"""
        batch_file = config.get_temp_file(self.workflow.pid, "bat")
        with open(batch_file, 'w', encoding='utf-8') as f:
            f.write(batch_content)

        result = subprocess.run([str(batch_file)], shell=True, capture_output=True, text=True, check=False, encoding='utf-8', errors='ignore', cwd=str(enhancer_root))
        print(f"Real-ESRGAN batch script return code: {result.returncode}")

        temp_output = temp_path +"/" + Path(image_path).stem + "_out" + Path(image_path).suffix
        return self.workflow.ffmpeg_processor.resize_image_smart(temp_output)


    def _enhance_image_in_api(self, image_path, face_enhance_weight=0.3):
        output_path = config.get_temp_file(self.workflow.pid, "webp")
        #url = "http://10.0.0.216:5050/enhance"
        url = os.getenv("ENHANCE_API_URL", "")
        
        # æ‰“å¼€å›¾ç‰‡æ–‡ä»¶
        with open(image_path, 'rb') as f:
            files = {'image': f}
            data = {
                'face_enhance_weight': face_enhance_weight
            }
            
            # å‘é€è¯·æ±‚
            response = requests.post(url, files=files, data=data)
        
        # æ£€æŸ¥å“åº”
        if response.status_code == 200:
            # ä¿å­˜å¢žå¼ºåŽçš„å›¾ç‰‡
            with open(output_path, 'wb') as f:
                f.write(response.content)
            
            print(f"âœ… å›¾ç‰‡å¢žå¼ºæˆåŠŸï¼è¾“å‡º: {output_path}")
            return self.workflow.ffmpeg_processor.resize_image_smart(output_path)
        else:
            error_msg = response.json().get('error', 'Unknown error')
            print(f"âŒ é”™è¯¯: {error_msg}")
            return None


    def post_multipart(self, 
                      full_url: str, 
                      data: Dict[str, Any] = None,
                      files: Dict[str, str] = None) -> requests.Response:
        # Prepare form data
        form_data = data.copy() if data else {}
        
        # Prepare files for upload
        files_to_upload = {}
        if files:
            for field_name, file_path in files.items():
                if not os.path.exists(file_path):
                    raise FileNotFoundError(f"File not found: {file_path}")
                # Open file and add to upload
                files_to_upload[field_name] = open(file_path, 'rb')
        
        try:
            response = requests.post(
                full_url,
                data=form_data,
                files=files_to_upload,
                timeout=60
            )
            
            return response
            
        finally:
            # Close all opened files
            for file_obj in files_to_upload.values():
                file_obj.close()

