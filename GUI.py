import matplotlib
matplotlib.use('Agg')  # Must be at the TOP of main.py

import tkinter as tk
import tkinter.ttk as ttk
import tkinter.scrolledtext as scrolledtext
import tkinter.filedialog as filedialog
import tkinter.messagebox as messagebox
import os
import json
import threading
import time
from datetime import datetime
import pygame
import uuid
from magic_workflow import MagicWorkflow
import config
import config_prompt
import utility.sd_image_processor as sd_image_processor
from PIL import Image, ImageTk
from project_manager import ProjectConfigManager, create_project_dialog
import project_manager
from gui.picture_in_picture_dialog import PictureInPictureDialog
import cv2
import os
from utility.file_util import get_file_path, is_image_file, is_audio_file, is_video_file, refresh_scene_media, build_scene_media_prefix
from gui.media_review_dialog import AVReviewDialog
from utility.minimax_speech_service import MinimaxSpeechService, EXPRESSION_STYLES
from gui.wan_prompt_editor_dialog import show_wan_prompt_editor  # æ·»åŠ è¿™ä¸€è¡Œ
from gui.image_prompts_review_dialog import IMAGE_PROMPT_OPTIONS, NEGATIVE_PROMPT_OPTIONS
import tkinterdnd2 as TkinterDnD
from tkinterdnd2 import DND_FILES
from utility.media_scanner import MediaScanner
import cv2
from pathlib import Path

from moviepy import VideoFileClip
import moviepy
mp = moviepy  # Create an alias for compatibility


def askchoice(title, choices):
    """
    è‡ªå®šä¹‰çš„å¤šé€‰æ‹©å¯¹è¯æ¡†å‡½æ•°
    è¿”å›ç”¨æˆ·é€‰æ‹©çš„é€‰é¡¹å­—ç¬¦ä¸²
    """
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„é€‰æ‹©å¯¹è¯æ¡†
    root = tk.Toplevel()
    root.title(title)
    root.geometry("300x200")
    root.resizable(False, False)
    
    # å±…ä¸­æ˜¾ç¤º
    root.transient()
    root.grab_set()
    
    result = None
    
    def on_choice(choice):
        nonlocal result
        result = choice
        root.destroy()
    
    # æ·»åŠ æ ‡é¢˜
    label = tk.Label(root, text=title, font=("Arial", 12, "bold"))
    label.pack(pady=10)
    
    # æ·»åŠ é€‰æ‹©æŒ‰é’®
    for choice in choices:
        btn = tk.Button(root, text=choice, width=20, 
                       command=lambda c=choice: on_choice(c))
        btn.pack(pady=5)
    
    # æ·»åŠ å–æ¶ˆæŒ‰é’®
    cancel_btn = tk.Button(root, text="å–æ¶ˆ", width=20, 
                          command=lambda: root.destroy())
    cancel_btn.pack(pady=10)
    
    # ç­‰å¾…ç”¨æˆ·é€‰æ‹©
    root.wait_window()
    return result

# askchoiceå‡½æ•°å®šä¹‰å®Œæˆï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨



STANDARD_FPS = 60  # Match FfmpegProcessor.STANDARD_FPS


class WorkflowGUI:
    # Standardized framerate to match video processing

    def __init__(self, root):
        # å¦‚æœæ‹–æ‹½æ”¯æŒå¯ç”¨ï¼Œåˆ™ä½¿ç”¨TkinterDnDæ ¹çª—å£
        self.root = TkinterDnD.Tk() if not isinstance(root, TkinterDnD.Tk) else root
        # å¦‚æœä¼ å…¥çš„rootä¸æ˜¯TkinterDnD.Tkï¼Œéœ€è¦é‡æ–°åˆ›å»º
        if root != self.root:
            root.destroy()

        self.root.title("é­”æ³•å·¥ä½œæµ GUI")
        try:
            self.root.state('zoomed') # Windowså…¨å±
        except:
            screen_width = self.root.winfo_screenwidth()
            screen_height = self.root.winfo_screenheight()
            self.root.geometry(f"{screen_width}x{screen_height}+0+0")

        try:
            pygame.mixer.init()
            self.pygame_mixer_available = True
        except Exception as e:
            self.pygame_mixer_available = False
        
        self.playing_delta = 0.0
        self.second_delta = 0.0

        # åˆå§‹åŒ–é…ç½®åŠ è½½æ ‡å¿—
        self._loading_config = False
        self.current_scene_index = 0

        # æ˜¾ç¤ºé¡¹ç›®é€‰æ‹©å¯¹è¯æ¡†
        if not self.show_project_selection():
            self.root.destroy()
            return
        
        # é¦–å…ˆåˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€è·Ÿè¸ª - å¢å¼ºç‰ˆ
        self.tasks = {}
        self.completed_tasks = []  # å­˜å‚¨å·²å®Œæˆçš„ä»»åŠ¡
        self.last_notified_tasks = set()  # è·Ÿè¸ªå·²é€šçŸ¥çš„ä»»åŠ¡
        self.status_update_timer_id = None  # çŠ¶æ€æ›´æ–°å®šæ—¶å™¨ID
        self.monitoring_scenes = {}  # è·Ÿè¸ªæ­£åœ¨ç›‘æ§çš„åœºæ™¯ {scene_index: {"found_files": [], "start_time": time}}
        self.processed_output_files = set()  # è·Ÿè¸ªå·²å¤„ç†çš„ X:\output æ–‡ä»¶
        
        # å•ä¾‹åå°æ£€æŸ¥çº¿ç¨‹æ§åˆ¶
        self.video_check_thread = None  # åå°æ£€æŸ¥çº¿ç¨‹
        self.video_check_running = False  # çº¿ç¨‹è¿è¡Œæ ‡å¿—
        self.video_check_stop_event = threading.Event()  # åœæ­¢äº‹ä»¶
        
        # æ·»åŠ è§†é¢‘æ•ˆæœé€‰æ‹©å­˜å‚¨
        self.effect_radio_vars = {}  # {scene_index: tk.StringVar}
        
        # æ·»åŠ å½“å‰æ•ˆæœå’Œå›¾åƒç±»å‹é€‰æ‹©å˜é‡
        self.scene_second_animation = tk.StringVar(value=config_prompt.ANIMATE_SOURCE[0])
        
        # åˆ›å»ºåŠ¨ç”»åç§°åˆ°æç¤ºè¯­çš„æ˜ å°„å­—å…¸ï¼ˆåŒå‘ï¼‰
        self.animation_name_to_prompt = {item["name"]: item["prompt"] for item in config_prompt.ANIMATION_PROMPTS}
        self.animation_prompt_to_name = {item["prompt"]: item["name"] for item in config_prompt.ANIMATION_PROMPTS}
        self.animation_names = [""] + list(self.animation_name_to_prompt.keys())
        
        # æ·»åŠ ç¬¬äºŒè½¨é“éŸ³é‡æ§åˆ¶å˜é‡
        self.track_volume_var = tk.DoubleVar(value=0.2)
        
        # åˆ›å»ºä¸»æ¡†æ¶
        main_frame = ttk.Frame(root)
        main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
        
        # åˆ›å»ºå…±äº«ä¿¡æ¯åŒºåŸŸ
        self.create_shared_info_area(main_frame)
        
        # åˆ›å»ºæ ‡ç­¾é¡µæ§ä»¶
        self.notebook = ttk.Notebook(main_frame)
        self.notebook.pack(fill=tk.BOTH, expand=True, pady=(10, 0))
        
        # åˆ›å»ºå„ä¸ªæ ‡ç­¾é¡µ
        self.create_video_tab()
        
        self.setup_drag_and_drop()
        
        # ç»‘å®šæ ‡ç­¾é¡µåˆ‡æ¢äº‹ä»¶
        self.notebook.bind("<<NotebookTabChanged>>", self.on_tab_changed)
        
        # å¯åŠ¨ä»»åŠ¡çŠ¶æ€æ›´æ–°å®šæ—¶å™¨
        self.start_status_update_timer()
        
        # åŠ è½½ä¸Šæ¬¡ä¿å­˜çš„é…ç½®ï¼ˆå¿…é¡»åœ¨æ‰€æœ‰æ§ä»¶åˆ›å»ºå®Œæˆåï¼Œåœ¨ç»‘å®šäº‹ä»¶ä¹‹å‰ï¼‰
        self.load_config()
        self.bind_config_change_events()
        
        # ç«‹å³åˆ›å»ºå·¥ä½œæµå®ä¾‹ï¼ˆä¸å†ä½¿ç”¨æ‡’åŠ è½½ï¼‰
        self.create_workflow_instance()
        
        # å¯åŠ¨å•ä¾‹åå°è§†é¢‘æ£€æŸ¥çº¿ç¨‹
        self.start_video_check_thread()
        
        self.media_scanner = MediaScanner(self.workflow, 10)
        # ç»‘å®šçª—å£å…³é—­äº‹ä»¶

        self.workflow.load_scenes()
        self.on_tab_changed(None)

        self.root.protocol("WM_DELETE_WINDOW", self.on_closing)



    pid = None

    def get_pid(self):
        if self.pid is None:
            self.pid = project_manager.PROJECT_CONFIG.get('pid')
        return self.pid
    


    def create_workflow_instance(self):
        """ç«‹å³åˆ›å»ºå·¥ä½œæµå®ä¾‹ï¼ˆéæ‡’åŠ è½½ï¼‰"""
        try:
            # Get video dimensions from project config
            video_width = project_manager.PROJECT_CONFIG.get('video_width')
            video_height = project_manager.PROJECT_CONFIG.get('video_height')
            language = project_manager.PROJECT_CONFIG.get('language')
            channel = project_manager.PROJECT_CONFIG.get('channel')

            self.workflow = MagicWorkflow(self.get_pid(), language, channel, video_width, video_height)
            self.speech_service = MinimaxSpeechService(self.get_pid())
            
            current_gui_title = self.video_title.get().strip()
            self.workflow.post_init(current_gui_title)
            
            print("âœ… å·¥ä½œæµå®ä¾‹åˆ›å»ºå®Œæˆ")
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºå·¥ä½œæµå®ä¾‹å¤±è´¥: {e}")
            self.workflow = None


    def get_current_scene(self):
        if not hasattr(self, 'workflow') or self.workflow is None or not hasattr(self.workflow, 'scenes') or self.workflow.scenes is None:
            return None
            
        if self.workflow.scenes and self.current_scene_index >= 0 and self.current_scene_index < len(self.workflow.scenes):
            return self.workflow.scenes[self.current_scene_index]
        else:
            return None
    

    def get_previous_scene(self):
        if self.workflow.scenes and self.current_scene_index > 0 and self.current_scene_index < len(self.workflow.scenes):
            return self.workflow.scenes[self.current_scene_index - 1]
        else:
            return None    


    def get_next_scene(self):
        if self.workflow.scenes and self.current_scene_index >= 0 and self.current_scene_index < len(self.workflow.scenes)-1:
            return self.workflow.scenes[self.current_scene_index + 1]
        else:
            return None


    def get_previous_story_last_scene(self):
        if self.workflow.scenes and self.current_scene_index > 0 and self.current_scene_index < len(self.workflow.scenes):
            # loop from self.current_scene_index to 0,  
            for i in range(self.current_scene_index, 0, -1):
                if self.workflow.scenes[i]["id"]%10000 != self.workflow.scenes[self.current_scene_index]["id"]%10000:
                    return self.workflow.scenes[i]
        return None    

    
    def show_project_selection(self):
        # ä½¿ç”¨æ–°çš„é¡¹ç›®ç®¡ç†å™¨
        result, selected_config = create_project_dialog(self.root)
        
        if result == 'cancel':
            return False
        elif result == 'new':
            # ç«‹å³åˆ›å»ºProjectConfigManagerå¹¶ä¿å­˜æ–°é¡¹ç›®é…ç½®
            pid = selected_config.get('pid')
            try:
                # å…ˆè®¾ç½®å…¨å±€ project_manager.PROJECT_CONFIG
                ProjectConfigManager.set_global_config(selected_config)
                # ç„¶ååˆ›å»º ProjectConfigManager å¹¶ä¿å­˜
                config_manager = ProjectConfigManager(pid)
                config_manager.save_project_config(selected_config)
                print(f"âœ… æ–°é¡¹ç›®é…ç½®å·²ä¿å­˜: {pid}")
            except Exception as e:
                print(f"âŒ ä¿å­˜æ–°é¡¹ç›®é…ç½®å¤±è´¥: {e}")
            
            return True
        elif result == 'open':
            # æ‰“å¼€ç°æœ‰é¡¹ç›®
            if selected_config is None:
                print("âŒ é”™è¯¯ï¼šselected_config ä¸º None")
                return False
            # æ³¨æ„ï¼šproject_manager.PROJECT_CONFIG å·²ç»åœ¨ open_selected() ä¸­è®¾ç½®äº†ï¼Œè¿™é‡Œå†æ¬¡ç¡®è®¤è®¾ç½®
            ProjectConfigManager.set_global_config(selected_config)
            return True
        
        return False

   
    def create_shared_info_area(self, parent):
        """åˆ›å»ºå…±äº«ä¿¡æ¯åŒºåŸŸ"""
        shared_frame = ttk.LabelFrame(parent, text="å…±äº«é…ç½®", padding=10)
        shared_frame.pack(fill=tk.X, pady=(0, 10))
        
        # ç¬¬ä¸€è¡Œï¼šåŸºæœ¬é¡¹ç›®é…ç½®
        row1_frame = ttk.Frame(shared_frame)
        row1_frame.pack(fill=tk.X, pady=(0, 5))
        
        scene_nav_row = ttk.Frame(row1_frame)
        scene_nav_row.pack(side=tk.LEFT, padx=(0, 10))

        ttk.Separator(scene_nav_row, orient='vertical').pack(side=tk.LEFT, fill=tk.Y, padx=10)
        ttk.Button(scene_nav_row, text="â®", width=3, command=self.first_scene).pack(side=tk.LEFT, padx=2)
        ttk.Separator(scene_nav_row, orient='vertical').pack(side=tk.LEFT, fill=tk.Y, padx=10)
        ttk.Label(scene_nav_row, text="åœºæ™¯:").pack(side=tk.LEFT)
        ttk.Button(scene_nav_row, text="â—€", width=3, command=self.prev_scene).pack(side=tk.LEFT, padx=2)
        self.scene_label = ttk.Label(scene_nav_row, text="0 / 0", width=7)
        self.scene_label.pack(side=tk.LEFT, padx=2)
        ttk.Button(scene_nav_row, text="â–¶", width=3, command=self.next_scene).pack(side=tk.LEFT, padx=2)
        ttk.Separator(scene_nav_row, orient='vertical').pack(side=tk.LEFT, fill=tk.Y, padx=10)
        ttk.Button(scene_nav_row, text="â­", width=3, command=self.last_scene).pack(side=tk.LEFT, padx=2)
        ttk.Separator(scene_nav_row, orient='vertical').pack(side=tk.LEFT, fill=tk.Y, padx=10)

        ttk.Button(row1_frame, text="æ‹·è´å›¾",   command=self.copy_images_to_next).pack(side=tk.LEFT, padx=2)
        ttk.Button(row1_frame, text="åœºæ™¯äº¤æ¢", command=self.swap_scene).pack(side=tk.LEFT, padx=2)

        ttk.Separator(row1_frame, orient='vertical').pack(side=tk.LEFT, fill=tk.Y, padx=10)
        ttk.Button(row1_frame, text="è§†é¢‘åˆæˆ", command=lambda:self.run_finalize_video()).pack(side=tk.LEFT, padx=2)
        #ttk.Button(row1_frame, text="è§†èƒŒåˆæˆ", command=lambda:self.run_finalize_video(zero_audio_only=True)).pack(side=tk.LEFT, padx=2)
        ttk.Button(row1_frame, text="æ¨å¹¿åˆæˆ", command=lambda:self.run_promotion_video()).pack(side=tk.LEFT, padx=2)
        ttk.Button(row1_frame, text="ä¸Šä¼ è§†é¢‘", command=self.run_upload_video).pack(side=tk.LEFT, padx=2)
        #ttk.Button(scene_nav_row, text="æ‹¼æ¥è§†é¢‘", command=self.run_final_concat_video).pack(side=tk.LEFT, padx=2)

        ttk.Separator(row1_frame, orient='vertical').pack(side=tk.LEFT, fill=tk.Y, padx=10)
        pid_frame = ttk.Frame(row1_frame)
        pid_frame.pack(side=tk.LEFT, padx=(0, 10))
        ttk.Label(pid_frame, text="PID").pack(side=tk.LEFT)
        self.shared_pid = ttk.Label(pid_frame, width=20, relief="sunken", background="white")
        self.shared_pid.pack(side=tk.LEFT, padx=(5, 0))
        
        # è¯­è¨€ç»„ (åªè¯»)
        lang_frame = ttk.Frame(row1_frame)
        lang_frame.pack(side=tk.LEFT, padx=(0, 10))
        ttk.Label(lang_frame, text="è¯­è¨€").pack(side=tk.LEFT)
        self.shared_language = ttk.Label(lang_frame, width=5, relief="sunken", background="white")
        self.shared_language.pack(side=tk.LEFT, padx=(5, 0))
        
        # è§†é¢‘æ ‡é¢˜ç»„
        title_frame = ttk.Frame(row1_frame)
        title_frame.pack(side=tk.LEFT, padx=(0, 10))
        ttk.Label(title_frame, text="æ ‡é¢˜").pack(side=tk.LEFT)
        self.video_title = ttk.Entry(title_frame, width=20)
        self.video_title.pack(side=tk.LEFT)
        ttk.Label(title_frame, text="é¢‘é“").pack(side=tk.LEFT)
        self.shared_channel = ttk.Label(title_frame, width=15, relief="sunken", background="white")
        self.shared_channel.pack(side=tk.LEFT)
        
        ttk.Separator(row1_frame, orient='vertical').pack(side=tk.LEFT, fill=tk.Y, padx=10)

        tool_frame = ttk.Frame(row1_frame)
        tool_frame.pack(side=tk.LEFT, padx=(0, 10))
        ttk.Button(tool_frame, text="Videoç”Ÿæˆ", command=self.start_video_gen_batch).pack(side=tk.LEFT) 
        ttk.Button(tool_frame, text="åª’ä½“æ¸…ç†",  command=self.clean_media).pack(side=tk.LEFT) 
        ttk.Button(tool_frame, text="WANæ¸…ç†",   command=self.clean_wan).pack(side=tk.LEFT) 
        ttk.Button(tool_frame, text="æ ‡è®°æ¸…ç†",  command=self.clean_media_mark).pack(side=tk.LEFT)

   
    def open_image_prompt_dialog(self, create_image_callback, scene, image_mode, language:str):
        """æ‰“å¼€æç¤ºè¯å®¡æŸ¥å¯¹è¯æ¡†ï¼Œç”¨äºåœ¨åˆ›å»ºå›¾åƒå‰é¢„è§ˆå’Œç¼–è¾‘æç¤ºè¯"""
        from gui.image_prompts_review_dialog import ImagePromptsReviewDialog
        
        dialog = ImagePromptsReviewDialog(
            parent=self,
            workflow=self.workflow,
            create_image_callback=create_image_callback,
            scene=scene,
            track=image_mode,
            language=language
        )
        dialog.show()


    def swap_second(self):
        """äº¤æ¢ç¬¬ä¸€è½¨é“ä¸ç¬¬äºŒè½¨é“"""
        current_scene = self.get_current_scene()
        clip_video_path = get_file_path(current_scene, 'clip')
        clip_audio_path = get_file_path(current_scene, 'clip_audio')
        track_path = get_file_path(current_scene, "second")
        if not track_path:
            messagebox.showwarning("è­¦å‘Š", "second è½¨é“è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨")
            return
        temp_track = self.workflow.ffmpeg_processor.add_audio_to_video(track_path, clip_audio_path)

        refresh_scene_media(current_scene, "second", '.mp4', clip_video_path)
        refresh_scene_media(current_scene, "second_audio", '.wav', clip_audio_path, True)

        refresh_scene_media(current_scene, 'clip', '.mp4', temp_track)
        self.refresh_gui_scenes()


    def swap_zero(self):
        """äº¤æ¢ç¬¬ä¸€è½¨é“ä¸ç¬¬äºŒè½¨é“"""
        current_scene = self.get_current_scene()
        clip_video_path = get_file_path(current_scene, 'clip')
        clip_audio_path = get_file_path(current_scene, 'clip_audio')
        zero_path = get_file_path(current_scene, "zero")
        if not zero_path:
            messagebox.showwarning("è­¦å‘Š", "zeroè½¨é“è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨")
            return

        refresh_scene_media(current_scene, "back", '.mp4', clip_video_path)

        start_time_in_story, clip_duration, story_duration, indx, count, is_story_last_clip = self.workflow.get_scene_detail(current_scene)
        end_time = start_time_in_story + clip_duration

        temp_track = self.workflow.ffmpeg_processor.trim_video(zero_path, start_time_in_story, end_time)
        temp_track = self.workflow.ffmpeg_processor.add_audio_to_video(temp_track, clip_audio_path)

        refresh_scene_media(current_scene, 'clip', '.mp4', temp_track)
        self.refresh_gui_scenes()


    def track_recover(self):
        current_scene = self.get_current_scene()
        clip = current_scene.get('clip', None)
        back = current_scene.get('back', None)
        if not back:
            messagebox.showwarning("è­¦å‘Š", "èƒŒæ™¯è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨")
            return

        paths = back.split(',')
        back_path = None
        for i in range(len(paths)):
            back_path = paths[i]
            back = ','.join(paths[i+1:])
            if os.path.exists(back_path):
                break
            back_path = None
            if i == len(paths) - 1:
                back = ""

        if not back_path:
            return

        if clip:
            current_scene['back'] = clip + "," + back

        refresh_scene_media(current_scene, 'clip', '.mp4', back_path)
        self.workflow.save_scenes_to_json()
        self.refresh_gui_scenes()


    def reset_track_offset(self):
        """é‡ç½®ç¬¬äºŒè½¨é“æ’­æ”¾åç§»é‡åˆ°å½“å‰åœºæ™¯çš„èµ·å§‹ä½ç½®"""
        current_scene = self.get_current_scene()
        if not current_scene:
            self.second_track_offset = 0
            self.second_track_paused_time = None
            return
            
        self.second_track_offset, clip_duration, story_duration, indx, count, is_story_last_clip = self.workflow.get_scene_detail(current_scene)
        self.second_track_paused_time = None
        print(f"ğŸ”„ é‡ç½®ç¬¬äºŒè½¨é“åç§»é‡: {self.second_track_offset:.2f}s")
        self.update_second_track_time_display()


    def fetch_second_clip(self, to_end, volume):
        current_scene = self.get_current_scene()
        second_track_path = get_file_path(current_scene, 'second')
        second_audio_path = get_file_path(current_scene, 'second_audio')
        if not second_track_path:
            messagebox.showwarning("è­¦å‘Š", "ç¬¬äºŒè½¨é“è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨")
            return
        
        second_track_duration = self.workflow.ffmpeg_processor.get_duration(second_track_path)

        if not self.second_track_cap:
            second_time = 0
        else:
            second_pos = self.second_track_cap.get(cv2.CAP_PROP_POS_FRAMES)
            second_time = second_pos / STANDARD_FPS

        if second_time <= 0:
            second_time, clip_duration, story_duration, indx, count, is_story_last_clip = self.workflow.get_scene_detail(current_scene)

        if second_track_duration < second_time:
            second_time = 0

        if to_end:
            second_v = self.workflow.ffmpeg_processor.trim_video(second_track_path, second_time, None, volume)
            second_a = self.workflow.ffmpeg_audio_processor.audio_cut_fade(second_audio_path, second_time, None, 1.0, 1.0,volume)
        else:
            clip_duration = self.workflow.find_clip_duration(current_scene)
            second_v = self.workflow.ffmpeg_processor.trim_video(second_track_path, second_time, second_time+clip_duration, volume)
            second_a = self.workflow.ffmpeg_audio_processor.audio_cut_fade(second_audio_path, second_time, clip_duration, 1.0, 1.0, volume)

        return second_v, second_a


    def choose_second_track(self, track_id):
        """é€‰æ‹©ç¬¬äºŒè½¨é“å¹¶é‡ç½®æ’­æ”¾çŠ¶æ€"""
        self.selected_second_track = track_id
        # é‡ç½®æ’­æ”¾åç§»é‡åˆ°å½“å‰åœºæ™¯çš„èµ·å§‹ä½ç½®
        self.reset_track_offset()
        # åˆ‡æ¢ tab å¹¶åŠ è½½ç¬¬ä¸€å¸§
        self.on_second_track_tab_changed()



    def pip_second_track(self, from_zero):
        """å°†ç¬¬äºŒè½¨é“ä½œä¸ºç”»ä¸­ç”»å åŠ åˆ°ä¸»è½¨é“è§†é¢‘ä¸Š"""
        try:
            current_scene = self.get_current_scene()
            second_path = get_file_path(current_scene, self.selected_second_track)
            second_audio = get_file_path(current_scene, self.selected_second_track+'_audio')
            second_left = get_file_path(current_scene, self.selected_second_track+'_left')
            second_right = get_file_path(current_scene, self.selected_second_track+'_right')
            if not second_path or not second_audio:
                messagebox.showwarning("è­¦å‘Š", "ç¬¬äºŒè½¨é“è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨")
                return

            clip_video = get_file_path(current_scene, "clip")
            clip_audio = get_file_path(current_scene, "clip_audio")
            start_time, clip_duration, story_duration, indx, count, is_story_last_clip = self.workflow.get_scene_detail(current_scene)
            if from_zero:
                start_time = 0

            start_time = start_time + self.second_delta

            if is_story_last_clip: 
                second_track_copy = self.workflow.ffmpeg_processor.trim_video(second_path, start_time)
                second_audio_copy = self.workflow.ffmpeg_audio_processor.audio_cut_fade(second_audio, start_time, None, 0, 0, 1.0)
            else:    
                second_track_copy = self.workflow.ffmpeg_processor.trim_video(second_path, start_time, start_time+clip_duration)
                second_audio_copy = self.workflow.ffmpeg_audio_processor.audio_cut_fade(second_audio, start_time, clip_duration, 0, 0, 1.0)
            print(f"ğŸ“º æ‰“å¼€ç”»ä¸­ç”»è®¾ç½®å¯¹è¯æ¡†...")
            
            # åˆ›å»ºç”»ä¸­ç”»è®¾ç½®å¯¹è¯æ¡†
            pip_dialog = PictureInPictureDialog(self.root, clip_video, second_track_copy, second_left, second_right)
            
            # ç­‰å¾…å¯¹è¯æ¡†å…³é—­
            self.root.wait_window(pip_dialog.dialog)
            
            # æ£€æŸ¥ç”¨æˆ·çš„é€‰æ‹©
            if pip_dialog.result:
                settings = pip_dialog.result
                print(f"ğŸ“º ç”¨æˆ·é€‰æ‹©çš„ç”»ä¸­ç”»è®¾ç½®: {settings}")

                back = current_scene.get('back', '')
                current_scene['back'] = clip_video + "," + back

                if settings['position'] == "full":
                    v = self.workflow.ffmpeg_processor.add_audio_to_video(second_track_copy, clip_audio)
                    refresh_scene_media(current_scene, 'clip', '.mp4', v)
                elif settings['position'] == "av":
                    refresh_scene_media(current_scene, 'clip', '.mp4', second_track_copy)
                    refresh_scene_media(current_scene, 'clip_audio', '.wav', second_audio_copy)
                else:
                    # å¤„ç†ç”»ä¸­ç”»
                    self.process_picture_in_picture(
                        background_audio=clip_audio,
                        background_video=clip_video,
                        overlay_video=second_track_copy,
                        overlay_audio=second_audio_copy,
                        overlay_left=second_left,
                        overlay_right=second_right,
                        settings=settings
                    )

                # æ›´æ–°æ˜¾ç¤º
                self.workflow.save_scenes_to_json()
                self.refresh_gui_scenes()
                messagebox.showinfo("æˆåŠŸ", f"ç”»ä¸­ç”»å¤„ç†å®Œæˆ")

            else:
                print("ğŸš« ç”¨æˆ·å–æ¶ˆäº†ç”»ä¸­ç”»è®¾ç½®")
                
        except Exception as e:
            error_msg = f"ç”»ä¸­ç”»å¤„ç†å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            messagebox.showerror("é”™è¯¯", error_msg)


    def process_picture_in_picture(self, background_video, background_audio, overlay_video, overlay_audio, overlay_left, overlay_right, settings):
        """å¤„ç†ç”»ä¸­ç”»è§†é¢‘ç”Ÿæˆ"""
        try:
            print(f"ğŸ¬ å¼€å§‹å¤„ç†ç”»ä¸­ç”»...")
            if not self.video_cap:
                current_time = 0
            else:
                current_frame = self.video_cap.get(cv2.CAP_PROP_POS_FRAMES)
                current_time = current_frame / STANDARD_FPS

            left_video = None
            right_video = None
            if settings['position'] == "left" and overlay_left:
                left_video = overlay_left
            elif settings['position'] == "right" and overlay_right:
                right_video = overlay_right
            elif settings['position'] == "center" and overlay_left and overlay_right:
                left_video = overlay_left
                right_video = overlay_right

            if left_video or right_video:
                #    background_audio=background_audio,
                output_video = self.workflow.ffmpeg_processor.add_left_right_picture_in_picture(
                                    background_video=background_video,
                                    overlay_video_left=left_video,
                                    overlay_video_right=right_video,
                                    ratio=settings['ratio'],
                                    delay_time=settings.get('delay_time', 0),
                                    edge_blur=0
                                )
            else:
                output_video = self.workflow.ffmpeg_processor.add_picture_in_picture(
                    background_video=background_video,
                    slide_in_video=overlay_video,
                    start_time=current_time,
                    ratio=settings['ratio'],
                    transition_duration=settings['transition_duration'],
                    position=settings['position'],
                    mask=settings['shape']
                )

            print(f"âœ… ç”»ä¸­ç”»å¤„ç†å®Œæˆ: {output_video}")

            output_audio = None
            if settings['audio_volume'] == 0.0:
                olda, output_audio = refresh_scene_media(self.get_current_scene(), "clip_audio", ".wav", background_audio, True)
                output_video = self.workflow.ffmpeg_processor.add_audio_to_video(output_video, background_audio)
                olda, output_video = refresh_scene_media(self.get_current_scene(), "clip", ".mp4", output_video, True)
            else:
                output_audio = background_audio
                if overlay_audio:
                    volume_main = 1
                    volume_overlay = 1
                    if settings['audio_volume'] > 0 :
                        volume_overlay = settings['audio_volume']
                        if volume_overlay > 0.9:
                            volume_overlay = 0.9
                    elif settings['audio_volume'] < 0:
                        volume_main = settings['audio_volume']
                        if volume_main < -0.9:
                            volume_main = -0.9
                        volume_main = 1+volume_main    

                    output_audio = self.workflow.ffmpeg_audio_processor.audio_mix(background_audio, volume_main, current_time, overlay_audio, volume_overlay)
                    olda, output_audio = refresh_scene_media(self.get_current_scene(), "clip_audio", ".wav", output_audio, True)

                    output_video = self.workflow.ffmpeg_processor.add_audio_to_video(output_video, output_audio)
                    olda, output_video = refresh_scene_media(self.get_current_scene(), "clip", ".mp4", output_video, True)
            
            return output_video, output_audio

        except Exception as e:
            error_msg = f"ç”»ä¸­ç”»å¤„ç†å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            messagebox.showerror("é”™è¯¯", error_msg)
            return None, None


    def upload_promo_video(self):
        task_id = str(uuid.uuid4())
        self.tasks[task_id] = {
            "type": "upload_promo_video",
            "status": "è¿è¡Œä¸­",
            "start_time": datetime.now(),
            "pid": self.workflow.pid
        }
        
        def run_task():
            try:
                print(f"ğŸ¬ ä¸Šä¼ å®£ä¼ è§†é¢‘...")
                title = self.video_title.get().strip()
                
                # è°ƒç”¨å·¥ä½œæµçš„æ–¹æ³•
                result_video_path = self.workflow.upload_promo_video(title, "")

                print(f"âœ… å®£ä¼ è§†é¢‘ä¸Šä¼ å®Œæˆ: {result_video_path}")
                
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                self.tasks[task_id]["status"] = "å®Œæˆ"
                self.tasks[task_id]["result"] = f"å®£ä¼ è§†é¢‘å·²ä¸Šä¼ : {os.path.basename(result_video_path)}"
                
            except Exception as e:
                error_msg = f"ä¸Šä¼ å®£ä¼ è§†é¢‘å¤±è´¥: {str(e)}"
                print(f"âŒ {error_msg}")
                
                # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
                self.tasks[task_id]["status"] = "å¤±è´¥"
                self.tasks[task_id]["error"] = str(e)
                
                # é€šçŸ¥é”™è¯¯
                self.root.after(0, lambda: messagebox.showerror("é”™è¯¯", error_msg))
        # å¯åŠ¨åå°ä»»åŠ¡
        thread = threading.Thread(target=run_task, daemon=True)
        thread.start()
        
        print(f"ğŸš€ ä¸Šä¼ å®£ä¼ è§†é¢‘ä»»åŠ¡å·²å¯åŠ¨ï¼Œä»»åŠ¡ID: {task_id}")
        


    def create_video_tab(self):
        """åˆ›å»ºè§†é¢‘ç”Ÿæˆæ ‡ç­¾é¡µ"""
        tab = ttk.Frame(self.notebook)
        self.notebook.add(tab, text="ç”Ÿæˆè§†é¢‘--")
        
        # ä¸»å†…å®¹åŒºåŸŸ
        main_content = ttk.Frame(tab)
        main_content.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)
        
        # å·¦ä¾§ï¼šè§†é¢‘é¢„è§ˆåŒºåŸŸ
        video_frame = ttk.LabelFrame(main_content, text="é¢„è§ˆ", padding=10)
        video_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=False, padx=(0, 5))
        # è®¾ç½®å·¦ä¾§é¢æ¿çš„æœ€å¤§å®½åº¦ï¼Œä¸ºå³ä¾§é¢æ¿ç•™å‡ºç©ºé—´
        video_frame.configure(width=1700)
        video_frame.pack_propagate(False)

        # åˆ›å»ºæ°´å¹³å¸ƒå±€æ¡†æ¶æ¥å¹¶æ’æ˜¾ç¤ºå›¾åƒæ ‡ç­¾å’Œè§†é¢‘ç”»å¸ƒ
        preview_frame = ttk.Frame(video_frame)
        preview_frame.pack(fill=tk.BOTH, expand=True)
        
        # å·¦ä¾§åŒºåŸŸï¼šèƒŒæ™¯è½¨é“å’Œç¬¬äºŒè½¨é“ï¼ˆå‡å°‘å®½åº¦ç»™video_canvasæ›´å¤šç©ºé—´ï¼‰
        left_frame = ttk.Frame(preview_frame)
        left_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=False, padx=(0, 5))
        # è®¾ç½®å·¦ä¾§æ¡†æ¶çš„å®½åº¦ï¼Œä¸ºvideo_canvasç•™å‡ºæ›´å¤šç©ºé—´
        left_frame.configure(width=640)
        left_frame.pack_propagate(False)
        
        # è§’è‰²é€‰æ‹©ç»„åˆæ¡†æ¡†æ¶
        roles_frame = ttk.Frame(left_frame)
        roles_frame.pack(fill=tk.X, pady=(0, 5))
        
        # å›¾ç‰‡é¢„è§ˆåŒºåŸŸï¼ˆåŸzeroä½ç½®ï¼‰
        images_preview_frame = ttk.LabelFrame(left_frame, text="å›¾ç‰‡é¢„è§ˆ (æ”¯æŒæ‹–æ”¾)", padding=5)
        images_preview_frame.pack(fill=tk.BOTH, expand=True, pady=5)
        
        # åˆ›å»º3ä¸ªå›¾ç‰‡é¢„è§ˆcanvas (clip_image, second_image, zero_image)
        images_container = ttk.Frame(images_preview_frame)
        images_container.pack(fill=tk.BOTH, expand=True)
        
        # Replace the images_preview_frame section (lines 794-859) with this enhanced version:
        # === Clip Image Canvas (clip_image + clip_image_last) ===
        clip_img_frame = ttk.Frame(images_container)
        clip_img_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(0, 2))
        ttk.Label(clip_img_frame, text="Clip", anchor=tk.CENTER).pack()

        # Clip image container with two sub-canvases
        clip_canvas_container = ttk.Frame(clip_img_frame)
        clip_canvas_container.pack(fill=tk.BOTH, expand=True)

        # Top: clip_image
        self.clip_image_canvas = tk.Canvas(clip_canvas_container, bg='gray20', width=150, height=75, 
                                        highlightthickness=2, highlightbackground='blue')
        self.clip_image_canvas.pack(fill=tk.BOTH, expand=True, pady=(0, 1))
        self.clip_image_canvas.create_text(75, 37, text="Clip\nImage", fill="gray", font=("Arial", 8), 
                                        justify=tk.CENTER, tags="hint")

        self.clip_image_canvas.drop_target_register(DND_FILES)
        self.clip_image_canvas.dnd_bind('<<Drop>>', lambda e: self.on_image_drop(e, 'clip_image'))

        # Bottom: clip_image_last
        self.clip_image_last_canvas = tk.Canvas(clip_canvas_container, bg='gray20', width=150, height=75, 
                                                highlightthickness=2, highlightbackground='blue')
        self.clip_image_last_canvas.pack(fill=tk.BOTH, expand=True, pady=(1, 0))
        self.clip_image_last_canvas.create_text(75, 37, text="Clip\nLast", fill="gray", font=("Arial", 8), 
                                            justify=tk.CENTER, tags="hint")

        self.clip_image_last_canvas.drop_target_register(DND_FILES)
        self.clip_image_last_canvas.dnd_bind('<<Drop>>', lambda e: self.on_image_drop(e, 'clip_image_last'))

        # === Zero Image Canvas (zero_image + zero_image_last) ===
        zero_img_frame = ttk.Frame(images_container)
        zero_img_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=2)
        ttk.Label(zero_img_frame, text="Zero", anchor=tk.CENTER).pack()

        zero_canvas_container = ttk.Frame(zero_img_frame)
        zero_canvas_container.pack(fill=tk.BOTH, expand=True)

        # Top: zero_image
        self.zero_image_canvas = tk.Canvas(zero_canvas_container, bg='gray20', width=150, height=75, 
                                        highlightthickness=2, highlightbackground='orange')
        self.zero_image_canvas.pack(fill=tk.BOTH, expand=True, pady=(0, 1))
        self.zero_image_canvas.create_text(75, 37, text="Zero\nImage", fill="gray", font=("Arial", 8), 
                                        justify=tk.CENTER, tags="hint")

        self.zero_image_canvas.drop_target_register(DND_FILES)
        self.zero_image_canvas.dnd_bind('<<Drop>>', lambda e: self.on_image_drop(e, 'zero_image'))

        # Bottom: zero_image_last
        self.zero_image_last_canvas = tk.Canvas(zero_canvas_container, bg='gray20', width=150, height=75, 
                                                highlightthickness=2, highlightbackground='orange')
        self.zero_image_last_canvas.pack(fill=tk.BOTH, expand=True, pady=(1, 0))
        self.zero_image_last_canvas.create_text(75, 37, text="Zero\nLast", fill="gray", font=("Arial", 8), 
                                            justify=tk.CENTER, tags="hint")

        self.zero_image_last_canvas.drop_target_register(DND_FILES)
        self.zero_image_last_canvas.dnd_bind('<<Drop>>', lambda e: self.on_image_drop(e, 'zero_image_last'))

        # === One Image Canvas (one_image + one_image_last) ===
        one_img_frame = ttk.Frame(images_container)
        one_img_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=2)
        ttk.Label(one_img_frame, text="One", anchor=tk.CENTER).pack()

        one_canvas_container = ttk.Frame(one_img_frame)
        one_canvas_container.pack(fill=tk.BOTH, expand=True)

        # Top: one_image
        self.one_image_canvas = tk.Canvas(one_canvas_container, bg='gray20', width=150, height=75, 
                                        highlightthickness=2, highlightbackground='purple')
        self.one_image_canvas.pack(fill=tk.BOTH, expand=True, pady=(0, 1))
        self.one_image_canvas.create_text(75, 37, text="One\nImage", fill="gray", font=("Arial", 8), 
                                        justify=tk.CENTER, tags="hint")

        self.one_image_canvas.drop_target_register(DND_FILES)
        self.one_image_canvas.dnd_bind('<<Drop>>', lambda e: self.on_image_drop(e, 'one_image'))

        # Bottom: one_image_last
        self.one_image_last_canvas = tk.Canvas(one_canvas_container, bg='gray20', width=150, height=75, 
                                            highlightthickness=2, highlightbackground='purple')
        self.one_image_last_canvas.pack(fill=tk.BOTH, expand=True, pady=(1, 0))
        self.one_image_last_canvas.create_text(75, 37, text="One\nLast", fill="gray", font=("Arial", 8), 
                                            justify=tk.CENTER, tags="hint")

        self.one_image_last_canvas.drop_target_register(DND_FILES)
        self.one_image_last_canvas.dnd_bind('<<Drop>>', lambda e: self.on_image_drop(e, 'one_image_last'))

        # === Second Image Canvas (second_image + second_image_last) ===
        second_img_frame = ttk.Frame(images_container)
        second_img_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(2, 0))
        ttk.Label(second_img_frame, text="Second", anchor=tk.CENTER).pack()

        second_canvas_container = ttk.Frame(second_img_frame)
        second_canvas_container.pack(fill=tk.BOTH, expand=True)

        # Top: second_image
        self.second_image_canvas = tk.Canvas(second_canvas_container, bg='gray20', width=150, height=75, 
                                            highlightthickness=2, highlightbackground='green')
        self.second_image_canvas.pack(fill=tk.BOTH, expand=True, pady=(0, 1))
        self.second_image_canvas.create_text(75, 37, text="Second\nImage", fill="gray", font=("Arial", 8), 
                                            justify=tk.CENTER, tags="hint")

        self.second_image_canvas.drop_target_register(DND_FILES)
        self.second_image_canvas.dnd_bind('<<Drop>>', lambda e: self.on_image_drop(e, 'second_image'))

        # Bottom: second_image_last
        self.second_image_last_canvas = tk.Canvas(second_canvas_container, bg='gray20', width=150, height=75, 
                                                highlightthickness=2, highlightbackground='green')
        self.second_image_last_canvas.pack(fill=tk.BOTH, expand=True, pady=(1, 0))
        self.second_image_last_canvas.create_text(75, 37, text="Second\nLast", fill="gray", font=("Arial", 8), 
                                                justify=tk.CENTER, tags="hint")

        self.second_image_last_canvas.drop_target_register(DND_FILES)
        self.second_image_last_canvas.dnd_bind('<<Drop>>', lambda e: self.on_image_drop(e, 'second_image_last'))
        

        # è§†é¢‘è½¨é“é¢„è§ˆåŒºåŸŸ - ä½¿ç”¨Tabæ§ä»¶ï¼ˆåŒ…å«secondå’Œzeroï¼‰
        track_video_frame = ttk.LabelFrame(left_frame, text="è½¨é“è§†é¢‘é¢„è§ˆ", padding=5)
        track_video_frame.pack(fill=tk.BOTH, expand=True, pady=5)
        
        # åˆ›å»ºNotebook (Tabæ§ä»¶)
        self.second_notebook = ttk.Notebook(track_video_frame)
        self.second_notebook.pack(fill=tk.BOTH, expand=True)
        
        # === Tab 1: å®Œæ•´ç¬¬äºŒè½¨é“ ===
        tab_full_second = ttk.Frame(self.second_notebook)
        self.second_notebook.add(tab_full_second, text="å®Œæ•´è§†é¢‘")
        
        # ç¬¬äºŒè½¨é“è§†é¢‘ç”»å¸ƒ
        self.second_track_canvas = tk.Canvas(tab_full_second, bg='black', width=360, height=180)
        self.second_track_canvas.pack(fill=tk.BOTH, expand=True, padx=2, pady=2)
        
        # ç¬¬äºŒè½¨é“æç¤ºæ–‡æœ¬
        self.second_track_canvas.create_text(160, 90, text="ç¬¬äºŒè½¨é“è§†é¢‘é¢„è§ˆ\né€‰æ‹©è§†é¢‘åæ’­æ”¾æ˜¾ç¤º", 
                                            fill="gray", font=("Arial", 10), justify=tk.CENTER, tags="hint")
        
        # === Tab 2: ç”»ä¸­ç”» Left & Right ===
        tab_pip_lr = ttk.Frame(self.second_notebook)
        self.second_notebook.add(tab_pip_lr, text="ç”»ä¸­ç”»L/R")
        
        # åˆ›å»ºå·¦å³å¹¶æ’çš„ç”»å¸ƒæ¡†æ¶
        pip_lr_frame = ttk.Frame(tab_pip_lr)
        pip_lr_frame.pack(fill=tk.BOTH, expand=True, padx=2, pady=2)
        
        # å·¦ä¾§è§†é¢‘ç”»å¸ƒ
        left_canvas_frame = ttk.Frame(pip_lr_frame)
        left_canvas_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(0, 2))
        ttk.Label(left_canvas_frame, text="Left", anchor=tk.CENTER).pack()
        self.pip_left_canvas = tk.Canvas(left_canvas_frame, bg='black', width=175, height=180)
        self.pip_left_canvas.pack(fill=tk.BOTH, expand=True)
        self.pip_left_canvas.create_text(77, 80, text="Left\nç”»ä¸­ç”»å·¦ä¾§", fill="gray", font=("Arial", 9), justify=tk.CENTER, tags="hint")
        
        # å³ä¾§è§†é¢‘ç”»å¸ƒ
        right_canvas_frame = ttk.Frame(pip_lr_frame)
        right_canvas_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(2, 0))
        ttk.Label(right_canvas_frame, text="Right", anchor=tk.CENTER).pack()
        self.pip_right_canvas = tk.Canvas(right_canvas_frame, bg='black', width=175, height=180)
        self.pip_right_canvas.pack(fill=tk.BOTH, expand=True)
        self.pip_right_canvas.create_text(77, 80, text="Right\nç”»ä¸­ç”»å³ä¾§", fill="gray", font=("Arial", 9), justify=tk.CENTER, tags="hint")
        
        # è½¨é“è§†é¢‘æ§åˆ¶å™¨ï¼ˆåœ¨é¢„è§ˆåŒºåŸŸä¸‹æ–¹ï¼Œæ‰€æœ‰tabå…±ç”¨ï¼‰
        self.track_frame = ttk.Frame(left_frame)
        self.track_frame.pack(fill=tk.X, pady=5)
        
        # ç¬¬äºŒè½¨é“æ’­æ”¾æŒ‰é’®
        self.track_play_button = ttk.Button(self.track_frame, text="â–¶", command=self.toggle_track_playback,width=3)
        self.track_play_button.pack(side=tk.LEFT, padx=2)

        # add field to display current playing time / duration of second track, and 2 buttons to move forward and backward seconds
        self.track_time_label = ttk.Label(self.track_frame, text="00:00 / 00:00")
        self.track_time_label.pack(side=tk.LEFT, padx=2)
        
        #ttk.Button(self.second_track_frame, text="â—€", command=self.move_second_track_backward, width=3).pack(side=tk.LEFT, padx=2)
        #ttk.Button(self.second_track_frame, text="â–¶", command=self.move_second_track_forward, width=3).pack(side=tk.LEFT, padx=2)
        ttk.Button(self.track_frame, text="ğŸ“º", command=lambda:self.pip_second_track(False), width=3).pack(side=tk.LEFT, padx=2)
        ttk.Button(self.track_frame, text="ğŸ“º", command=lambda:self.pip_second_track(True), width=3).pack(side=tk.LEFT, padx=2)
        ttk.Button(self.track_frame, text="ğŸ”„", command=self.reset_track_offset, width=3).pack(side=tk.LEFT, padx=2)
        ttk.Button(self.track_frame, text="ğŸ’«", command=lambda:self.choose_second_track('zero'), width=3).pack(side=tk.LEFT, padx=2)
        ttk.Button(self.track_frame, text="ğŸ’«", command=lambda:self.choose_second_track('one'), width=3).pack(side=tk.LEFT, padx=2)
        ttk.Button(self.track_frame, text="ğŸ’«", command=lambda:self.choose_second_track('second'), width=3).pack(side=tk.LEFT, padx=2)
        #ttk.Button(self.track_frame, text="ğŸ’«", command=self.swap_second, width=3).pack(side=tk.LEFT, padx=2)
        #ttk.Button(self.track_frame, text="âœ¨", command=self.swap_zero, width=3).pack(side=tk.LEFT, padx=2)
        #ttk.Button(self.track_frame, text="ğŸ”Š", command=self.pip_second_sound, width=3).pack(side=tk.LEFT, padx=2)
        ttk.Button(self.track_frame, text="â±",  command=self.track_recover, width=3).pack(side=tk.LEFT, padx=2)
        
        # æ·»åŠ éŸ³é‡æ§åˆ¶æ»‘å—ï¼ˆå…±ç”¨ï¼Œæ ¹æ®å½“å‰tabè‡ªåŠ¨é€‰æ‹©ï¼‰
        ttk.Label(self.track_frame, text="éŸ³é‡:").pack(side=tk.LEFT, padx=(10, 2))
        self.volume_scale = ttk.Scale(self.track_frame, from_=0.0, to=1.5, 
                                     variable=self.track_volume_var, orient=tk.HORIZONTAL, length=60)
        self.volume_scale.pack(side=tk.LEFT, padx=2)
        self.volume_label = ttk.Label(self.track_frame, text="0.2")
        self.volume_label.pack(side=tk.LEFT, padx=2)

        ttk.Button(self.track_frame, text="ã€Šã€Š", command=lambda: self.adjust_second_delta(-0.5), width=3).pack(side=tk.LEFT, padx=1)
        self.second_delta_label = ttk.Label(self.track_frame, text="0.0s", width=4)
        self.second_delta_label.pack(side=tk.LEFT, padx=1)
        ttk.Button(self.track_frame, text="ã€‹ã€‹", command=lambda: self.adjust_second_delta(0.25), width=3).pack(side=tk.LEFT, padx=1)

        
        # ç»‘å®šéŸ³é‡å˜åŒ–äº‹ä»¶æ¥æ›´æ–°æ ‡ç­¾
        self.track_volume_var.trace('w', self.on_track_volume_change)
        
        # åˆå§‹åŒ–æ‰€æœ‰è½¨é“æ’­æ”¾ç›¸å…³å˜é‡
        # å›¾ç‰‡é¢„è§ˆå¼•ç”¨ï¼ˆé˜²æ­¢åƒåœ¾å›æ”¶ï¼‰
        self._clip_image_photo = None
        self._second_image_photo = None
        self._zero_image_photo = None
        
        # ç»‘å®štabåˆ‡æ¢äº‹ä»¶
        self.second_notebook.bind("<<NotebookTabChanged>>", self.on_second_track_tab_changed)

        # å³ä¾§åŒºåŸŸï¼šè§†é¢‘ç”»å¸ƒå’Œæ§åˆ¶æŒ‰é’®
        right_frame = ttk.Frame(preview_frame)
        right_frame.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True, padx=(5, 0))
        
        # è§†é¢‘é¢„è§ˆç”»å¸ƒï¼ˆç”¨äºæ˜¾ç¤ºè§†é¢‘å¸§ï¼‰
        self.video_canvas = tk.Canvas(right_frame, bg='black', height=480)
        self.video_canvas.pack(fill=tk.BOTH, expand=True)
        
        # æ·»åŠ æ‹–æ‹½æç¤ºæ–‡æœ¬ï¼ˆä½ç½®ä¼šåœ¨canvasé…ç½®ååŠ¨æ€è°ƒæ•´ï¼‰
        self.video_canvas.create_text(400, 180, text="æ‹–æ‹½MP4æ–‡ä»¶åˆ°æ­¤å¤„å¯æ›¿æ¢å½“å‰è§†é¢‘ç‰‡æ®µ\n\næ³¨æ„ï¼š\nâ€¢ è¾“å…¥è§†é¢‘ä¸èƒ½è¶…è¿‡å½“å‰åœºæ™¯æ—¶é•¿\nâ€¢ å¦‚æœè¾“å…¥è§†é¢‘è¾ƒçŸ­ï¼Œä¼šè‡ªåŠ¨å»¶é•¿", 
                                    fill="gray", font=("Arial", 12), justify=tk.CENTER, tags="drag_hint")
        
        # ç»‘å®šé…ç½®äº‹ä»¶æ¥åŠ¨æ€è°ƒæ•´æç¤ºæ–‡æœ¬ä½ç½®
        self.video_canvas.bind('<Configure>', self.on_video_canvas_configure)
        
        # è§†é¢‘æ§åˆ¶æŒ‰é’®æ¡†æ¶ï¼ˆåœ¨è§†é¢‘ç”»å¸ƒä¸‹æ–¹ï¼‰
        video_control_frame = ttk.Frame(right_frame)
        video_control_frame.pack(fill=tk.X, pady=5)
        
        # æ’­æ”¾/æš‚åœæŒ‰é’®
        self.video_play_button = ttk.Button(video_control_frame, text="â–¶", 
                                          command=self.toggle_video_playback, width=3)
        self.video_play_button.pack(side=tk.LEFT, padx=1)
        
        # åœæ­¢æŒ‰é’®
        self.video_stop_button = ttk.Button(video_control_frame, text="â¹", 
                                          command=self.stop_video_playback, width=3)
        self.video_stop_button.pack(side=tk.LEFT, padx=1)

        # ç¿»è½¬æŒ‰é’®
        ttk.Button(video_control_frame, text="ã€Šã€Š", command=lambda: self.move_video(-0.25), width=3).pack(side=tk.LEFT, padx=1)
        self.playing_delta_label = ttk.Label(video_control_frame, text="0.0s", width=4)
        self.playing_delta_label.pack(side=tk.LEFT, padx=1)
        ttk.Button(video_control_frame, text="ã€‹ã€‹", command=lambda: self.move_video(0.25), width=3).pack(side=tk.LEFT, padx=1)

        separator = ttk.Separator(video_control_frame, orient='vertical')
        separator.pack(side=tk.LEFT, fill=tk.Y, padx=5)

        ttk.Button(video_control_frame, text="åˆ†ç¦»", command=self.split_scene, width=5).pack(side=tk.LEFT, padx=1) 
        ttk.Button(video_control_frame, text="ä¸‹ç§»", command=lambda: self.shift_scene(True), width=5).pack(side=tk.LEFT, padx=1)
        ttk.Button(video_control_frame, text="ä¸Šç§»", command=lambda: self.shift_scene(False), width=5).pack(side=tk.LEFT, padx=1)
        ttk.Button(video_control_frame, text="æ™ºåˆ†", command=self.split_smart_scene, width=5).pack(side=tk.LEFT, padx=1) 
        ttk.Button(video_control_frame, text="åˆ åˆ", command=self.merge_or_delete, width=5).pack(side=tk.LEFT, padx=1)

        separator = ttk.Separator(video_control_frame, orient='vertical')
        separator.pack(side=tk.LEFT, fill=tk.Y, padx=5)

        ttk.Button(video_control_frame, text="äº¤æ¢", command=self.swap_with_next_image, width=5).pack(side=tk.LEFT, padx=1)
        ttk.Button(video_control_frame, text="åè½¬", command=self.reverse_video, width=5).pack(side=tk.LEFT, padx=1)
        ttk.Button(video_control_frame, text="é•œåƒ", command=self.mirror_video, width=5).pack(side=tk.LEFT, padx=1)
        ttk.Button(video_control_frame, text="æ ‡é¢˜", command=self.print_title, width=5).pack(side=tk.LEFT, padx=1)
        #ttk.Button(video_control_frame, text="èƒŒèµ·", command=self.zero_start, width=5).pack(side=tk.LEFT, padx=1)
        #ttk.Button(video_control_frame, text="èƒŒç»§", command=self.zero_continue, width=5).pack(side=tk.LEFT, padx=1)
        #ttk.Button(video_control_frame, text="èƒŒç»ˆ", command=self.zero_end, width=5).pack(side=tk.LEFT, padx=1)

        # åˆ†éš”ç¬¦
        separator = ttk.Separator(video_control_frame, orient='vertical')
        separator.pack(side=tk.LEFT, fill=tk.Y, padx=5)

        # å­˜å‚¨æŒ‰é’®å¼•ç”¨ä»¥ä¾¿åç»­æ§åˆ¶çŠ¶æ€
        self.insert_scene_button = ttk.Button(video_control_frame, text="å‰æ’", command=self.insert_story_scene, width=6)
        self.insert_scene_button.pack(side=tk.LEFT, padx=1)

        self.append_scene_button = ttk.Button(video_control_frame, text="åæ’", command=self.append_scene, width=6)
        self.append_scene_button.pack(side=tk.LEFT, padx=1)

        # add 2 marks, to mark the current video progress secondsï¼Œã€€then add a button 'make_silence'ã€€to make the audio  period between mark1 mark2 be silient
        separator = ttk.Separator(video_control_frame, orient='vertical')
        separator.pack(side=tk.LEFT, fill=tk.Y, padx=5)
        
        ttk.Button(video_control_frame, text="é™éŸ³", command=self.make_silence_between_marks, width=6).pack(side=tk.LEFT, padx=1)
        # Mark buttons and labels
        ttk.Button(video_control_frame, text="M1", command=self.set_mark1, width=3).pack(side=tk.LEFT, padx=1)
        self.mark1_label = ttk.Label(video_control_frame, text="--:--.--", width=10)
        self.mark1_label.pack(side=tk.LEFT, padx=1)
        
        ttk.Button(video_control_frame, text="M2", command=self.set_mark2, width=3).pack(side=tk.LEFT, padx=1)
        self.mark2_label = ttk.Label(video_control_frame, text="--:--.--", width=10)
        self.mark2_label.pack(side=tk.LEFT, padx=1)

        # è§†é¢‘è¿›åº¦æ ‡ç­¾
        self.video_progress_label = ttk.Label(video_control_frame, text="00:00.00 / 00:00.00")
        self.video_progress_label.pack(side=tk.RIGHT, padx=1)
        
        # åˆå§‹åŒ–è§†é¢‘è¿›åº¦æ˜¾ç¤º
        self.update_video_progress_display()
        
        # è§†é¢‘æ’­æ”¾çŠ¶æ€
        self.video_playing = False
        self.video_cap = None
        self.video_after_id = None
        self.video_start_time = None
        self.video_pause_time = None  # è®°å½•æš‚åœæ—¶çš„ç´¯è®¡æ’­æ”¾æ—¶é—´
        
        # æ ‡è®°æ—¶é—´ç‚¹
        self.mark1_time = None
        self.mark2_time = None
        
        # å³ä¾§ï¼šåœºæ™¯ä¿¡æ¯æ˜¾ç¤ºåŒºåŸŸ
        self.video_edit_frame = ttk.LabelFrame(main_content, text="åœºæ™¯ä¿¡æ¯", padding=10)
        self.video_edit_frame.pack(side=tk.RIGHT, fill=tk.Y, padx=(5, 0))
        # è®¾ç½®å³ä¾§é¢æ¿çš„å›ºå®šå®½åº¦ï¼Œé˜²æ­¢è¢«æŒ¤å‹
        self.video_edit_frame.configure(width=650)
        self.video_edit_frame.pack_propagate(False)
        
        row_number = 1

        # æŒç»­æ—¶é—´å’Œå®£ä¼ æ¨¡å¼åœ¨åŒä¸€è¡Œ
        duration_promo_frame = ttk.Frame(self.video_edit_frame)
        duration_promo_frame.grid(row=row_number, column=0, columnspan=2, sticky=tk.W+tk.E, pady=2)
        row_number += 1

        # æŒç»­æ—¶é—´ï¼ˆåªè¯»ï¼‰
        ttk.Label(duration_promo_frame, text="æŒç»­:").pack(side=tk.LEFT)
        self.scene_duration = ttk.Entry(duration_promo_frame, width=12, state="readonly")
        self.scene_duration.pack(side=tk.LEFT, padx=(2, 10))
        
        # å®£ä¼ æ¨¡å¼ï¼ˆå¯ç¼–è¾‘ï¼‰
        ttk.Label(duration_promo_frame, text="ä¸»åŠ¨ç”»:").pack(side=tk.LEFT, padx=(5, 5))
        self.scene_main_animate = tk.StringVar(value="")
        self.main_animate_combobox = ttk.Combobox(duration_promo_frame, textvariable=self.scene_main_animate, 
                                               values=config_prompt.ANIMATE_SOURCE, 
                                               state="readonly", width=10)
        self.main_animate_combobox.pack(side=tk.LEFT)
        self.main_animate_combobox.bind('<<ComboboxSelected>>', self.on_video_clip_animation_change)


        ttk.Label(duration_promo_frame, text="æ¬¡åŠ¨ç”»:").pack(side=tk.LEFT, padx=(0, 5))
        self.second_animation_combobox = ttk.Combobox(duration_promo_frame, textvariable=self.scene_second_animation,
                                               values=config_prompt.ANIMATE_SOURCE, 
                                               state="readonly", width=10)
        self.second_animation_combobox.pack(side=tk.LEFT, padx=(0, 10))
        self.second_animation_combobox.bind('<<ComboboxSelected>>', self.on_image_type_change)

        # ç±»å‹ã€æƒ…ç»ªã€åŠ¨ä½œé€‰æ‹©ï¼ˆåœ¨åŒä¸€è¡Œï¼‰
        type_mood_action_frame = ttk.Frame(self.video_edit_frame)
        type_mood_action_frame.grid(row=row_number, column=0, columnspan=2, sticky=tk.W+tk.E, pady=2)
        row_number += 1

        ttk.Button(type_mood_action_frame, text="è§†è§‰æç¤º", width=10, command=lambda: self.recreate_clip_image("en")).pack(side=tk.LEFT)
        ttk.Button(type_mood_action_frame, text="ç”Ÿè§†è§‰åŒ–", width=10, command=lambda: self.refresh_scene_visual()).pack(side=tk.LEFT)
        #ttk.Button(action_frame, text="ç”Ÿä¸»å›¾-è‹±", width=10, command=lambda: self.recreate_clip_image("en", True)).pack(side=tk.LEFT, padx=2)
        #ttk.Button(action_frame, text="ç”Ÿæ¬¡å›¾-ä¸­", width=8, command=lambda: self.recreate_clip_image("zh", False)).pack(side=tk.LEFT, padx=2)
        #ttk.Button(action_frame, text="ç”Ÿæ¬¡å›¾-è‹±", width=8, command=lambda: self.recreate_clip_image("en", False)).pack(side=tk.LEFT, padx=2)
        ttk.Button(type_mood_action_frame, text="ç”ŸåœºéŸ³é¢‘", width=10, command=lambda: self.regenerate_audio()).pack(side=tk.LEFT)
        ttk.Button(type_mood_action_frame, text="ç”Ÿä¸»åŠ¨ç”»", width=10, command=lambda: self.regenerate_video("clip")).pack(side=tk.LEFT)
        ttk.Button(type_mood_action_frame, text="ç”Ÿæ¬¡åŠ¨ç”»", width=10, command=lambda: self.regenerate_video(None)).pack(side=tk.LEFT)


        action_frame = ttk.Frame(self.video_edit_frame)
        action_frame.grid(row=row_number, column=0, columnspan=2, sticky=tk.W+tk.E, pady=2)
        row_number += 1

        ttk.Button(action_frame, text="å¢ä¸»è½¨", width=10, command=lambda: self.enhance_clip(True, False)).pack(side=tk.LEFT)
        ttk.Button(action_frame, text="å¢æ¬¡è½¨", width=10, command=lambda: self.enhance_clip(False, False)).pack(side=tk.LEFT)

        # add a choice list to choose the enhance level, values are from config.FACE_ENHANCE, default value to "0"
        FACE_ENHANCE = ["0", "15", "30", "60"]
        self.enhance_level = ttk.Combobox(action_frame, width=5, values=FACE_ENHANCE)
        self.enhance_level.pack(side=tk.LEFT, padx=2)
        self.enhance_level.set("30")

        ttk.Button(action_frame, text="æ’ä¸»è½¨", width=10, command=lambda: self.enhance_clip(True, True)).pack(side=tk.LEFT)
        ttk.Button(action_frame, text="æ’æ¬¡è½¨", width=10, command=lambda: self.enhance_clip(False, True)).pack(side=tk.LEFT)
        #RIFE_EXP = ["0", "1", "2"]
        #self.rife_exp = ttk.Combobox(action_frame, width=5, values=RIFE_EXP)
        #self.rife_exp.pack(side=tk.LEFT, padx=2)
        #self.rife_exp.set("0")

        ttk.Label(self.video_edit_frame, text="å†…å®¹:").grid(row=row_number, column=0, sticky=tk.NW, pady=2)
        self.scene_story_content = scrolledtext.ScrolledText(self.video_edit_frame, width=35, height=2)
        self.scene_story_content.grid(row=row_number, column=1, sticky=tk.W, padx=5, pady=2)
        row_number += 1

        # add the text field to show the kernel
        ttk.Label(self.video_edit_frame, text="æ ¸å¿ƒ:").grid(row=row_number, column=0, sticky=tk.NW, pady=2)
        self.scene_kernel = scrolledtext.ScrolledText(self.video_edit_frame, width=35, height=2)
        self.scene_kernel.grid(row=row_number, column=1, sticky=tk.W, padx=5, pady=2)
        row_number += 1

        # add the text field to show the kernel
        ttk.Label(self.video_edit_frame, text="æ•…äº‹:").grid(row=row_number, column=0, sticky=tk.NW, pady=2)
        self.scene_story = scrolledtext.ScrolledText(self.video_edit_frame, width=35, height=2)
        self.scene_story.grid(row=row_number, column=1, sticky=tk.W, padx=5, pady=2)
        row_number += 1

        ttk.Label(self.video_edit_frame, text="ä¸»ä½“:").grid(row=row_number, column=0, sticky=tk.NW, pady=2)
        self.scene_subject = scrolledtext.ScrolledText(self.video_edit_frame, width=35, height=2)
        self.scene_subject.grid(row=row_number, column=1, sticky=tk.W, padx=5, pady=2)
        row_number += 1
        
        ttk.Label(self.video_edit_frame, text="å¼€åœº:").grid(row=row_number, column=0, sticky=tk.NW, pady=2)
        self.scene_visual_image = scrolledtext.ScrolledText(self.video_edit_frame, width=35, height=2)
        self.scene_visual_image.grid(row=row_number, column=1, sticky=tk.W, padx=5, pady=2)
        row_number += 1

        ttk.Label(self.video_edit_frame, text="ç»“æŸ:").grid(row=row_number, column=0, sticky=tk.NW, pady=2)
        self.scene_person_action = scrolledtext.ScrolledText(self.video_edit_frame, width=35, height=2)
        self.scene_person_action.grid(row=row_number, column=1, sticky=tk.W, padx=5, pady=2)
        row_number += 1

        ttk.Label(self.video_edit_frame, text="æ—¶ä»£:").grid(row=row_number, column=0, sticky=tk.NW, pady=2)
        self.scene_era_time = scrolledtext.ScrolledText(self.video_edit_frame, width=35, height=1)
        self.scene_era_time.grid(row=row_number, column=1, sticky=tk.W, padx=5, pady=2)
        row_number += 1
        
        ttk.Label(self.video_edit_frame, text="ç¯å¢ƒ:").grid(row=row_number, column=0, sticky=tk.NW, pady=2)
        self.scene_environment = ttk.Entry(self.video_edit_frame, width=35)
        self.scene_environment.grid(row=row_number, column=1, sticky=tk.W, padx=5, pady=2)
        row_number += 1
        
        ttk.Label(self.video_edit_frame, text="æ‘„å½±:").grid(row=row_number, column=0, sticky=tk.NW, pady=2)
        self.scene_cinematography = scrolledtext.ScrolledText(self.video_edit_frame, width=35, height=2)
        self.scene_cinematography.grid(row=row_number, column=1, sticky=tk.W, padx=5, pady=2)
        row_number += 1
        
        ttk.Label(self.video_edit_frame, text="éŸ³æ•ˆ:").grid(row=row_number, column=0, sticky=tk.NW, pady=2)
        self.scene_sound_effect = scrolledtext.ScrolledText(self.video_edit_frame, width=35, height=2)
        self.scene_sound_effect.grid(row=row_number, column=1, sticky=tk.W, padx=5, pady=2)
        row_number += 1

        ttk.Label(self.video_edit_frame, text="FYI:").grid(row=row_number, column=0, sticky=tk.NW, pady=2)
        self.scene_extra =  scrolledtext.ScrolledText(self.video_edit_frame, width=35, height=2)
        self.scene_extra.grid(row=row_number, column=1, sticky=tk.W, padx=5, pady=2)
        row_number += 1

        ttk.Label(self.video_edit_frame, text="è®²å‘˜:").grid(row=row_number, column=0, sticky=tk.NW, pady=2)
        self.scene_speaker_action = scrolledtext.ScrolledText(self.video_edit_frame, width=35, height=2)
        self.scene_speaker_action.grid(row=row_number, column=1, sticky=tk.W, padx=5, pady=2)
        row_number += 1

        ttk.Label(self.video_edit_frame, text="æƒ…ç»ª:").grid(row=row_number, column=0, sticky=tk.NW, pady=2)
        self.scene_mood = ttk.Combobox(self.video_edit_frame, width=35, values=EXPRESSION_STYLES, state="readonly")
        self.scene_mood.set("calm")  # è®¾ç½®é»˜è®¤å€¼
        self.scene_mood.grid(row=row_number, column=1, sticky=tk.W, padx=5, pady=2)
        row_number += 1

        ttk.Label(self.video_edit_frame, text="è®²å‘˜:").grid(row=row_number, column=0, sticky=tk.NW, pady=2)
        self.scene_speaker = ttk.Combobox(self.video_edit_frame, width=32, values=config_prompt.ROLES)
        self.scene_speaker.grid(row=row_number, column=1, sticky=tk.W, padx=5, pady=2)
        row_number += 1

        ttk.Label(self.video_edit_frame, text="å·¦å³:").grid(row=row_number, column=0, sticky=tk.NW, pady=2)
        self.scene_speaker_position = ttk.Combobox(self.video_edit_frame, width=32, values=config_prompt.SPEAKER_POSITIONS)
        self.scene_speaker_position.grid(row=row_number, column=1, sticky=tk.W, padx=5, pady=2)
        row_number += 1

        # add a choice list to choose font of the title, values are from config.FONT_LIST(choose from all languages, show language name in choice, keep value), default value to self.workflow.font_video
        ttk.Label(self.video_edit_frame, text="å­—ä½“:").grid(row=row_number, column=0, sticky=tk.NW, pady=2)
        self.scene_language = ttk.Combobox(self.video_edit_frame, width=32, values=list(config.FONT_LIST.keys()))
        self.scene_language.grid(row=row_number, column=1, sticky=tk.W, padx=5, pady=2)
        row_number += 1
        self.scene_language.set(self.shared_language.cget('text'))

        # add a text field "promotion info" here, default empty, if enter text, then need to save to current scene["promotion"] 
        ttk.Label(self.video_edit_frame, text="ä¿¡æ¯:").grid(row=row_number, column=0, sticky=tk.NW, pady=2)
        self.scene_promotion = scrolledtext.ScrolledText(self.video_edit_frame, width=35, height=2)
        self.scene_promotion.grid(row=row_number, column=1, sticky=tk.W, padx=5, pady=2)
        row_number += 1

        # ç¬¬äºŒè½¨é“æ’­æ”¾çŠ¶æ€
        self.second_track_playing = False
        self.second_track_cap = None
        self.second_track_after_id = None
        
        # ç¬¬äºŒè½¨é“éŸ³é¢‘æ’­æ”¾çŠ¶æ€
        self.second_track_audio_playing = False
        self.second_track_audio_start_time = None
        
        # ç¬¬äºŒè½¨é“æš‚åœä½ç½®
        self.second_track_paused_time = None
        self.second_track_paused_audio_time = None
        self.second_track_cap = None
        self.second_track_after_id = None
        self.second_track_start_time = None

        self.second_track_playing = False
        self.second_track_offset = 0.0
        self.second_track_end_time = 0.0
        self.selected_second_track = "second"
        
        # PIP L/R (ç”»ä¸­ç”»å·¦å³)
        self.pip_lr_playing = False
        self.pip_left_cap = None
        self.pip_right_cap = None
        self.pip_lr_after_id = None
        self.pip_lr_start_time = None
        self.pip_lr_paused_time = None
        
        self.track_time_label.config(text="00:00 / 00:00")

        # åº•éƒ¨ï¼šæ—¥å¿—åŒºåŸŸ
        log_frame = ttk.LabelFrame(tab, text="æ“ä½œæ—¥å¿—", padding=10)
        log_frame.pack(fill=tk.X, padx=10, pady=5)
        
        self.video_output = scrolledtext.ScrolledText(log_frame, height=6)
        self.video_output.pack(fill=tk.BOTH, expand=True)
        
        # ç»‘å®šé…ç½®å˜åŒ–äº‹ä»¶
        # ç»‘å®šç¼–è¾‘äº‹ä»¶
        self.bind_edit_events()
        self.bind_config_change_events()


    def log_to_output(self, output_widget, message):
        """å‘è¾“å‡ºæ§ä»¶å†™å…¥æ—¥å¿—ä¿¡æ¯"""
        if output_widget and hasattr(output_widget, 'insert'):
            timestamp = datetime.now().strftime("%H:%M:%S")
            output_widget.insert(tk.END, f"[{timestamp}] {message}\n")
            output_widget.see(tk.END)
            output_widget.update_idletasks()


    def start_status_update_timer(self):
        """å¯åŠ¨çŠ¶æ€æ›´æ–°å®šæ—¶å™¨"""
        # å¦‚æœå·²æœ‰å®šæ—¶å™¨ï¼Œå…ˆå–æ¶ˆ
        if self.status_update_timer_id is not None:
            self.root.after_cancel(self.status_update_timer_id)
        
        self.update_status_and_check_completion()
        # æ¯5ç§’æ›´æ–°ä¸€æ¬¡çŠ¶æ€ï¼Œå¹¶ä¿å­˜å®šæ—¶å™¨ID
        self.status_update_timer_id = self.root.after(5000, self.start_status_update_timer)


    def update_status_and_check_completion(self):
        """æ›´æ–°çŠ¶æ€å¹¶æ£€æŸ¥ä»»åŠ¡å®Œæˆæƒ…å†µ"""
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å®Œæˆçš„ä»»åŠ¡
        newly_completed = []
        for task_id, task_info in list(self.tasks.items()):
            if task_info["status"] in ["å®Œæˆ", "å¤±è´¥"] and task_id not in self.last_notified_tasks:
                newly_completed.append((task_id, task_info))
                self.last_notified_tasks.add(task_id)
                
                # å°†å®Œæˆçš„ä»»åŠ¡ç§»åˆ°å®Œæˆåˆ—è¡¨
                self.completed_tasks.append({
                    "id": task_id,
                    "info": task_info.copy(),
                    "completion_time": datetime.now()
                })
        
        # é€šçŸ¥æ–°å®Œæˆçš„ä»»åŠ¡
        for task_id, task_info in newly_completed:
            """é€šçŸ¥ä»»åŠ¡å®Œæˆ"""
            task_type = task_info.get("type", "æœªçŸ¥ä»»åŠ¡")
            task_status = task_info.get("status", "æœªçŸ¥çŠ¶æ€")
            pid = task_info.get("pid", "")
            
            if task_status == "å®Œæˆ":
                title = "âœ… ä»»åŠ¡å®Œæˆ"
                message = f"ä»»åŠ¡ç±»å‹: {task_type}\né¡¹ç›®ID: {pid}\nçŠ¶æ€: æˆåŠŸå®Œæˆ"
                if "result" in task_info:
                    message += f"\nç»“æœ: {task_info['result']}"
            else:
                title = "âŒ ä»»åŠ¡å¤±è´¥"
                message = f"ä»»åŠ¡ç±»å‹: {task_type}\né¡¹ç›®ID: {pid}\nçŠ¶æ€: æ‰§è¡Œå¤±è´¥"
                if "error" in task_info:
                    message += f"\né”™è¯¯: {task_info['error']}"
            
            # æ˜¾ç¤ºé€šçŸ¥å¯¹è¯æ¡†
            messagebox.showinfo(title, message)



        
        # æ£€æŸ¥ç”Ÿæˆçš„è§†é¢‘ï¼ˆåå°æŒç»­æ£€æŸ¥ï¼‰
        self.check_generated_videos_background()


    def start_video_check_thread(self):
        if not hasattr(self, 'workflow'):
            print("âš ï¸ å·¥ä½œæµå®ä¾‹æœªåˆ›å»º")
            return

        if self.video_check_running:
            print("âš ï¸ åå°æ£€æŸ¥çº¿ç¨‹å·²åœ¨è¿è¡Œ")
            return
        
        self.video_check_running = True
        self.video_check_stop_event.clear()
        
        def video_check_loop():
            """å•ä¾‹åå°çº¿ç¨‹çš„ä¸»å¾ªç¯"""
            print("ğŸš€ å¯åŠ¨åå°è§†é¢‘æ£€æŸ¥çº¿ç¨‹")
            
            while not self.video_check_stop_event.is_set():
                try:
                    self._perform_video_check()
                except Exception as e:
                    print(f"âŒ åå°æ£€æŸ¥çº¿ç¨‹å‡ºé”™: {str(e)}")
                # å‡ºé”™åç­‰å¾…5ç§’å†ç»§ç»­
                self.video_check_stop_event.wait(5)
            
            print("ğŸ›‘ åå°è§†é¢‘æ£€æŸ¥çº¿ç¨‹å·²åœæ­¢")
            self.video_check_running = False
        
        # åˆ›å»ºå¹¶å¯åŠ¨daemonçº¿ç¨‹
        self.video_check_thread = threading.Thread(target=video_check_loop, daemon=True)
        self.video_check_thread.start()
    

    def stop_video_check_thread(self):
        """åœæ­¢åå°è§†é¢‘æ£€æŸ¥çº¿ç¨‹"""
        if self.video_check_running:
            print("ğŸ›‘ æ­£åœ¨åœæ­¢åå°è§†é¢‘æ£€æŸ¥çº¿ç¨‹...")
            self.video_check_stop_event.set()
            if self.video_check_thread:
                self.video_check_thread.join(timeout=2)
    

    def _perform_video_check(self):
        """æ‰§è¡Œè§†é¢‘æ£€æŸ¥ä»»åŠ¡ï¼ˆç”±å•ä¾‹çº¿ç¨‹è°ƒç”¨ï¼‰"""
        animate_gen_list = []
        for scene_index, scene in enumerate(self.workflow.scenes):
            #clip_animation = scene.get("clip_animation", "")
            #if clip_animation in config_prompt.ANIMATE_SOURCE and clip_animation != "":
            scene_name = build_scene_media_prefix(self.workflow.pid, str(scene["id"]), "clip", "", False)
            animate_gen_list.append((scene_name, "clip", scene))
            #second_animation = scene.get("second_animation", "")
            #if second_animation in config_prompt.ANIMATE_SOURCE and second_animation != "":
            scene_name = build_scene_media_prefix(self.workflow.pid, str(scene["id"]), "second", "", False)
            animate_gen_list.append((scene_name, "second", scene))

        if animate_gen_list == []:
            return
        
        try:
            # 1. æ£€æŸ¥ X:\output ä¸­æ–°ç”Ÿæˆçš„åŸå§‹è§†é¢‘ï¼ˆç›‘æ§é€»è¾‘ï¼‰
            self.media_scanner.scanning("X:\\output", config.BASE_MEDIA_PATH+"\\input_mp4")                      # clip_p202512231259_10005_S2V__00003-audio.mp4
            self.media_scanner.scanning("Z:\\wan_video\\output_mp4", config.BASE_MEDIA_PATH+"\\input_mp4")                     # clip_p202512231259_10005_INT_25115141_30__00001.mp4  ~~~ interpolate
            self.media_scanner.scanning("W:\\wan_video\\output_mp4", config.BASE_MEDIA_PATH+"\\input_mp4")      # clip_p20251208_10708_ENH_13231028_0_.mp4   clip_p202512231259_10005_EHN_.mp4  ~~~ enhance

            self.media_scanner.check_gen_video(config.BASE_MEDIA_PATH+"\\input_mp4", animate_gen_list)                 # clip_p202512231259_10005_S2V_23155421.mp4
            #self.media_scanner.scanning("Y:\\output", config.BASE_MEDIA_PATH+"\\input_mp4")

            self.workflow.save_scenes_to_json()

        except Exception as e:
            # å¿½ç•¥å•ä¸ªåœºæ™¯çš„é”™è¯¯ï¼Œç»§ç»­æ£€æŸ¥å…¶ä»–åœºæ™¯
            print(f"âŒ åå°æ£€æŸ¥çº¿ç¨‹å‡ºé”™: {str(e)}")
            pass


    def check_generated_videos_background(self):
        """å®šæ—¶å™¨è°ƒç”¨æ­¤æ–¹æ³•ï¼Œä½†ä¸å†åˆ›å»ºæ–°çº¿ç¨‹ï¼ˆå•ä¾‹çº¿ç¨‹å·²åœ¨è¿è¡Œï¼‰"""
        # æ£€æŸ¥å•ä¾‹çº¿ç¨‹æ˜¯å¦è¿˜åœ¨è¿è¡Œï¼Œå¦‚æœæ²¡æœ‰åˆ™é‡å¯
        if not self.video_check_running or not self.video_check_thread or not self.video_check_thread.is_alive():
            print("âš ï¸ æ£€æµ‹åˆ°åå°çº¿ç¨‹æœªè¿è¡Œï¼Œæ­£åœ¨é‡å¯...")
            self.start_video_check_thread()
    
    
    def run_promotion_video(self):
        pid = self.get_pid()
        task_id = str(uuid.uuid4())
        self.tasks[task_id] = {
            "type": "promotion_video",
            "status": "è¿è¡Œä¸­",
            "start_time": datetime.now(),
            "pid": pid
        }
        def run_task():
            try:
                self.workflow.promotion_video(self.video_title.get().strip())
                self.log_to_output(self.video_output, "âœ… æœ€ç»ˆè§†é¢‘ç”Ÿæˆå®Œæˆï¼")
                self.tasks[task_id]["status"] = "å®Œæˆ"
            except Exception as e:
                self.log_to_output(self.video_output, f"âŒ æœ€ç»ˆè§†é¢‘ç”Ÿæˆå¤±è´¥: {str(e)}")
                self.tasks[task_id]["status"] = "å¤±è´¥"
                self.tasks[task_id]["error"] = str(e)

        threading.Thread(target=run_task, daemon=True).start()


    def run_finalize_video(self):
        pid = self.get_pid()
        task_id = str(uuid.uuid4())
        self.tasks[task_id] = {
            "type": "video_finalize",
            "status": "è¿è¡Œä¸­",
            "start_time": datetime.now(),
            "pid": pid
        }

        def run_task():
            try:
                self.workflow.finalize_video(self.video_title.get().strip(), False)
                self.log_to_output(self.video_output, "âœ… æœ€ç»ˆè§†é¢‘ç”Ÿæˆå®Œæˆï¼")
                self.tasks[task_id]["status"] = "å®Œæˆ"
            except Exception as e:
                self.log_to_output(self.video_output, f"âŒ æœ€ç»ˆè§†é¢‘ç”Ÿæˆå¤±è´¥: {str(e)}")
                self.tasks[task_id]["status"] = "å¤±è´¥"
                self.tasks[task_id]["error"] = str(e)

        threading.Thread(target=run_task, daemon=True).start()


    def run_upload_video(self):
        """ä¸Šä¼ è§†é¢‘åˆ°YouTubeï¼ˆæˆ–å…¶ä»–å¹³å°ï¼‰"""
        pid = self.get_pid()
        title = self.video_title.get().strip()

        if not pid:
            messagebox.showerror("é”™è¯¯", "è¯·è¾“å…¥é¡¹ç›®ID")
            return

        task_id = str(uuid.uuid4())
        self.tasks[task_id] = {
            "type": "upload_video",
            "status": "è¿è¡Œä¸­",
            "start_time": datetime.now(),
            "pid": pid
        }

        def run_task():
            try:
                self.log_to_output(self.video_output, f"å¼€å§‹ä¸Šä¼ è§†é¢‘ - PID: {pid}")
                workflow = self.workflow
                if workflow is None:
                    raise Exception("æ— æ³•è·å–å·¥ä½œæµå¯¹è±¡")

                workflow.upload_video(title)
                self.log_to_output(self.video_output, "âœ… è§†é¢‘ä¸Šä¼ å®Œæˆï¼")
                self.tasks[task_id]["status"] = "å®Œæˆ"
            except Exception as e:
                self.log_to_output(self.video_output, f"âŒ è§†é¢‘ä¸Šä¼ å¤±è´¥: {str(e)}")
                self.tasks[task_id]["status"] = "å¤±è´¥"
                self.tasks[task_id]["error"] = str(e)

        threading.Thread(target=run_task, daemon=True).start()


    def _cleanup_video_before_switch(self):
        """åˆ‡æ¢åœºæ™¯å‰æ¸…ç†è§†é¢‘èµ„æº"""
        # åœæ­¢è§†é¢‘æ’­æ”¾
        if self.video_playing:
            self.stop_video_playback()
        
        # æ¸…ç†è§†é¢‘æ•è·å¯¹è±¡
        if self.video_cap:
            self.video_cap.release()
            self.video_cap = None
        
        # å–æ¶ˆå®šæ—¶å™¨
        if self.video_after_id:
            self.root.after_cancel(self.video_after_id)
            self.video_after_id = None
        
        # åœæ­¢éŸ³é¢‘
        self.stop_audio_playback()
        
        # é‡ç½®æ’­æ”¾çŠ¶æ€
        self.video_playing = False
        self.video_play_button.config(text="â–¶")
        
        # æ›´æ–°è§†é¢‘è¿›åº¦æ˜¾ç¤º
        self.update_video_progress_display()
        
        # æ¸…ç©ºç”»å¸ƒ
        self.video_canvas.delete("all")
        
        # é‡ç½®è§†é¢‘ç›¸å…³å˜é‡
        self.video_start_time = None
        self.video_pause_time = None
        
        # æ¸…ç†å›¾ç‰‡å¼•ç”¨ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
        if hasattr(self, 'current_video_frame'):
            self.current_video_frame = None


    def clear_video_scene_fields(self):
        self.scene_duration.config(state="normal")
        self.scene_duration.delete(0, tk.END)
        self.scene_duration.config(state="readonly")
        
        self.clear_video_preview()


    def load_video_first_frame(self):
        self._cleanup_video_before_switch()

        current_scene = self.get_current_scene()
            
        video_path = get_file_path(current_scene, "clip")
        if not video_path:
            return

        if not video_path:
            self.clear_video_preview()
            return
            
        try:
            self.video_canvas.delete("all")
            
            cap = cv2.VideoCapture(video_path)
            
            if not cap.isOpened():
                cap.release()
                self.clear_video_preview()
                return
            
            ret, frame = cap.read()
            cap.release()
            
            if ret and frame is not None:
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                pil_image = Image.fromarray(frame_rgb)
                
                canvas_width = self.video_canvas.winfo_width()
                canvas_height = self.video_canvas.winfo_height()
                
                if canvas_width > 1 and canvas_height > 1:  # ç¡®ä¿ç”»å¸ƒå·²ç»åˆå§‹åŒ–
                    pil_image.thumbnail((canvas_width - 10, canvas_height - 10), Image.Resampling.LANCZOS)
                else:
                    pil_image.thumbnail((630, 350), Image.Resampling.LANCZOS)
                
                # è½¬æ¢ä¸ºTkinterå¯ç”¨çš„æ ¼å¼
                self.current_video_frame = ImageTk.PhotoImage(pil_image)
                
                # åœ¨ç”»å¸ƒä¸­å¤®æ˜¾ç¤ºå›¾åƒ
                self.video_canvas.delete("all")
                canvas_width = self.video_canvas.winfo_width() or 640
                canvas_height = self.video_canvas.winfo_height() or 360
                x = canvas_width // 2
                y = canvas_height // 2
                
                # ç¡®ä¿å›¾åƒå¯¹è±¡å­˜åœ¨åå†åˆ›å»ºç”»å¸ƒå›¾åƒ
                if self.current_video_frame:
                    try:
                        self.video_canvas.create_image(x, y, anchor=tk.CENTER, image=self.current_video_frame)
                    except tk.TclError as e:
                        # å¦‚æœå›¾åƒå¯¹è±¡æ— æ•ˆï¼Œé‡æ–°åˆ›å»º
                        print(f"âš ï¸ å›¾åƒå¯¹è±¡æ— æ•ˆï¼Œé‡æ–°åˆ›å»º: {e}")
                        self.current_video_frame = ImageTk.PhotoImage(pil_image)
                        self.video_canvas.create_image(x, y, anchor=tk.CENTER, image=self.current_video_frame)
                
                self.video_canvas.create_text(x, y + pil_image.height//2 + 20, 
                                            text="ç‚¹å‡» 'â–¶ æ’­æ”¾' å¼€å§‹æ’­æ”¾è§†é¢‘", 
                                            fill="white", font=("Arial", 12))
                
                self.video_canvas.create_text(x, y + pil_image.height//2 + 40, 
                                            text="ğŸ’¡ æ‹–æ‹½MP4æ–‡ä»¶å¯æ›¿æ¢æ­¤è§†é¢‘", 
                                            fill="gray", font=("Arial", 10))
            else:
                self.clear_video_preview()
                self.log_to_output(self.video_output, f"âŒ æ— æ³•è¯»å–è§†é¢‘ç¬¬ä¸€å¸§")
                
        except Exception as e:
            self.clear_video_preview()
            self.log_to_output(self.video_output, f"âŒ åŠ è½½è§†é¢‘é¢„è§ˆå¤±è´¥: {str(e)}")


    def clear_video_preview(self):
        """æ¸…ç©ºè§†é¢‘é¢„è§ˆ"""
        # å…ˆæ¸…ç†å›¾ç‰‡å¼•ç”¨ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
        if hasattr(self, 'current_video_frame'):
            self.current_video_frame = None
        
        # æ¸…ç©ºç”»å¸ƒ
        self.video_canvas.delete("all")
        
        # æ˜¾ç¤ºæç¤ºæ–‡æœ¬
        canvas_width = self.video_canvas.winfo_width() or 640
        canvas_height = self.video_canvas.winfo_height() or 360
        x = canvas_width // 2
        y = canvas_height // 2
        
        self.video_canvas.create_text(x, y, text="é€‰æ‹©åœºæ™¯åä¼šæ˜¾ç¤ºè§†é¢‘é¢„è§ˆ\n\nğŸ’¡ å¯ä»¥æ‹–æ‹½MP4æ–‡ä»¶åˆ°æ­¤å¤„æ›¿æ¢è§†é¢‘ç‰‡æ®µ", fill="white", 
                                    font=("Arial", 12), justify=tk.CENTER, tags="no_video_hint")


    def toggle_video_playback(self):
        current_scene = self.get_current_scene()
        video_path = None
        if current_scene:
            video_path = get_file_path(current_scene, "clip")
            
        if not video_path:
            self.log_to_output(self.video_output, "âŒ æ²¡æœ‰å¯æ’­æ”¾çš„è§†é¢‘æ–‡ä»¶")
            return
            
        if self.video_playing:
            self.pause_video()
        else:
            # å¦‚æœæ˜¯ä»æš‚åœçŠ¶æ€æ¢å¤ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
            if self.video_cap is not None:
                self.video_playing = True
                self.video_play_button.config(text="â¸")
                # é‡æ–°è®¾ç½®å¼€å§‹æ—¶é—´ï¼Œè€ƒè™‘ä¹‹å‰æš‚åœçš„æ—¶é—´
                self.video_start_time = time.time()
                self.resume_audio_playback()
                print(f"â–¶ï¸ æ¢å¤æ’­æ”¾ï¼Œå·²æ’­æ”¾æ—¶é—´: {self.video_pause_time or 0:.2f}ç§’")
                self.play_next_frame()
            else:
                self.play_video()


    def play_video(self):
        """æ’­æ”¾è§†é¢‘"""
        current_scene = self.get_current_scene()
        video_path = None
        if current_scene:
            video_path = get_file_path(current_scene, "clip")
            
        if not video_path:
            return

        if self.video_cap is None:
            self.video_cap = cv2.VideoCapture(video_path)
            
        if not self.video_cap.isOpened():
            self.log_to_output(self.video_output, "âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
            return
            
        self.video_playing = True
        self.video_play_button.config(text="â¸")
        
        # è®°å½•æ’­æ”¾å¼€å§‹æ—¶é—´ï¼Œé‡ç½®æš‚åœæ—¶é—´
        self.video_start_time = time.time()
        self.video_pause_time = None  # é‡ç½®æš‚åœæ—¶é—´
        
        # å¼€å§‹æ’­æ”¾éŸ³é¢‘ï¼ˆå¦‚æœæœ‰ï¼‰
        self.start_audio_playback()
        
        self.play_next_frame()


    def start_audio_playback(self):
        clip = get_file_path(self.get_current_scene(), "clip_audio")
        if not clip:
            return
        pygame.mixer.music.load(clip)
        pygame.mixer.music.play()

    def pause_audio_playback(self):
        pygame.mixer.music.pause()

    def resume_audio_playback(self):
        pygame.mixer.music.unpause()

    def stop_audio_playback(self):
        pygame.mixer.music.stop()
    

    def pause_video(self):
        """æš‚åœè§†é¢‘"""
        self.video_playing = False
        self.video_play_button.config(text="â–¶")
        if self.video_after_id:
            self.root.after_cancel(self.video_after_id)
            self.video_after_id = None
        
        # è®°å½•æš‚åœæ—¶å·²æ’­æ”¾çš„æ—¶é—´
        if self.video_start_time:
            elapsed = time.time() - self.video_start_time
            self.video_pause_time = (self.video_pause_time or 0) + elapsed
            
        # æš‚åœéŸ³é¢‘
        self.pause_audio_playback()
        print(f"â¸ï¸ è§†é¢‘æš‚åœï¼Œæ€»æ’­æ”¾æ—¶é—´: {self.video_pause_time or 0:.2f}ç§’")

    def stop_video_playback(self):
        """åœæ­¢è§†é¢‘æ’­æ”¾"""
        self.video_playing = False
        self.video_play_button.config(text="â–¶")
        
        if self.video_after_id:
            self.root.after_cancel(self.video_after_id)
            self.video_after_id = None
            
        if self.video_cap:
            self.video_cap.release()
            self.video_cap = None
            
        # åœæ­¢éŸ³é¢‘
        self.stop_audio_playback()
            
        # é‡ç½®æ—¶é—´ç›¸å…³å˜é‡
        self.video_start_time = None
        self.video_pause_time = None
            
        self.refresh_gui_scenes()


    def play_next_frame(self):
        """æ’­æ”¾ä¸‹ä¸€å¸§"""
        if not self.video_playing or not self.video_cap:
            return
        
        # é¦–å…ˆæ£€æŸ¥éŸ³é¢‘æ˜¯å¦è¿˜åœ¨æ’­æ”¾
        audio_is_playing = pygame.mixer.music.get_busy()
        if not audio_is_playing:
            # éŸ³é¢‘æ’­æ”¾å®Œæ¯•ï¼Œåœæ­¢è§†é¢‘
            self.stop_video_playback()
            self.log_to_output(self.video_output, "âœ… éŸ³é¢‘æ’­æ”¾å®Œæ¯•ï¼Œè§†é¢‘åŒæ­¥åœæ­¢")
            return
            
        # è®¡ç®—åº”è¯¥æ’­æ”¾çš„å¸§ä½ç½®ä»¥ä¿æŒä¸éŸ³é¢‘åŒæ­¥
        total_frames = self.video_cap.get(cv2.CAP_PROP_FRAME_COUNT)
        
        if self.video_start_time:
            # è®¡ç®—å®é™…ç»è¿‡çš„æ—¶é—´
            elapsed_time = time.time() - self.video_start_time
            current_time = elapsed_time + (self.video_pause_time or 0)
            
            # è®¡ç®—åº”è¯¥åœ¨ç¬¬å‡ å¸§ (æ­£å¸¸1å€é€Ÿæ’­æ”¾)
            target_frame = int(current_time * STANDARD_FPS)
            current_frame = int(self.video_cap.get(cv2.CAP_PROP_POS_FRAMES))
            
            # å¦‚æœè§†é¢‘å¸§è½åäºéŸ³é¢‘è¿›åº¦ï¼Œè·³å¸§è¿½èµ¶
            if target_frame > current_frame + 2:  # å…è®¸2å¸§çš„å®¹é”™
                self.video_cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)
        
        ret, frame = self.video_cap.read()
        
        if ret:
            # è½¬æ¢é¢œè‰²æ ¼å¼
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            
            # è°ƒæ•´å›¾åƒå¤§å°
            canvas_width = self.video_canvas.winfo_width()
            canvas_height = self.video_canvas.winfo_height()
            
            if canvas_width > 1 and canvas_height > 1:
                pil_image.thumbnail((canvas_width - 10, canvas_height - 10), Image.Resampling.LANCZOS)
            else:
                pil_image.thumbnail((630, 350), Image.Resampling.LANCZOS)
            
            # æ›´æ–°ç”»å¸ƒ
            self.current_video_frame = ImageTk.PhotoImage(pil_image)
            self.video_canvas.delete("all")
            
            canvas_width = canvas_width or 640
            canvas_height = canvas_height or 360
            x = canvas_width // 2
            y = canvas_height // 2
            
            # ç¡®ä¿å›¾åƒå¯¹è±¡å­˜åœ¨åå†åˆ›å»ºç”»å¸ƒå›¾åƒ
            if self.current_video_frame:
                try:
                    self.video_canvas.create_image(x, y, anchor=tk.CENTER, image=self.current_video_frame)
                except tk.TclError as e:
                    # å¦‚æœå›¾åƒå¯¹è±¡æ— æ•ˆï¼Œé‡æ–°åˆ›å»º
                    print(f"âš ï¸ å›¾åƒå¯¹è±¡æ— æ•ˆï¼Œé‡æ–°åˆ›å»º: {e}")
                    self.current_video_frame = ImageTk.PhotoImage(pil_image)
                    self.video_canvas.create_image(x, y, anchor=tk.CENTER, image=self.current_video_frame)
            
            current_time, total_time = self.get_current_video_time()
            
            # Format time with 0.01 second precision
            current_time_str = self.format_time_with_centiseconds(current_time)
            total_time_str = self.format_time_with_centiseconds(total_time)
            
            self.video_progress_label.config(text=f"{current_time_str} / {total_time_str}")
            
            # è®¡ç®—ä¸‹ä¸€å¸§çš„å»¶è¿Ÿæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰- æ­£å¸¸1å€æ’­æ”¾é€Ÿåº¦
            delay = int(1000 / STANDARD_FPS)  # æ­£å¸¸æ’­æ”¾é€Ÿåº¦
            self.video_after_id = self.root.after(delay, self.play_next_frame)

        else:
            # è§†é¢‘æ–‡ä»¶è¯»å–å®Œæ¯•ï¼Œä½†ä»éœ€ç­‰å¾…éŸ³é¢‘æ’­æ”¾å®Œæˆ
            if audio_is_playing:
                # é‡æ–°å¼€å§‹è§†é¢‘å¾ªç¯æ’­æ”¾ä»¥é…åˆéŸ³é¢‘
                self.video_cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                self.video_after_id = self.root.after(33, self.play_next_frame)
                print("ğŸ”„ è§†é¢‘å¾ªç¯æ’­æ”¾ä»¥ç­‰å¾…éŸ³é¢‘å®Œæˆ")
            else:
                self.stop_video_playback()
                self.log_to_output(self.video_output, "âœ… è§†é¢‘æ’­æ”¾å®Œæ¯•")


    def refresh_gui_scenes(self):
        """åˆ·æ–°åœºæ™¯åˆ—è¡¨"""
        # self.workflow.load_scenes()
        if self.current_scene_index >= len(self.workflow.scenes) :
            self.current_scene_index = 0

        # æ¸…ç†æ‰€æœ‰è½¨é“çš„ VideoCaptureï¼ˆé¿å…ä½¿ç”¨æ—§åœºæ™¯çš„è§†é¢‘ï¼‰
        self.cleanup_track_video_captures()

        # æ£€æŸ¥ç°æœ‰å›¾åƒ
        self.update_scene_display()
        
        # æ›´æ–°è§†é¢‘è¿›åº¦æ˜¾ç¤º
        self.update_video_progress_display()

        # æ›´æ–°æŒ‰é’®çŠ¶æ€
        self.update_scene_buttons_state()

        self.reset_track_offset()

        # å»¶è¿ŸåŠ è½½ç¬¬ä¸€å¸§ï¼Œç¡®ä¿canvaså·²å®Œå…¨æ¸²æŸ“
        self.root.after(100, self.load_all_first_frames)


    
    def cleanup_track_video_captures(self):
        if hasattr(self, 'second_track_cap') and self.second_track_cap:
            try:
                self.second_track_cap.release()
            except:
                pass
            self.second_track_cap = None
        
        # é‡ç½®ç¬¬äºŒè½¨é“çš„æ’­æ”¾çŠ¶æ€
        if hasattr(self, 'second_track_playing'):
            self.second_track_playing = False
        if hasattr(self, 'second_track_after_id') and self.second_track_after_id:
            try:
                self.root.after_cancel(self.second_track_after_id)
            except:
                pass
            self.second_track_after_id = None
        
        # æ¸…ç† PIP å·¦å³è½¨é“
        if hasattr(self, 'pip_left_cap') and self.pip_left_cap:
            try:
                self.pip_left_cap.release()
            except:
                pass
            self.pip_left_cap = None
            
        if hasattr(self, 'pip_right_cap') and self.pip_right_cap:
            try:
                self.pip_right_cap.release()
            except:
                pass
            self.pip_right_cap = None
        
        # é‡ç½® PIP çš„æ’­æ”¾çŠ¶æ€
        if hasattr(self, 'pip_lr_playing'):
            self.pip_lr_playing = False
        if hasattr(self, 'pip_lr_after_id') and self.pip_lr_after_id:
            try:
                self.root.after_cancel(self.pip_lr_after_id)
            except:
                pass
            self.pip_lr_after_id = None


    def load_all_first_frames(self):
        """åŠ è½½æ‰€æœ‰è½¨é“çš„ç¬¬ä¸€å¸§"""
        self.load_video_first_frame()
        
        # åŠ è½½æ‰€æœ‰å›¾ç‰‡é¢„è§ˆ
        if hasattr(self, 'clip_image_canvas'):
            self.load_all_images_preview()
        
        # æ ¹æ®å½“å‰é€‰ä¸­çš„tabåŠ è½½è½¨é“è§†é¢‘é¢„è§ˆ
        current_tab_index = self.second_notebook.index(self.second_notebook.select())
        if current_tab_index == 0:
            self.load_second_track_first_frame()
        elif current_tab_index == 1:
            self.load_pip_lr_first_frame()


    def load_second_track_first_frame(self):
        """åŠ è½½ç¬¬äºŒè½¨é“è§†é¢‘çš„ç¬¬ä¸€å¸§åˆ°ç”»å¸ƒï¼ˆä»å½“å‰åç§»ä½ç½®ï¼‰"""
        current_scene = self.get_current_scene()
        if not current_scene:
            return
            
        track_path = get_file_path(current_scene, self.selected_second_track)

        try:
            self.second_track_canvas.delete("all")

            if not track_path:
                # æ¸…é™¤ç”»å¸ƒæ˜¾ç¤ºæç¤ºä¿¡æ¯
                self.second_track_canvas.create_text(160, 90, text="ç¬¬äºŒè½¨é“è§†é¢‘é¢„è§ˆ\né€‰æ‹©è§†é¢‘åæ’­æ”¾æ˜¾ç¤º",
                                                   fill='white', font=('Arial', 12), 
                                                   justify=tk.CENTER, tags="hint")
                self.track_time_label.config(text="00:00 / 00:00")
                return
            
            # æ‰“å¼€è§†é¢‘æ–‡ä»¶
            temp_cap = cv2.VideoCapture(track_path)
            if not temp_cap.isOpened():
                print(f"âŒ æ— æ³•æ‰“å¼€ç¬¬äºŒè½¨é“è§†é¢‘æ–‡ä»¶: {track_path}")
                return
            
            # è®¡ç®—åº”è¯¥æ˜¾ç¤ºçš„å¸§ä½ç½®ï¼ˆåŸºäº offset + deltaï¼‰
            start_position = self.second_track_offset + self.second_delta
            if start_position < 0:
                start_position = 0
            
            # è·³åˆ°æ­£ç¡®çš„å¸§ä½ç½®
            temp_cap.set(cv2.CAP_PROP_POS_FRAMES, int(start_position * STANDARD_FPS))
            
            ret, frame = temp_cap.read()
            if ret:
                # æ˜¾ç¤ºç¬¬ä¸€å¸§åˆ°Canvas
                from PIL import Image, ImageTk
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                pil_image = Image.fromarray(frame_rgb)
                
                # è°ƒæ•´å›¾åƒå¤§å°é€‚åº”Canvas
                canvas_width = self.second_track_canvas.winfo_width()
                canvas_height = self.second_track_canvas.winfo_height()
                
                if canvas_width > 1 and canvas_height > 1:
                    pil_image.thumbnail((canvas_width - 10, canvas_height - 10), Image.Resampling.LANCZOS)
                else:
                    pil_image.thumbnail((310, 170), Image.Resampling.LANCZOS)
                
                # æ›´æ–°ç”»å¸ƒæ˜¾ç¤ºç¬¬ä¸€å¸§
                self.current_second_track_frame = ImageTk.PhotoImage(pil_image)
                
                canvas_width = canvas_width or 320
                canvas_height = canvas_height or 180
                x = canvas_width // 2
                y = canvas_height // 2
                self.second_track_canvas.create_image(x, y, anchor=tk.CENTER, image=self.current_second_track_frame)
                
            # æ›´æ–°æ—¶é—´æ˜¾ç¤º
            total_frames = temp_cap.get(cv2.CAP_PROP_FRAME_COUNT)
            total_duration = total_frames / STANDARD_FPS
            total_str = f"{int(total_duration // 60):02d}:{int(total_duration % 60):02d}"
            
            # æ˜¾ç¤ºå½“å‰åç§»ä½ç½®å’Œæ€»æ—¶é•¿
            current_str = f"{int(start_position // 60):02d}:{int(start_position % 60):02d}"
            self.track_time_label.config(text=f"{current_str} / {total_str}")
            
            temp_cap.release()
            print(f"âœ… å·²åŠ è½½ç¬¬äºŒè½¨é“è§†é¢‘å¸§ (ä½ç½®: {start_position:.2f}s): {os.path.basename(track_path)}")

        except Exception as e:
            print(f"âŒ åŠ è½½ç¬¬äºŒè½¨é“è§†é¢‘ç¬¬ä¸€å¸§å¤±è´¥: {e}")
            self.second_track_canvas.delete("all")
            self.second_track_canvas.create_text(160, 90, text="ç¬¬äºŒè½¨é“è§†é¢‘é¢„è§ˆ\né€‰æ‹©è§†é¢‘åæ’­æ”¾æ˜¾ç¤º",
                                               fill='white', font=('Arial', 12), 
                                               justify=tk.CENTER, tags="hint")


    def update_scene_display(self):
        """æ›´æ–°åœºæ™¯æ˜¾ç¤º"""
        if len(self.workflow.scenes) == 0:
            self.scene_label.config(text="0 / 0")
            self.clear_scene_fields()
            self.clear_video_scene_fields()
            return
            
        self.scene_label.config(text=f"{self.current_scene_index + 1} / {len(self.workflow.scenes)}")
        scene_data = self.get_current_scene()
        if not scene_data:
            return
        
        # æ˜¾ç¤ºæŒç»­æ—¶é—´
        self.scene_duration.config(state="normal")
        self.scene_duration.delete(0, tk.END)
        duration = self.workflow.find_clip_duration(scene_data)
        self.scene_duration.insert(0, f"{duration:.2f} ç§’")
        self.scene_duration.config(state="readonly")
        
        # è®¾ç½®å®£ä¼ å¤é€‰æ¡†çŠ¶æ€
        clip_animation = scene_data.get("clip_animation", "")
        self.scene_main_animate.set(clip_animation)
        
        # åŠ è½½å½“å‰åœºæ™¯çš„å›¾åƒç±»å‹è®¾ç½®
        current_image_type = scene_data.get("second_animation", config_prompt.ANIMATE_SOURCE[0])
        self.scene_second_animation.set(current_image_type)
        
        self.scene_visual_image.delete("1.0", tk.END)
        self.scene_visual_image.insert("1.0", scene_data.get("visual_image", ""))
        
        self.scene_subject.delete("1.0", tk.END)
        self.scene_subject.insert("1.0", scene_data.get("subject", ""))
        
        self.scene_person_action.delete("1.0", tk.END)
        self.scene_person_action.insert("1.0", scene_data.get("person_action", ""))
        
        self.scene_era_time.delete("1.0", tk.END)
        self.scene_era_time.insert("1.0", scene_data.get("era_time", ""))
        
        self.scene_environment.delete(0, tk.END)
        self.scene_environment.insert(0, scene_data.get("environment", ""))

        self.scene_cinematography.delete("1.0", tk.END)
        # å¦‚æœ cinematography æ˜¯å­—å…¸ï¼Œæ ¼å¼åŒ–æ˜¾ç¤ºï¼›å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥æ˜¾ç¤º
        cinematography_value = scene_data.get("cinematography", "")
        if isinstance(cinematography_value, dict):
            self.scene_cinematography.insert("1.0", json.dumps(cinematography_value, ensure_ascii=False, indent=2))
        else:
            self.scene_cinematography.insert("1.0", cinematography_value)
        
        self.scene_sound_effect.delete("1.0", tk.END)
        self.scene_sound_effect.insert("1.0", scene_data.get("sound_effect", ""))
        
        self.scene_kernel.delete("1.0", tk.END)
        self.scene_kernel.insert("1.0", scene_data.get("kernel", ""))

        self.scene_story.delete("1.0", tk.END)
        self.scene_story.insert("1.0", scene_data.get("story", ""))

        
        self.scene_extra.delete("1.0", tk.END)   
        self.scene_extra.insert("1.0", scene_data.get("caption", ""))

        self.scene_speaker_action.delete("1.0", tk.END)
        self.scene_speaker_action.insert("1.0", scene_data.get("speaker_action", ""))

        # scene_moodå­—æ®µç”¨äºè¯­éŸ³åˆæˆæƒ…ç»ª
        self.scene_speaker.set(scene_data.get("speaker", ""))
        self.scene_speaker_position.set(scene_data.get("speaker_position", ""))
        voice_synthesis_mood = scene_data.get("mood", "calm")
        if voice_synthesis_mood in EXPRESSION_STYLES:
            self.scene_mood.set(voice_synthesis_mood)
        else:
            self.scene_mood.set("calm")
        
        self.scene_story_content.delete("1.0", tk.END)
        self.scene_story_content.insert("1.0", scene_data.get("content", ""))
        
        # åŠ è½½å®£ä¼ ä¿¡æ¯
        self.scene_promotion.delete("1.0", tk.END)
        self.scene_promotion.insert("1.0", scene_data.get("promotion", ""))

        status = scene_data.get("clip_status", "")
        self.video_edit_frame.config(text=f"è§†é¢‘å°ºå¯¸: {status}")
        self.video_edit_frame.update()
        # video_width, video_height = self.workflow.ffmpeg_processor.check_video_size(input_media_path)
            # set self.video_edit_frame text tobe "è§†é¢‘å°ºå¯¸: width x height"



    def format_time_with_centiseconds(self, seconds):
        """Format time as MM:SS.CC (minutes:seconds.centiseconds)"""
        if seconds is None or seconds < 0:
            return "00:00.00"
        
        minutes = int(seconds // 60)
        remaining_seconds = seconds % 60
        secs = int(remaining_seconds)
        centiseconds = int((remaining_seconds - secs) * 100)
        
        return f"{minutes:02d}:{secs:02d}.{centiseconds:02d}"


    def get_current_video_time(self):
        """Get current video playback time in seconds"""
        #if self.video_start_time:
        #    elapsed_time = time.time() - self.video_start_time
        #    current_time = elapsed_time + (self.video_pause_time or 0)
        #else:
        current_frame = self.video_cap.get(cv2.CAP_PROP_POS_FRAMES)
        current_time = current_frame / STANDARD_FPS

        total_time = self.workflow.find_clip_duration(self.get_current_scene())
        
        if current_time > total_time:
            current_time = total_time

        return current_time, total_time


    def set_mark1(self):
        """Set mark1 to current video time"""
        current_time, total_time = self.get_current_video_time()
        current_time = current_time + self.playing_delta
        self.mark1_time = current_time
        time_str = self.format_time_with_centiseconds(current_time)
        self.mark1_label.config(text=time_str)
        print(f"âœ“ è®¾ç½®æ ‡è®°1: {time_str}")
    

    def set_mark2(self):
        """Set mark2 to current video time"""
        current_time, total_time = self.get_current_video_time()
        current_time = current_time + self.playing_delta
        self.mark2_time = current_time
        time_str = self.format_time_with_centiseconds(current_time)
        self.mark2_label.config(text=time_str)
        print(f"âœ“ è®¾ç½®æ ‡è®°2: {time_str}")
    

    def make_silence_between_marks(self):
        """Make audio silent between mark1 and mark2"""
        if self.mark1_time is None or self.mark2_time is None:
            messagebox.showwarning("è­¦å‘Š", "è¯·å…ˆè®¾ç½®æ ‡è®°1å’Œæ ‡è®°2")
            return
        
        mark1 = min(self.mark1_time, self.mark2_time)
        mark2 = max(self.mark1_time, self.mark2_time)
        
        if mark1 >= mark2:
            messagebox.showwarning("è­¦å‘Š", "æ ‡è®°1å’Œæ ‡è®°2æ—¶é—´ç›¸åŒæˆ–æ— æ•ˆ")
            return
        
        try:
            current_scene = self.get_current_scene()
            if not current_scene:
                messagebox.showerror("é”™è¯¯", "æ²¡æœ‰å½“å‰åœºæ™¯")
                return
            
            clip_audio_path = get_file_path(current_scene, "clip_audio")
            if not clip_audio_path or not os.path.exists(clip_audio_path):
                messagebox.showerror("é”™è¯¯", "æ‰¾ä¸åˆ°éŸ³é¢‘æ–‡ä»¶")
                return
            
            # Get total duration
            total_duration = self.workflow.ffmpeg_processor.get_duration(clip_audio_path)
            if total_duration <= 0:
                messagebox.showerror("é”™è¯¯", "æ— æ³•è·å–éŸ³é¢‘æ—¶é•¿")
                return
            
            # Ensure marks are within audio duration
            mark1 = max(0.0, min(mark1, total_duration))
            mark2 = max(mark1, min(mark2, total_duration))
            
            if mark1 >= mark2:
                messagebox.showwarning("è­¦å‘Š", "æ ‡è®°æ—¶é—´æ— æ•ˆ")
                return
            
            print(f"ğŸ”‡ é™éŸ³å¤„ç†: {mark1:.2f}s åˆ° {mark2:.2f}s")
            
            # Split audio into three parts: before mark1, between marks (silent), after mark2
            audio_segments = []
            
            # Part 1: from start to mark1
            if mark1 > 0:
                part1 = self.workflow.ffmpeg_audio_processor.audio_cut_fade(
                    clip_audio_path, 0, mark1, 0, 0, 1.0
                )
                if part1:
                    audio_segments.append(part1)
            
            # Part 2: silent segment from mark1 to mark2
            silent_duration = mark2 - mark1
            silent_segment = self.workflow.ffmpeg_audio_processor.make_silence(silent_duration)
            if silent_segment:
                audio_segments.append(silent_segment)
            
            # Part 3: from mark2 to end
            if mark2 < total_duration:
                part3_duration = total_duration - mark2
                part3 = self.workflow.ffmpeg_audio_processor.audio_cut_fade(
                    clip_audio_path, mark2, part3_duration, 0, 0, 1.0
                )
                if part3:
                    audio_segments.append(part3)
            
            # Concatenate all segments
            if audio_segments:
                output_audio = self.workflow.ffmpeg_audio_processor.concat_audios(audio_segments)
                if output_audio and os.path.exists(output_audio):
                    # Update scene audio file
                    old_audio, new_audio = refresh_scene_media(
                        current_scene, "clip_audio", ".wav", output_audio, True
                    )
                    
                    # Update video with new audio
                    clip_video = get_file_path(current_scene, "clip")
                    if clip_video and os.path.exists(clip_video):
                        output_video = self.workflow.ffmpeg_processor.add_audio_to_video(
                            clip_video, new_audio
                        )
                        if output_video:
                            refresh_scene_media(
                                current_scene, "clip", ".mp4", output_video, True
                            )
                    
                    messagebox.showinfo("æˆåŠŸ", f"å·²å°† {mark1:.2f}s åˆ° {mark2:.2f}s ä¹‹é—´çš„éŸ³é¢‘é™éŸ³")
                    print(f"âœ… é™éŸ³å¤„ç†å®Œæˆ")
                else:
                    messagebox.showerror("é”™è¯¯", "éŸ³é¢‘å¤„ç†å¤±è´¥")
            else:
                messagebox.showerror("é”™è¯¯", "æ— æ³•åˆ›å»ºéŸ³é¢‘ç‰‡æ®µ")
                
        except Exception as e:
            error_msg = f"é™éŸ³å¤„ç†å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            messagebox.showerror("é”™è¯¯", error_msg)


    def update_video_progress_display(self):
        """æ›´æ–°è§†é¢‘è¿›åº¦æ˜¾ç¤ºï¼ˆæœªæ’­æ”¾æ—¶æ˜¾ç¤ºæ€»æ—¶é•¿ï¼‰"""
        if not hasattr(self, 'workflow'):
            return

        try:
            current_scene = self.get_current_scene()
            if current_scene:
                clip_video = get_file_path(current_scene, "clip")
                if clip_video:
                    total_duration = self.workflow.ffmpeg_processor.get_duration(clip_video)
                else:
                    total_duration = 0.0
                
                if self.video_playing:
                    pass
                else:
                    total_time_str = self.format_time_with_centiseconds(total_duration)
                    self.video_progress_label.config(text=f"00:00.00 / {total_time_str}")
            else:
                self.video_progress_label.config(text="00:00.00 / 00:00.00")
                
        except Exception as e:
            self.video_progress_label.config(text="00:00.00 / 00:00.00")
            print(f"âš ï¸ æ›´æ–°è§†é¢‘è¿›åº¦æ˜¾ç¤ºå¤±è´¥: {e}")


    def clear_scene_fields(self):
        self.scene_duration.config(state="normal")
        self.scene_duration.delete(0, tk.END)
        self.scene_duration.config(state="readonly")
        
        self.scene_main_animate.set("")
        
        self.scene_visual_image.delete("1.0", tk.END)
        self.scene_era_time.delete("1.0", tk.END)
        self.scene_environment.delete(0, tk.END)
        self.scene_speaker.delete("1.0", tk.END)
        self.scene_speaker_action.delete("1.0", tk.END)
        self.scene_extra.delete("1.0", tk.END)
        self.scene_story.delete("1.0", tk.END)
        self.scene_speaker_position.set("")
        self.scene_mood.set("calm")
        self.scene_story_content.delete("1.0", tk.END)
        self.scene_kernel.delete("1.0", tk.END)
        self.scene_cinematography.delete("1.0", tk.END)
        self.scene_promotion.delete("1.0", tk.END)



    def first_scene(self):
        """ä¸Šä¸€ä¸ªåœºæ™¯"""
        self.update_current_scene()
        
        self.current_scene_index = 0
        self.refresh_gui_scenes()


    def last_scene(self):
        """ä¸Šä¸€ä¸ªåœºæ™¯"""
        self.update_current_scene()
        self.current_scene_index = len(self.workflow.scenes) - 1
        self.refresh_gui_scenes()


    def prev_scene(self):
        """ä¸Šä¸€ä¸ªåœºæ™¯"""
        self.update_current_scene()
        
        self.current_scene_index -= 1
        if self.current_scene_index < 0:
            self.current_scene_index = len(self.workflow.scenes) - 1

        self.refresh_gui_scenes()


    def next_scene(self):
        """ä¸‹ä¸€ä¸ªåœºæ™¯"""
        self.update_current_scene()
        
        self.current_scene_index += 1
        if self.current_scene_index >= len(self.workflow.scenes):
            self.current_scene_index = 0

        self.refresh_gui_scenes()



    def split_smart_scene(self):
        """åˆ†ç¦»å½“å‰åœºæ™¯"""
        current_scene = self.get_current_scene()
        original_duration = self.workflow.find_clip_duration(current_scene)
        if original_duration <= 0:
            return False

        gen_config = [
            sd_image_processor.GEN_CONFIG["S2V"].copy(),
            sd_image_processor.GEN_CONFIG["FS2V"].copy()
        ]

        for server_config in gen_config:
            section_duration = (server_config["max_frames"]-4) * 1.0 / server_config["frame_rate"]
            server_config["section_duration"] = section_duration
            sections = int(original_duration / section_duration)
            if original_duration / section_duration > sections:
                sections += 1
            server_config["sections"] = sections

        min_sections = 1000000
        best_config = None
        for server_config in gen_config:
            if server_config["sections"] < min_sections:
                min_sections = server_config["sections"]
                best_config = server_config

        if best_config is None:
            gen_config = gen_config[0]

        if gen_config[0]["sections"] == gen_config[1]["sections"]:
            best_config = gen_config[0]

        if best_config["sections"] == 1:
            return False

        if best_config == gen_config[0]:
            animate_mode = "S2V"
        else:
            animate_mode = "FS2V"
        current_scene["clip_animation"] = animate_mode

        new_scenes = self.workflow.split_smart_scene(current_scene, best_config["sections"])

        self.playing_delta = 0.0
        self.playing_delta_label.config(text=f"{self.playing_delta:.1f}s")
        self.refresh_gui_scenes()

        return new_scenes



    def split_scene(self):
        """åˆ†ç¦»å½“å‰åœºæ™¯"""      
        position = pygame.mixer.music.get_pos() / 1000.0
        self.workflow.split_scene_at_position(self.current_scene_index, position+self.playing_delta)
        self.playing_delta = 0.0
        self.playing_delta_label.config(text=f"{self.playing_delta:.1f}s")
        self.refresh_gui_scenes()


    def clean_media_mark(self):
        """æ ‡è®°æ¸…ç†"""
        for scene in self.workflow.scenes:
            scene["clip_animation"] = ""

        self.workflow.save_scenes_to_json()
        messagebox.showinfo("æˆåŠŸ", "æ ‡è®°æ¸…ç†æˆåŠŸï¼")


    def start_video_gen_batch(self):
        """å¯åŠ¨WANæ‰¹ç”Ÿæˆ"""
        current_scene = self.get_current_scene()
        previous_scene = self.get_previous_scene()
        next_scene = self.get_next_scene()

        ss = self.workflow.scenes_in_story(current_scene)
        for scene in ss:
            self.generate_video(scene, previous_scene, next_scene, "clip")
            self.generate_video(scene, previous_scene, next_scene, "second")

        self.refresh_gui_scenes()
        messagebox.showinfo("æˆåŠŸ", "WANè§†é¢‘æ‰¹é‡ç”ŸæˆæˆåŠŸï¼")


    def clean_wan(self):
        self.workflow.clean_folder("/wan_video/interpolated")
        self.workflow.clean_folder("/wan_video/enhanced")
        self.workflow.clean_folder("/wan_video/original")


    def clean_media(self):
        """åª’ä½“æ¸…ç†"""
        self.workflow.clean_media()
        self.workflow.save_scenes_to_json()
        messagebox.showinfo("æˆåŠŸ", "åª’ä½“æ¸…ç†æˆåŠŸï¼")


    def adjust_second_delta(self, delta):
        self.second_delta = self.second_delta + delta
        if self.second_delta < -10:
            self.second_delta = -10
        if self.second_delta > 10:
            self.second_delta = 10
        
        self.second_delta_label.config(text=f"{self.second_delta:.1f}s")


    def move_video(self, delta):
        self.playing_delta = self.playing_delta + delta
        if self.playing_delta < -2.0:
            self.playing_delta = -2.0
        if self.playing_delta > 2.0:
            self.playing_delta = 2.0
        
        self.playing_delta_label.config(text=f"{self.playing_delta:.1f}s")


    def insert_story_scene(self):
        self.update_scene_buttons_state()
        current_scene = self.get_current_scene()
        if current_scene and not self.workflow.first_scene_of_story(current_scene):
            return

        self.workflow.add_story_scene(
            self.current_scene_index,
            "",
            True,
            False,
        )

        self.workflow.save_scenes_to_json()
        self.refresh_gui_scenes()


    def append_scene(self):
        self.update_scene_buttons_state()
        current_scene = self.get_current_scene()
        if current_scene and not self.workflow.last_scene_of_story(current_scene):
            return

        self.workflow.add_story_scene(
            self.current_scene_index,
            "",
            True,
            True,
        )

        self.workflow.save_scenes_to_json()
        self.refresh_gui_scenes()


    def reverse_video(self):
        """ç¿»è½¬è§†é¢‘"""
        current_scene = self.get_current_scene()
        oldv, newv = refresh_scene_media(current_scene, "clip", ".mp4")
        os.replace(self.workflow.ffmpeg_processor.reverse_video(oldv), newv)
        self.workflow.save_scenes_to_json()
        self.refresh_gui_scenes()


    def get_current_playback_position(self):
        """
        è·å–å½“å‰ä¸»åœºæ™¯çš„æ’­æ”¾ä½ç½®ï¼ˆè€Œä¸æ˜¯å…¶ä»–è½¨é“çš„ä½ç½®ï¼‰
        ä¼˜å…ˆçº§ï¼šæš‚åœä½ç½® > å®æ—¶æ’­æ”¾ä½ç½® > 0
        """
        # è°ƒè¯•ä¿¡æ¯
        has_pause_time = hasattr(self, 'video_pause_time')
        pause_time_value = self.video_pause_time if has_pause_time else "å±æ€§ä¸å­˜åœ¨"
        is_playing = self.video_playing if hasattr(self, 'video_playing') else False
        
        # 1. å¦‚æœæœ‰æš‚åœä½ç½®ï¼Œä½¿ç”¨å®ƒï¼ˆæœ€å‡†ç¡®ï¼‰
        if has_pause_time and self.video_pause_time is not None and self.video_pause_time > 0:
            print(f"ğŸ¬ ä½¿ç”¨ä¸»è§†é¢‘æš‚åœä½ç½®: {self.video_pause_time:.2f}s")
            return self.video_pause_time
        
        # 2. å¦‚æœæ­£åœ¨æ’­æ”¾ï¼ŒåŸºäºæ—¶é—´è®¡ç®—å½“å‰ä½ç½®
        if is_playing and hasattr(self, 'video_start_time') and self.video_start_time:
            try:
                elapsed = time.time() - self.video_start_time
                # å¦‚æœæœ‰ç´¯ç§¯çš„æš‚åœæ—¶é—´ï¼ŒåŠ ä¸Šå®ƒ
                total_time = elapsed + (self.video_pause_time if self.video_pause_time else 0)
                print(f"ğŸ¬ ä½¿ç”¨ä¸»è§†é¢‘æ’­æ”¾ä½ç½®ï¼ˆå®æ—¶è®¡ç®—ï¼‰: {total_time:.2f}s (å½“å‰ç‰‡æ®µ: {elapsed:.2f}s, ç´¯ç§¯æš‚åœ: {self.video_pause_time or 0:.2f}s)")
                return total_time
            except:
                pass
        
        # 3. é»˜è®¤è¿”å› 0
        print(f"ğŸ¬ ä¸»è§†é¢‘æœªæ’­æ”¾æˆ–æ— æš‚åœä½ç½®ï¼Œè¿”å› 0")
        print(f"    è°ƒè¯•: video_pause_time={pause_time_value}, video_playing={is_playing}")
        return 0.0


    def mirror_video(self):
        """é•œåƒè§†é¢‘"""
        current_scene = self.get_current_scene()
        oldv, newv = refresh_scene_media(current_scene, "clip", ".mp4")
        os.replace(self.workflow.ffmpeg_processor.mirror_video(oldv), newv)
        self.workflow.save_scenes_to_json()
        self.refresh_gui_scenes()


    def print_title(self):
        """æ‰“å°æ ‡é¢˜"""
        current_scene = self.update_current_scene()
        content = current_scene['caption']
        if not content or content.strip() == "":
            messagebox.showinfo("æ ‡é¢˜", "æ ‡é¢˜ä¸ºç©º")
            return
        clip_video = get_file_path(current_scene, "clip")
        if not clip_video:
            messagebox.showinfo("æ ‡é¢˜", "è§†é¢‘ä¸ºç©º")
            return
       
        content = self.workflow.transcriber.translate_text(content, self.workflow.language, self.workflow.language)

        content_language = self.scene_language.get()
        if content_language in config.FONT_LIST:
            current_scene["content_language"] = content_language
            font = config.FONT_LIST[content_language]
        else:
            font = self.workflow.font_title

        v = self.workflow.ffmpeg_processor.add_script_to_video(clip_video, content, font)
        back = current_scene.get('back', '')
        current_scene['back'] = clip_video + "," + back
        refresh_scene_media(current_scene, "clip", ".mp4", v)

        self.workflow.save_scenes_to_json()
        self.refresh_gui_scenes()


    def toggle_track_playback(self):
        # æ£€æŸ¥å½“å‰é€‰ä¸­çš„tab
        current_tab_index = self.second_notebook.index(self.second_notebook.select())

        if current_tab_index == 1:
            if self.pip_lr_playing:
                self.pause_pip_lr() 
            else:
                self.play_pip_lr()
        else:
            if self.second_track_playing:
                self.pause_second_track()
            else:
                self.play_second_track()


    def play_second_track(self):
        """æ’­æ”¾ç¬¬äºŒè½¨é“è§†é¢‘çš„å½“å‰åœºæ™¯æ—¶é—´æ®µï¼ˆæ”¯æŒä»æš‚åœçŠ¶æ€å’Œåç§»ä½ç½®æ¢å¤ï¼‰"""
        second_video_path = get_file_path(self.get_current_scene(), self.selected_second_track)
        second_audio_path = get_file_path(self.get_current_scene(), self.selected_second_track+'_audio')
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ä»æš‚åœçŠ¶æ€æ¢å¤
            is_resuming = (self.second_track_cap and self.second_track_paused_time)

            #elif self.second_track_paused_time:
            #    play_start_time = self.second_track_paused_time
            #    print(f"â–¶ï¸ ä»æš‚åœä½ç½® {self.second_track_paused_time:.1f}s æ¢å¤æ’­æ”¾")
            #else:
            #    print(f"â–¶ï¸ ä»å¤´å¼€å§‹æ’­æ”¾ç¬¬äºŒè½¨é“")
            
            if is_resuming:
                play_start_time = self.second_track_paused_time
                # === ä»æš‚åœçŠ¶æ€æ¢å¤ï¼ˆä½†æ²¡æœ‰è®¾ç½®åç§»ï¼‰ ===
                self.second_track_start_time = time.time()
                self.second_track_playing = True
                self.track_play_button.config(text="â¸")
                if self.second_track_cap:
                    self.second_track_cap.set(cv2.CAP_PROP_POS_FRAMES, int(self.second_track_paused_time * STANDARD_FPS))
                try:
                    pygame.mixer.music.unpause()
                    print("â–¶ï¸ ç¬¬äºŒè½¨é“éŸ³é¢‘å·²æ¢å¤")
                except Exception as e:
                    print(f"âŒ æ¢å¤ç¬¬äºŒè½¨é“éŸ³é¢‘å¤±è´¥: {e}")
                    self.play_second_track_audio(second_audio_path)
                
            else:
                play_start_time = self.second_track_offset + self.second_delta
                # === å…¨æ–°å¼€å§‹æ’­æ”¾æˆ–ä»åç§»ä½ç½®æ’­æ”¾ ===
                if self.second_track_cap:
                    self.second_track_cap.release()
                self.second_track_cap = cv2.VideoCapture(second_video_path)
                if not self.second_track_cap.isOpened():
                    return

                self.second_track_cap.set(cv2.CAP_PROP_POS_FRAMES, int(play_start_time * STANDARD_FPS))
                
                self.second_track_end_time = self.workflow.ffmpeg_audio_processor.get_duration(second_video_path)
                
                self.second_track_playing = True
                self.track_play_button.config(text="â¸")
                
                self.second_track_start_time = time.time()
                self.second_track_paused_time = None
                
                self.play_second_track_audio(second_audio_path)
                
                print(f"â–¶ å¼€å§‹æ’­æ”¾ç¬¬äºŒè½¨é“è§†é¢‘ç‰‡æ®µ: {play_start_time:.1f}s - {self.second_track_end_time:.1f}s")
            
            # === é€šç”¨å¤„ç† - å¼€å§‹æ’­æ”¾å¾ªç¯
            self.play_second_track_frame()
            
            # æ›´æ–°æ—¶é—´æ˜¾ç¤º
            self.update_second_track_time_display()
            
        except Exception as e:
            print(f"âŒ æ’­æ”¾ç¬¬äºŒè½¨é“è§†é¢‘å¤±è´¥: {e}")


    def play_second_track_audio(self, audio_path):
        """æ’­æ”¾ç¬¬äºŒè½¨é“éŸ³é¢‘ï¼ˆæ”¯æŒä»åç§»ä½ç½®å¼€å§‹ï¼‰"""
        try:
            # åˆå§‹åŒ–pygame mixerï¼ˆå¦‚æœè¿˜æ²¡æœ‰åˆå§‹åŒ–ï¼‰
            if not pygame.mixer.get_init():
                pygame.mixer.init()
            
            # åœæ­¢ä»»ä½•æ­£åœ¨æ’­æ”¾çš„éŸ³é¢‘
            pygame.mixer.music.stop()
            
            # åŠ è½½éŸ³é¢‘æ–‡ä»¶
            pygame.mixer.music.load(audio_path)
            
            # ç¡®å®šéŸ³é¢‘å¼€å§‹æ’­æ”¾çš„åç§»æ—¶é—´
            audio_start_offset = self.second_track_offset + self.second_delta
            if self.second_track_paused_time:
                audio_start_offset = self.second_track_paused_time
            
            try:
                if audio_start_offset > 0:
                    pygame.mixer.music.play(start=audio_start_offset)
                else:
                    pygame.mixer.music.play()
            except TypeError:
                print("âš ï¸ å½“å‰pygameç‰ˆæœ¬ä¸æ”¯æŒä»æŒ‡å®šä½ç½®æ’­æ”¾éŸ³é¢‘ï¼Œå°†ä»å¤´æ’­æ”¾")
                pygame.mixer.music.play()
            
            # è®¾ç½®éŸ³é¢‘æ’­æ”¾çŠ¶æ€
            self.second_track_audio_playing = True
            
        except Exception as e:
            print(f"âŒ æ’­æ”¾ç¬¬äºŒè½¨é“éŸ³é¢‘å¤±è´¥: {e}")


    def stop_second_track_audio(self):
        """åœæ­¢ç¬¬äºŒè½¨é“éŸ³é¢‘æ’­æ”¾"""
        try:
            if self.second_track_audio_playing:
                pygame.mixer.music.stop()
                self.second_track_audio_playing = False
                self.second_track_audio_start_time = None
                print(f"â¹ ç¬¬äºŒè½¨é“éŸ³é¢‘æ’­æ”¾åœæ­¢")
        except Exception as e:
            print(f"âŒ åœæ­¢ç¬¬äºŒè½¨é“éŸ³é¢‘å¤±è´¥: {e}")


    def play_second_track_frame(self):
        """æ’­æ”¾ç¬¬äºŒè½¨é“è§†é¢‘çš„ä¸‹ä¸€å¸§ï¼ˆå¸¦åŒæ­¥æœºåˆ¶ï¼‰"""
        if not self.second_track_playing or not self.second_track_cap:
            return
            
        try:
            # æ£€æŸ¥éŸ³é¢‘æ˜¯å¦è¿˜åœ¨æ’­æ”¾
            audio_is_playing = pygame.mixer.music.get_busy()
            if not audio_is_playing:
                # éŸ³é¢‘æ’­æ”¾å®Œæ¯•ï¼Œåœæ­¢è§†é¢‘
                self.stop_second_track()
                print("âœ… ç¬¬äºŒè½¨é“éŸ³é¢‘æ’­æ”¾å®Œæ¯•ï¼Œè§†é¢‘åŒæ­¥åœæ­¢")
                return
            
            if self.second_track_start_time:
                # è®¡ç®—å®é™…ç»è¿‡çš„æ—¶é—´
                current_time = (time.time() - self.second_track_start_time) + self.second_track_offset + self.second_delta
                
                # è®¡ç®—åº”è¯¥åœ¨ç¬¬å‡ å¸§
                target_frame = int(current_time * STANDARD_FPS)
                current_frame = int(self.second_track_cap.get(cv2.CAP_PROP_POS_FRAMES))
                
                # å¦‚æœè§†é¢‘å¸§è½åäºéŸ³é¢‘è¿›åº¦ï¼Œè·³å¸§è¿½èµ¶
                if target_frame > current_frame + 2:  # å…è®¸2å¸§çš„å®¹é”™
                    self.second_track_cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)
                
                # æ£€æŸ¥æ˜¯å¦è¶…è¿‡äº†è§†é¢‘ç»“æŸæ—¶é—´
                if current_time >= self.second_track_end_time:
                    self.stop_second_track()
                    return
            
            ret, frame = self.second_track_cap.read()
            if not ret:
                # è§†é¢‘ç»“æŸï¼Œåœæ­¢æ’­æ”¾
                self.stop_second_track()
                return
            
            # æ˜¾ç¤ºè§†é¢‘å¸§åˆ°Canvas
            from PIL import Image, ImageTk
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            
            # è°ƒæ•´å›¾åƒå¤§å°é€‚åº”Canvas
            canvas_width = self.second_track_canvas.winfo_width()
            canvas_height = self.second_track_canvas.winfo_height()
            
            if canvas_width > 1 and canvas_height > 1:
                pil_image.thumbnail((canvas_width - 10, canvas_height - 10), Image.Resampling.LANCZOS)
            else:
                pil_image.thumbnail((310, 170), Image.Resampling.LANCZOS)
            
            # æ›´æ–°ç”»å¸ƒ
            self.current_second_track_frame = ImageTk.PhotoImage(pil_image)
            self.second_track_canvas.delete("all")
            
            canvas_width = canvas_width or 320
            canvas_height = canvas_height or 180
            x = canvas_width // 2
            y = canvas_height // 2
            self.second_track_canvas.create_image(x, y, anchor=tk.CENTER, image=self.current_second_track_frame)
            
            # æ›´æ–°æ—¶é—´æ˜¾ç¤º
            self.update_second_track_time_display()
            
            # å®‰æ’ä¸‹ä¸€å¸§æ’­æ”¾
            delay = max(1, int(1000 / STANDARD_FPS))  # æ¯«ç§’
            self.second_track_after_id = self.root.after(delay, self.play_second_track_frame)
            
        except Exception as e:
            print(f"âŒ æ’­æ”¾ç¬¬äºŒè½¨é“è§†é¢‘å¸§å¤±è´¥: {e}")
            self.stop_second_track()


    def pause_second_track(self):
        if not self.second_track_playing:
            return

        """æš‚åœç¬¬äºŒè½¨é“è§†é¢‘æ’­æ”¾"""
        self.second_track_playing = False
        self.track_play_button.config(text="â–¶")
        
        # è®¡ç®—å¹¶ä¿å­˜å½“å‰æ’­æ”¾åç§»æ—¶é—´ï¼ˆå…³é”®ï¼ä¸æ–°çš„åŒæ­¥æœºåˆ¶å…¼å®¹ï¼‰
        if self.second_track_start_time:
            try:
                self.second_track_paused_time = (time.time() - self.second_track_start_time) + self.second_track_offset + self.second_delta
                print(f"â¸ ä¿å­˜ç¬¬äºŒè½¨é“æš‚åœä½ç½®: {self.second_track_paused_time:.1f}s")
            except Exception as e:
                print(f"âŒ ä¿å­˜æš‚åœä½ç½®å¤±è´¥: {e}")
        
        # æš‚åœéŸ³é¢‘æ’­æ”¾
        try:
            pygame.mixer.music.pause()
            print("â¸ ç¬¬äºŒè½¨é“éŸ³é¢‘å·²æš‚åœ")
        except Exception as e:
            print(f"âŒ æš‚åœç¬¬äºŒè½¨é“éŸ³é¢‘å¤±è´¥: {e}")
        
        if self.second_track_after_id:
            self.root.after_cancel(self.second_track_after_id)
            self.second_track_after_id = None
            
        # æ›´æ–°æ—¶é—´æ˜¾ç¤º
        self.update_second_track_time_display()
    

    def stop_second_track(self):
        """åœæ­¢ç¬¬äºŒè½¨é“è§†é¢‘æ’­æ”¾"""
        self.second_track_playing = False
        self.track_play_button.config(text="â–¶")
        
        # åœæ­¢éŸ³é¢‘æ’­æ”¾
        self.stop_second_track_audio()
        
        if self.second_track_after_id:
            self.root.after_cancel(self.second_track_after_id)
            self.second_track_after_id = None
            
        if self.second_track_cap:
            self.second_track_cap.release()
            self.second_track_cap = None
            
        # æ¸…é™¤æ‰€æœ‰çŠ¶æ€å˜é‡
        self.second_track_paused_time = None
        self.second_track_paused_audio_time = None
        self.second_track_start_time = None
        self.reset_track_offset() # self.second_track_pause_offset
        
        print("â¹ æ¸…é™¤ç¬¬äºŒè½¨é“æ‰€æœ‰çŠ¶æ€")
            
        self.second_track_canvas.delete("all")
        self.second_track_canvas.create_text(160, 90, text="ç¬¬äºŒè½¨é“è§†é¢‘é¢„è§ˆ\né€‰æ‹©è§†é¢‘åæ’­æ”¾æ˜¾ç¤º", 
                                            fill="gray", font=("Arial", 10), justify=tk.CENTER, tags="hint")
        
        # æ›´æ–°æ—¶é—´æ˜¾ç¤º
        self.update_second_track_time_display()


    # ========== PIP L/R æ’­æ”¾æ§åˆ¶å‡½æ•° ==========
    
    def play_pip_lr(self):
        """åŒæ­¥æ’­æ”¾ second_left å’Œ second_right è§†é¢‘ï¼ˆæ”¯æŒä»æš‚åœæ¢å¤ï¼‰"""
        try:
            current_scene = self.get_current_scene()
            if not current_scene:
                return
            
            # è·å–è§†é¢‘è·¯å¾„
            left_path = current_scene.get('second_left')
            right_path = current_scene.get('second_right')
            audio_path = current_scene.get('clip_audio')
            
            if not left_path or not right_path:
                messagebox.showwarning("æç¤º", "å½“å‰åœºæ™¯æ²¡æœ‰ second_left æˆ– second_right è§†é¢‘")
                return
            
            if not os.path.exists(left_path) or not os.path.exists(right_path):
                messagebox.showerror("é”™è¯¯", "è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨")
                return
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ä»æš‚åœçŠ¶æ€æ¢å¤
            is_resuming = (self.pip_left_cap and hasattr(self, 'pip_lr_paused_time') and self.pip_lr_paused_time is not None)
            
            if is_resuming:
                # ä»æš‚åœæ¢å¤
                self.pip_lr_playing = True
                self.pip_lr_start_time = time.time() - self.pip_lr_paused_time
                self.track_play_button.config(text="â¸")
                
                # æ¢å¤éŸ³é¢‘æ’­æ”¾
                if audio_path and os.path.exists(audio_path):
                    try:
                        pygame.mixer.music.unpause()
                        print(f"â–¶ï¸ ä»æš‚åœä½ç½® {self.pip_lr_paused_time:.1f}s æ¢å¤æ’­æ”¾ PIP L/R")
                    except:
                        pass
                
                # æ¸…é™¤æš‚åœæ ‡è®°
                self.pip_lr_paused_time = None
                
                # ç»§ç»­æ’­æ”¾
                self.play_pip_lr_frame()
                
            else:
                # å…¨æ–°å¼€å§‹æ’­æ”¾
                # æ‰“å¼€è§†é¢‘æ–‡ä»¶
                self.pip_left_cap = cv2.VideoCapture(left_path)
                self.pip_right_cap = cv2.VideoCapture(right_path)
                
                if not self.pip_left_cap.isOpened() or not self.pip_right_cap.isOpened():
                    messagebox.showerror("é”™è¯¯", "æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
                    return
                
                # æ’­æ”¾éŸ³é¢‘
                if audio_path and os.path.exists(audio_path):
                    try:
                        pygame.mixer.music.load(audio_path)
                        pygame.mixer.music.set_volume(self.track_volume_var.get())
                        pygame.mixer.music.play()
                        print(f"ğŸ”Š æ’­æ”¾éŸ³é¢‘: {audio_path}")
                    except Exception as e:
                        print(f"âŒ æ’­æ”¾éŸ³é¢‘å¤±è´¥: {e}")
                
                # è®¾ç½®æ’­æ”¾çŠ¶æ€
                self.pip_lr_playing = True
                self.pip_lr_start_time = time.time()
                self.pip_lr_paused_time = None
                self.track_play_button.config(text="â¸")
                
                # å¼€å§‹æ’­æ”¾å¸§
                self.play_pip_lr_frame()
                
                print("â–¶ï¸ å¼€å§‹æ’­æ”¾ PIP L/R è§†é¢‘")
            
        except Exception as e:
            print(f"âŒ æ’­æ”¾ PIP L/R å¤±è´¥: {e}")
            self.stop_pip_lr()
    
    def play_pip_lr_frame(self):
        """æ’­æ”¾ PIP L/R çš„ä¸‹ä¸€å¸§ï¼ˆå¸¦éŸ³è§†é¢‘åŒæ­¥æœºåˆ¶ï¼‰"""
        try:
            if not self.pip_lr_playing:
                return
            
            if not self.pip_left_cap or not self.pip_right_cap:
                self.stop_pip_lr()
                return
            
            # æ£€æŸ¥éŸ³é¢‘æ˜¯å¦è¿˜åœ¨æ’­æ”¾
            try:
                audio_is_playing = pygame.mixer.music.get_busy()
                if not audio_is_playing:
                    # éŸ³é¢‘æ’­æ”¾å®Œæ¯•ï¼Œåœæ­¢è§†é¢‘
                    self.stop_pip_lr()
                    print("âœ… PIP L/R éŸ³é¢‘æ’­æ”¾å®Œæ¯•ï¼Œè§†é¢‘åŒæ­¥åœæ­¢")
                    return
            except:
                pass
            
            # è®¡ç®—åº”è¯¥æ’­æ”¾çš„å¸§ä½ç½®ä»¥ä¿æŒä¸éŸ³é¢‘åŒæ­¥
            if hasattr(self, 'pip_lr_start_time') and self.pip_lr_start_time:
                # è®¡ç®—å®é™…ç»è¿‡çš„æ—¶é—´
                elapsed_time = time.time() - self.pip_lr_start_time
                
                # è®¡ç®—åº”è¯¥åœ¨ç¬¬å‡ å¸§
                target_frame = int(elapsed_time * STANDARD_FPS)
                current_frame_left = int(self.pip_left_cap.get(cv2.CAP_PROP_POS_FRAMES))
                current_frame_right = int(self.pip_right_cap.get(cv2.CAP_PROP_POS_FRAMES))
                
                # å¦‚æœè§†é¢‘å¸§è½åäºéŸ³é¢‘è¿›åº¦ï¼Œè·³å¸§è¿½èµ¶ï¼ˆå…è®¸2å¸§çš„å®¹é”™ï¼‰
                if target_frame > current_frame_left + 2:
                    self.pip_left_cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)
                
                if target_frame > current_frame_right + 2:
                    self.pip_right_cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)
            
            # è¯»å–å·¦å³è§†é¢‘å¸§
            ret_left, frame_left = self.pip_left_cap.read()
            ret_right, frame_right = self.pip_right_cap.read()
            
            if not ret_left or not ret_right:
                # è§†é¢‘ç»“æŸ
                self.stop_pip_lr()
                return
            
            # æ˜¾ç¤ºå·¦ä¾§è§†é¢‘
            self.display_pip_frame(frame_left, self.pip_left_canvas)
            
            # æ˜¾ç¤ºå³ä¾§è§†é¢‘
            self.display_pip_frame(frame_right, self.pip_right_canvas)
            
            # æ›´æ–°æ—¶é—´æ˜¾ç¤º
            elapsed = time.time() - self.pip_lr_start_time
            total_frames_left = self.pip_left_cap.get(cv2.CAP_PROP_FRAME_COUNT)
            total_duration = total_frames_left / STANDARD_FPS
            
            current_str = f"{int(elapsed // 60):02d}:{int(elapsed % 60):02d}"
            total_str = f"{int(total_duration // 60):02d}:{int(total_duration % 60):02d}"
            self.track_time_label.config(text=f"{current_str} / {total_str}")
            
            # å®‰æ’ä¸‹ä¸€å¸§
            delay = max(1, int(1000 / STANDARD_FPS))
            self.pip_lr_after_id = self.root.after(delay, self.play_pip_lr_frame)
            
        except Exception as e:
            print(f"âŒ æ’­æ”¾ PIP L/R å¸§å¤±è´¥: {e}")
            self.stop_pip_lr()
    
    def display_pip_frame(self, frame, canvas):
        """åœ¨canvasä¸Šæ˜¾ç¤ºä¸€å¸§"""
        try:
            from PIL import Image, ImageTk
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            
            # è°ƒæ•´å›¾åƒå¤§å°
            canvas_width = canvas.winfo_width()
            canvas_height = canvas.winfo_height()
            
            if canvas_width > 1 and canvas_height > 1:
                pil_image.thumbnail((canvas_width - 4, canvas_height - 4), Image.Resampling.LANCZOS)
            else:
                pil_image.thumbnail((150, 150), Image.Resampling.LANCZOS)
            
            # æ›´æ–°ç”»å¸ƒ
            photo = ImageTk.PhotoImage(pil_image)
            canvas.delete("all")
            
            canvas_width = canvas_width or 155
            canvas_height = canvas_height or 160
            x = canvas_width // 2
            y = canvas_height // 2
            canvas.create_image(x, y, anchor=tk.CENTER, image=photo)
            
            # ä¿å­˜å¼•ç”¨é˜²æ­¢è¢«åƒåœ¾å›æ”¶
            if canvas == self.pip_left_canvas:
                self.current_pip_left_frame = photo
            else:
                self.current_pip_right_frame = photo
                
        except Exception as e:
            print(f"âŒ æ˜¾ç¤º PIP å¸§å¤±è´¥: {e}")
    
    
    def pause_pip_lr(self):
        if not self.pip_lr_playing:
            return

        """æš‚åœ PIP L/R æ’­æ”¾"""
        self.pip_lr_playing = False
        self.track_play_button.config(text="â–¶")
        
        # ä¿å­˜æš‚åœæ—¶é—´ç‚¹
        if hasattr(self, 'pip_lr_start_time') and self.pip_lr_start_time:
            self.pip_lr_paused_time = time.time() - self.pip_lr_start_time
            print(f"â¸ æš‚åœ PIP L/R æ’­æ”¾ï¼Œä½ç½®: {self.pip_lr_paused_time:.1f}s")
        
        # æš‚åœéŸ³é¢‘
        try:
            if pygame.mixer.music.get_busy():
                pygame.mixer.music.pause()
        except:
            pass
        
        # å–æ¶ˆä¸‹ä¸€å¸§è°ƒåº¦
        if self.pip_lr_after_id:
            self.root.after_cancel(self.pip_lr_after_id)
            self.pip_lr_after_id = None
    

    def stop_pip_lr(self):
        """åœæ­¢ PIP L/R æ’­æ”¾"""
        self.pip_lr_playing = False
        self.track_play_button.config(text="â–¶")
        
        # åœæ­¢éŸ³é¢‘
        try:
            if pygame.mixer.music.get_busy():
                pygame.mixer.music.stop()
        except:
            pass
        
        # å–æ¶ˆè°ƒåº¦
        if self.pip_lr_after_id:
            self.root.after_cancel(self.pip_lr_after_id)
            self.pip_lr_after_id = None
        
        # é‡Šæ”¾è§†é¢‘
        if self.pip_left_cap:
            self.pip_left_cap.release()
            self.pip_left_cap = None
        
        if self.pip_right_cap:
            self.pip_right_cap.release()
            self.pip_right_cap = None
        
        # æ¸…é™¤æ’­æ”¾çŠ¶æ€
        self.pip_lr_start_time = None
        self.pip_lr_paused_time = None
        
        # æ¸…ç©ºç”»å¸ƒ
        self.pip_left_canvas.delete("all")
        self.pip_left_canvas.create_text(77, 80, text="Left\nç”»ä¸­ç”»å·¦ä¾§", 
                                         fill="gray", font=("Arial", 9), justify=tk.CENTER, tags="hint")
        
        self.pip_right_canvas.delete("all")
        self.pip_right_canvas.create_text(77, 80, text="Right\nç”»ä¸­ç”»å³ä¾§", 
                                          fill="gray", font=("Arial", 9), justify=tk.CENTER, tags="hint")
        
        # é‡ç½®æ—¶é—´æ˜¾ç¤º
        self.track_time_label.config(text="00:00 / 00:00")
        
        print("â¹ åœæ­¢ PIP L/R æ’­æ”¾")

    
    def on_second_track_tab_changed(self, event=None):
        """tabåˆ‡æ¢æ—¶åœæ­¢æ­£åœ¨æ’­æ”¾çš„è§†é¢‘å¹¶åŠ è½½é¢„è§ˆå¸§"""
        # å…ˆåœæ­¢æ‰€æœ‰æ’­æ”¾
        self.pause_second_track()
        self.pause_pip_lr()
        
        # æ ¹æ®å½“å‰ tab åŠ è½½ç›¸åº”çš„é¢„è§ˆå¸§
        current_tab_index = self.second_notebook.index(self.second_notebook.select())
        if current_tab_index == 0:
            # ç¬¬äºŒè½¨é“ tabï¼šä»å½“å‰åç§»ä½ç½®åŠ è½½ç¬¬ä¸€å¸§
            self.load_second_track_first_frame()
        elif current_tab_index == 1:
            # PIP L/R tabï¼šä»èµ·å§‹ä½ç½®åŠ è½½ç¬¬ä¸€å¸§
            self.load_pip_lr_first_frame()

    
    def load_pip_lr_first_frame(self):
        """åŠ è½½ PIP L/R è§†é¢‘çš„ç¬¬ä¸€å¸§"""
        try:
            current_scene = self.get_current_scene()
            if not current_scene:
                return
            
            left_path = current_scene.get(self.selected_second_track+'_left')
            right_path = current_scene.get(self.selected_second_track+'_right')
            
            if not left_path or not right_path:
                # æ¸…ç©ºç”»å¸ƒæ˜¾ç¤ºæç¤º
                self.pip_left_canvas.delete("all")
                self.pip_left_canvas.create_text(77, 80, text="Left\nç”»ä¸­ç”»å·¦ä¾§\næœªç”Ÿæˆ", 
                                                 fill='gray', font=('Arial', 9), justify=tk.CENTER, tags="hint")
                self.pip_right_canvas.delete("all")
                self.pip_right_canvas.create_text(77, 80, text="Right\nç”»ä¸­ç”»å³ä¾§\næœªç”Ÿæˆ", 
                                                  fill='gray', font=('Arial', 9), justify=tk.CENTER, tags="hint")
                self.track_time_label.config(text="00:00 / 00:00")
                return
            
            if not os.path.exists(left_path) or not os.path.exists(right_path):
                print(f"âŒ PIP L/R è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨")
                return
            
            # æ‰“å¼€å·¦ä¾§è§†é¢‘è·å–ç¬¬ä¸€å¸§
            temp_cap_left = cv2.VideoCapture(left_path)
            if temp_cap_left.isOpened():
                ret, frame = temp_cap_left.read()
                if ret:
                    self.display_pip_frame(frame, self.pip_left_canvas)
                
                # è·å–æ€»æ—¶é•¿
                total_frames = temp_cap_left.get(cv2.CAP_PROP_FRAME_COUNT)
                total_duration = total_frames / STANDARD_FPS
                total_str = f"{int(total_duration // 60):02d}:{int(total_duration % 60):02d}"
                self.track_time_label.config(text=f"00:00 / {total_str}")
                
                temp_cap_left.release()
            
            # æ‰“å¼€å³ä¾§è§†é¢‘è·å–ç¬¬ä¸€å¸§
            temp_cap_right = cv2.VideoCapture(right_path)
            if temp_cap_right.isOpened():
                ret, frame = temp_cap_right.read()
                if ret:
                    self.display_pip_frame(frame, self.pip_right_canvas)
                temp_cap_right.release()
            
            print(f"âœ… å·²åŠ è½½ PIP L/R ç¬¬ä¸€å¸§")
            
        except Exception as e:
            print(f"âŒ åŠ è½½ PIP L/R ç¬¬ä¸€å¸§å¤±è´¥: {e}")
    
    
    def on_image_drop(self, event, image_type):
        """å¤„ç†å›¾ç‰‡æ‹–æ”¾äº‹ä»¶
        
        Args:
            event: æ‹–æ”¾äº‹ä»¶
            image_type: 'clip_image', 'second_image', æˆ– 'zero_image'
        """
        file_path = event.data.strip('{}').strip('"')
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå›¾ç‰‡æ–‡ä»¶
        if not (file_path.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.webp'))):
            messagebox.showerror("é”™è¯¯", "è¯·æ‹–æ”¾å›¾ç‰‡æ–‡ä»¶ (PNG, JPG, WEBPç­‰)")
            return
        
        if not os.path.exists(file_path):
            messagebox.showerror("é”™è¯¯", "æ–‡ä»¶ä¸å­˜åœ¨")
            return
        
        file_path = self.workflow.ffmpeg_processor.resize_image_smart(file_path)
        try:
            # è·å–å½“å‰åœºæ™¯
            current_scene = self.get_current_scene()
            if not current_scene:
                messagebox.showerror("é”™è¯¯", "æ²¡æœ‰é€‰ä¸­åœºæ™¯")
                return
            
            # å¤åˆ¶å›¾ç‰‡åˆ°é¡¹ç›®ç›®å½•
            oldi, image_path = refresh_scene_media(current_scene, image_type, ".webp", file_path, True)

            if image_type == 'clip_image' or image_type == 'clip_image_last':
                self.workflow.ask_replace_scene_info_from_image(current_scene, image_path)

            # åˆ·æ–°æ˜¾ç¤º
            self.display_image_on_canvas_for_track(image_type)
            
            self.workflow.save_scenes_to_json()
            print(f"âœ… å·²æ›´æ–° {image_type}: {os.path.basename(file_path)}")
            messagebox.showinfo("æˆåŠŸ", f"å·²æ›´æ–° {image_type.replace('_', ' ')}")
            
        except Exception as e:
            error_msg = f"æ›´æ–°å›¾ç‰‡å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            messagebox.showerror("é”™è¯¯", error_msg)


    def display_image_on_canvas_for_track(self, image_type):
        try:
            current_scene = self.get_current_scene()
            if not current_scene:
                return
            
            image_path = current_scene.get(image_type)
            if not image_path or not os.path.exists(image_path):
                return
            
            canvas_mapping = {
                'clip_image': (self.clip_image_canvas, "Clip\nImage", '_clip_image_photo'),
                'clip_image_last': (self.clip_image_last_canvas, "Clip\nLast", '_clip_image_last_photo'),
                'second_image': (self.second_image_canvas, "Second\nImage", '_second_image_photo'),
                'second_image_last': (self.second_image_last_canvas, "Second\nLast", '_second_image_last_photo'),
                'zero_image': (self.zero_image_canvas, "Zero\nImage", '_zero_image_photo'),
                'zero_image_last': (self.zero_image_last_canvas, "Zero\nLast", '_zero_image_last_photo'),
                'one_image': (self.one_image_canvas, "One\nImage", '_one_image_photo'),
                'one_image_last': (self.one_image_last_canvas, "One\nLast", '_one_image_last_photo'),
            }
            
            if image_type not in canvas_mapping:
                return
            
            canvas, label, photo_attr = canvas_mapping[image_type]
            
            from PIL import Image, ImageTk
            img = Image.open(image_path)
            
            canvas.delete("all")
            
            canvas.update_idletasks()
            canvas_width = canvas.winfo_width()
            canvas_height = canvas.winfo_height()
            
            if canvas_width <= 1 or canvas_height <= 1:
                canvas_width, canvas_height = 150, 75
            
            img_width, img_height = img.size
            aspect_ratio = img_width / img_height
            
            margin = 5
            available_width = canvas_width - margin
            available_height = canvas_height - margin
            
            if available_width / available_height > aspect_ratio:
                new_height = available_height
                new_width = int(new_height * aspect_ratio)
            else:
                new_width = available_width
                new_height = int(new_width / aspect_ratio)
            
            img_resized = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
            photo = ImageTk.PhotoImage(img_resized)
            
            x = canvas_width // 2
            y = canvas_height // 2
            canvas.create_image(x, y, image=photo, anchor=tk.CENTER, tags="image")
            
            setattr(self, photo_attr, photo)
            
            print(f"âœ… å·²æ˜¾ç¤º  {image_type}: {os.path.basename(image_path)}")
            
        except Exception as e:
            print(f"âŒ æ˜¾ç¤ºå›¾ç‰‡å¤±è´¥ ({image_type}): {e}")



    def load_all_images_preview(self):
        """åŠ è½½æ‰€æœ‰å›¾ç‰‡é¢„è§ˆ"""
        self.display_image_on_canvas_for_track('clip_image')
        self.display_image_on_canvas_for_track('clip_image_last')
        self.display_image_on_canvas_for_track('second_image')
        self.display_image_on_canvas_for_track('second_image_last')
        self.display_image_on_canvas_for_track('zero_image')
        self.display_image_on_canvas_for_track('zero_image_last')
        self.display_image_on_canvas_for_track('one_image')
        self.display_image_on_canvas_for_track('one_image_last')

    
    def on_track_volume_change(self, *args):
        """éŸ³é‡å˜åŒ–å¤„ç†ï¼ˆå…±ç”¨ï¼‰"""
        volume = self.track_volume_var.get()
        self.volume_label.config(text=f"{volume:.2f}")

        if hasattr(pygame.mixer, 'music') and pygame.mixer.music.get_busy():
            pygame.mixer.music.set_volume(volume)

    
    def update_second_track_time_display(self):
        """æ›´æ–°ç¬¬äºŒè½¨é“æ’­æ”¾æ—¶é—´æ˜¾ç¤º"""
        try:
            if not hasattr(self, 'second_track_cap') or not self.second_track_cap:
                self.track_time_label.config(text="00:00 / 00:00")
                return
            
            # è·å–è§†é¢‘æ€»æ—¶é•¿
            total_frames = self.second_track_cap.get(cv2.CAP_PROP_FRAME_COUNT)
            total_duration = total_frames / STANDARD_FPS
            
            # ç¡®å®šå½“å‰æ’­æ”¾æ—¶é—´
            current_time = 0.0
            if self.second_track_playing and self.second_track_start_time:
                # æ’­æ”¾çŠ¶æ€ï¼šæ ¹æ®å®é™…ç»è¿‡æ—¶é—´è®¡ç®—
                current_time = (time.time() - self.second_track_start_time) + self.second_track_offset + self.second_delta
            elif self.second_track_paused_time:
                # æš‚åœçŠ¶æ€ï¼šä½¿ç”¨æš‚åœæ—¶é—´
                current_time = self.second_track_paused_time
            elif self.second_track_offset:
                # ä½¿ç”¨åç§»ä½ç½®
                current_time = self.second_track_offset + self.second_delta
            else:
                # é»˜è®¤ï¼šä»è§†é¢‘å¸§ä½ç½®è®¡ç®—
                current_pos = self.second_track_cap.get(cv2.CAP_PROP_POS_FRAMES)
                current_time = current_pos / STANDARD_FPS
            
            # ç¡®ä¿æ—¶é—´åœ¨åˆç†èŒƒå›´å†…
            current_time = max(0, min(current_time, total_duration))
            
            # æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º (MM:SS æ ¼å¼)
            current_str = f"{int(current_time // 60):02d}:{int(current_time % 60):02d}"
            total_str = f"{int(total_duration // 60):02d}:{int(total_duration % 60):02d}"
            
            self.track_time_label.config(text=f"{current_str} / {total_str}")
            
        except Exception as e:
            print(f"âŒ æ›´æ–°ç¬¬äºŒè½¨é“æ—¶é—´æ˜¾ç¤ºå¤±è´¥: {e}")
            self.track_time_label.config(text="00:00 / 00:00")


    def move_second_track_forward(self):
        """ç¬¬äºŒè½¨é“å‰è¿›1ç§’"""
        try:
            if not hasattr(self, 'second_track_cap') or not self.second_track_cap:
                return
                
            # è·å–å½“å‰æ’­æ”¾ä½ç½®
            current_pos = self.second_track_cap.get(cv2.CAP_PROP_POS_FRAMES)
            current_time = current_pos / STANDARD_FPS
            
            # å‰è¿›1ç§’
            new_time = current_time + 1.0
            
            # è·å–è§†é¢‘æ€»æ—¶é•¿
            total_frames = self.second_track_cap.get(cv2.CAP_PROP_FRAME_COUNT)
            total_duration = total_frames / STANDARD_FPS
            
            # ç¡®ä¿ä¸è¶…è¿‡è§†é¢‘æ€»æ—¶é•¿
            if new_time >= total_duration:
                new_time = total_duration - 0.1
                
            # è·³è½¬åˆ°æ–°ä½ç½®
            self.second_track_cap.set(cv2.CAP_PROP_POS_FRAMES, int(new_time * STANDARD_FPS))
            
            # æ›´æ–°æ—¶é—´æ˜¾ç¤º
            self.update_second_track_time_display()
            
            print(f"â© ç¬¬äºŒè½¨é“å‰è¿›1ç§’: {current_time:.1f}s -> {new_time:.1f}s")
            
        except Exception as e:
            print(f"âŒ ç¬¬äºŒè½¨é“å‰è¿›å¤±è´¥: {e}")


    def move_second_track_backward(self):
        """ç¬¬äºŒè½¨é“åé€€1ç§’"""
        try:
            if not hasattr(self, 'second_track_cap') or not self.second_track_cap:
                return
            # è·å–å½“å‰æ’­æ”¾ä½ç½®
            current_pos = self.second_track_cap.get(cv2.CAP_PROP_POS_FRAMES)
            # åé€€1ç§’
            new_time = current_pos / STANDARD_FPS - 1.0
            if new_time < 0:
                new_time = 0
                
            # è·³è½¬åˆ°æ–°ä½ç½®
            self.second_track_cap.set(cv2.CAP_PROP_POS_FRAMES, int(new_time * STANDARD_FPS))
            
            # æ›´æ–°æ—¶é—´æ˜¾ç¤º
            self.update_second_track_time_display()
            
            print(f"âª ç¬¬äºŒè½¨é“åé€€1ç§’")
            
        except Exception as e:
            print(f"âŒ ç¬¬äºŒè½¨é“åé€€å¤±è´¥: {e}")
    

    def shift_scene(self, forward=True):
        position = pygame.mixer.music.get_pos() / 1000.0
        if position <= 0.001:
            position = 0.0

        if position == 0.0 and forward and self.playing_delta < 0.0 and self.current_scene_index > 0:
                current_index = self.current_scene_index - 1
                next_index = self.current_scene_index
                position = self.workflow.find_clip_duration(self.workflow.scenes[current_index])
        else:
            current_index = self.current_scene_index
            next_index = current_index + 1 if forward else current_index - 1
            if (next_index < 0 or next_index >= len(self.workflow.scenes)) and position + self.playing_delta <= 0.0 :
                return

        self.workflow.shift_scene(current_index, next_index, position+self.playing_delta)
        self.refresh_gui_scenes()


    def shift_before(self):
        """ä¸‹ç§»å½“å‰åœºæ™¯"""
        position = pygame.mixer.music.get_pos() / 1000.0
        self.workflow.shift_scene(self.current_scene_index, self.current_scene_index-1, position+self.playing_delta)
        self.playing_delta = 0.0

        self.refresh_gui_scenes()


    def merge_or_delete(self):
        """åˆå¹¶å½“å‰å›¾ç‰‡ä¸ä¸‹ä¸€å¼ å›¾ç‰‡"""
        if len(self.workflow.scenes) == 0:
            messagebox.showinfo("è­¦å‘Š", "âš ï¸ æ— åœºæ™¯")
            return

        current_scene = self.get_current_scene()
        ss = self.workflow.scenes_in_story(current_scene)
        if len(ss) <= 1:
            result = messagebox.askyesnocancel("è­¦å‘Š", "âš ï¸ åˆ é™¤å”¯ä¸€åœºæ™¯?")
            if result is True:
                ss = self.workflow.replace_scene(self.current_scene_index)
        else:
            if ss[-1] == current_scene:
                result = messagebox.askyesnocancel("è­¦å‘Š", "âš ï¸ åˆ é™¤å½“å‰åœºæ™¯?")
                if result is True:
                    ss = self.workflow.replace_scene(self.current_scene_index)
            else:
                result = messagebox.askyesnocancel("è­¦å‘Š", "âš ï¸ è¯·é€‰æ‹©æ“ä½œï¼š\næ˜¯: åˆå¹¶åœºæ™¯\nå¦: åˆ é™¤åœºæ™¯\nå–æ¶ˆ: å–æ¶ˆæ“ä½œ")
                if result is True:
                    result = messagebox.askyesno("è­¦å‘Š", "âš ï¸ è¯·é€‰æ‹©ä¿ç•™åœºæ™¯ï¼š\næ˜¯: ä¿ç•™å½“å‰åœºæ™¯\nå¦: ä¿ç•™ä¸‹ä¸€åœºæ™¯")
                    if result is True:
                        self.workflow.merge_scene(self.current_scene_index, self.current_scene_index+1, keep_current=True)
                    else :
                        self.workflow.merge_scene(self.current_scene_index, self.current_scene_index+1, False)
                elif result is False:
                    # åˆ é™¤åœºæ™¯
                    result = messagebox.askyesno("è­¦å‘Š", "âš ï¸ åˆ é™¤å½“å‰åœºæ™¯?")
                    if result:
                        ss = self.workflow.replace_scene(self.current_scene_index)
                # result is None è¡¨ç¤ºå–æ¶ˆï¼Œä¸åšä»»ä½•æ“ä½œ
            
        self.refresh_gui_scenes()
        messagebox.showinfo("åˆå¹¶åœºæ™¯", "å®Œæˆ")


    def swap_with_next_image(self):
        """äº¤æ¢å½“å‰å›¾ç‰‡ä¸ä¸‹ä¸€å¼ å›¾ç‰‡"""
        current_index = self.current_scene_index
        current_scene = self.workflow.scenes[current_index]

        ss = self.workflow.scenes_in_story(current_scene)
        if len(ss) <= 1 or current_scene == ss[-1]:
            messagebox.showinfo("è­¦å‘Š", "âš ï¸ å½“å‰åœºæ™¯æ— æ³•äº¤æ¢")
            return
        
        next_index = current_index + 1
        next_scene = self.workflow.scenes[next_index]

        # æŸ¥æ‰¾å½“å‰åœºæ™¯å’Œä¸‹ä¸€ä¸ªåœºæ™¯çš„å›¾åƒæ–‡ä»¶
        temp_image = current_scene["clip_image"]
        current_scene["clip_image"] = next_scene["clip_image"]
        next_scene["clip_image"] = temp_image

        # self.workflow._generate_video_from_image(current_scene)
        # self.workflow._generate_video_from_image(next_scene)
        
        # æ˜¾ç¤ºæˆåŠŸæ¶ˆæ¯
        messagebox.showinfo("æˆåŠŸ", f"å·²æˆåŠŸäº¤æ¢åœºæ™¯ {current_index + 1} å’Œåœºæ™¯ {next_index + 1} çš„å›¾ç‰‡ï¼")


    def swap_scene(self):
        """äº¤æ¢å½“å‰åœºæ™¯ä¸ä¸‹ä¸€å¼ åœºæ™¯"""
        self.workflow.swap_scene(self.current_scene_index, self.current_scene_index+1)
        self.refresh_gui_scenes()


    def refresh_scene_visual(self):
        self.workflow.refresh_scene_visual( self.get_current_scene() )
        self.refresh_gui_scenes()


    def copy_images_to_next(self):
        current_scene = self.get_current_scene()
        next_scene = self.workflow.next_scene_of_story(current_scene)
        if current_scene and next_scene:
            clip_image_split = current_scene.get("clip_image_split", "")
            clip_animation = current_scene.get("clip_animation", "")
            second_animation = current_scene.get("second_animation", "")

            next_scene["clip_image_split"] = clip_image_split
            next_scene["clip_animation"] =  clip_animation
            next_scene["second_animation"] = second_animation

            clip_image = current_scene.get("clip_image", "")
            clip_image_last = current_scene.get("clip_image_last", "")
            if clip_image:
                refresh_scene_media(next_scene, "clip_image", ".webp", clip_image, True)
            if clip_image_last:
                refresh_scene_media(next_scene, "clip_image_last", ".webp", clip_image_last, True)

            second_image = current_scene.get("second_image", "")
            second_image_last = current_scene.get("second_image_last", "")
            if second_image:
                refresh_scene_media(next_scene, "second_image", ".webp", second_image, True)
            if second_image_last:
                refresh_scene_media(next_scene, "second_image_last", ".webp", second_image_last, True)

            self.workflow.save_scenes_to_json()
            self.refresh_gui_scenes()


    def enhance_clip(self, clip_or_second:bool, fps_enhace:bool):
        """å¢å¼ºä¸»å›¾æˆ–æ¬¡å›¾"""
        scene = self.get_current_scene()
        level = self.enhance_level.get()
        self.workflow.sd_processor.enhance_clip(self.get_pid(), scene, "clip" if clip_or_second else "second", level, fps_enhace)
        self.refresh_gui_scenes()


    def recreate_clip_image(self, language:str):
        """é‡æ–°åˆ›å»ºä¸»å›¾ï¼Œå…ˆæ‰“å¼€å¯¹è¯æ¡†è®©ç”¨æˆ·å®¡æŸ¥å’Œç¼–è¾‘æç¤ºè¯"""
        scene = self.get_current_scene()
        
        # å®šä¹‰åˆ›å»ºå›¾åƒçš„å›è°ƒå‡½æ•°
        def create_clip_image(edited_positive, edited_negative):
            pass
            #oldi, newi = refresh_scene_media(scene, "clip_image", ".webp")
            #self.workflow._create_image(self.workflow.sd_processor.gen_config["Story"], 
            #                                    newi,
            #                                    None,
            #                                    newi,
            #                                    edited_positive,
            #                                    edited_negative,
            #                                    int(time.time())
            #                                )
            #self.workflow.save_scenes_to_json()
            #self.refresh_gui_scenes()
            #print("âœ… ä¸»å›¾å·²é‡æ–°åˆ›å»º")
        
        # æ„å»ºæ­£é¢æç¤ºè¯é¢„è§ˆ
        self.open_image_prompt_dialog(create_clip_image, scene, "clip", language)


    def update_current_scene(self):
        scene = self.get_current_scene()
        
        # å¤„ç† cinematography å­—æ®µï¼šå°è¯•è§£æ JSON å­—ç¬¦ä¸²
        cinematography_text = self.scene_cinematography.get("1.0", tk.END).strip()
        cinematography_value = cinematography_text
        if cinematography_text:
            try:
                # å°è¯•è§£æä¸º JSON å¯¹è±¡
                cinematography_value = json.loads(cinematography_text)
            except json.JSONDecodeError:
                # å¦‚æœä¸æ˜¯æœ‰æ•ˆ JSONï¼Œä¿æŒä¸ºå­—ç¬¦ä¸²
                cinematography_value = cinematography_text
        
        scene.update({
            "content": self.scene_story_content.get("1.0", tk.END).strip(),
            "kernel": self.scene_kernel.get("1.0", tk.END).strip(),
            "story": self.scene_story.get("1.0", tk.END).strip(),
            "subject": self.scene_subject.get("1.0", tk.END).strip(),
            "visual_image": self.scene_visual_image.get("1.0", tk.END).strip(),
            "person_action": self.scene_person_action.get("1.0", tk.END).strip(),
            "era_time": self.scene_era_time.get("1.0", tk.END).strip(),
            "environment": self.scene_environment.get(),
            "cinematography": cinematography_value,
            "sound_effect": self.scene_sound_effect.get("1.0", tk.END).strip(),
            "caption": self.scene_extra.get("1.0", tk.END).strip(),
            "speaker_action": self.scene_speaker_action.get("1.0", tk.END).strip(),
            "speaker": self.scene_speaker.get(),
            "speaker_position": self.scene_speaker_position.get(),  # æ·»åŠ è®²å‘˜ä½ç½®å­—æ®µ
            "mood": self.scene_mood.get(),         # è¯­éŸ³åˆæˆæƒ…ç»ª
            "clip_animation": self.scene_main_animate.get(),
            "promotion": self.scene_promotion.get("1.0", tk.END).strip()
        })
        self.workflow.save_scenes_to_json()
        return scene


    def load_config(self):
        """åŠ è½½å½“å‰é¡¹ç›®çš„é…ç½®"""
        try:
            # æ£€æŸ¥ project_manager.PROJECT_CONFIG æ˜¯å¦å·²è®¾ç½®
            if project_manager.PROJECT_CONFIG is None:
                print("âŒ é”™è¯¯ï¼šproject_manager.PROJECT_CONFIG æœªè®¾ç½®ï¼è¯·ç¡®ä¿å·²é€‰æ‹©é¡¹ç›®ã€‚")
                print(f"   è°ƒè¯•ä¿¡æ¯ï¼šshow_project_selection åº”è¯¥å·²ç»è®¾ç½®äº† project_manager.PROJECT_CONFIG")
                exit()
            
            # ä¸´æ—¶ç¦ç”¨è‡ªåŠ¨ä¿å­˜ï¼Œé¿å…åŠ è½½è¿‡ç¨‹ä¸­è§¦å‘ä¿å­˜
            self._loading_config = True
            self.apply_config_to_gui(project_manager.PROJECT_CONFIG)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆPID
            saved_pid = project_manager.PROJECT_CONFIG.get('pid', '')
            if not saved_pid:
                print("âš ï¸ é¡¹ç›®é…ç½®ä¸­æ²¡æœ‰æœ‰æ•ˆçš„PID")
                exit()

            # åŒæ­¥æ ‡é¢˜åˆ°workflow
            saved_video_title = project_manager.PROJECT_CONFIG.get('video_title', 'é»˜è®¤æ ‡é¢˜')
            if saved_video_title and saved_video_title != 'é»˜è®¤æ ‡é¢˜':
                self.video_title.delete(0, tk.END)
                self.video_title.insert(0, saved_video_title)
                # åªåœ¨workflowå·²åˆ›å»ºæ—¶è®¾ç½®æ ‡é¢˜
                if hasattr(self, 'workflow') and self.workflow is not None:
                    self.workflow.set_title(saved_video_title)

        except Exception as e:
            print(f"âŒ åŠ è½½é…ç½®å¤±è´¥: {e}")
            exit()
        finally:
            # é‡æ–°å¯ç”¨è‡ªåŠ¨ä¿å­˜
            self._loading_config = False


    def apply_config_to_gui(self, config_data):
        """å°†é…ç½®æ•°æ®åº”ç”¨åˆ°GUIç»„ä»¶"""
        try:
            # åŠ è½½PID (åªè¯»æ ‡ç­¾)
            pid = config_data.get('pid', '')
            if hasattr(self, 'shared_pid'):
                self.shared_pid.config(text=pid)
                
            # åŠ è½½è¯­è¨€ (åªè¯»æ ‡ç­¾)
            language = config_data.get('language', 'tw')
            if hasattr(self, 'shared_language'):
                self.shared_language.config(text=language)
                
            # åŠ è½½é¢‘é“ (åªè¯»æ ‡ç­¾)
            channel = config_data.get('channel', 'strange_zh')
            if hasattr(self, 'shared_channel'):
                self.shared_channel.config(text=channel)
                
            # åŠ è½½è§†é¢‘æ ‡é¢˜
            video_title = config_data.get('video_title', 'é»˜è®¤æ ‡é¢˜')
            if hasattr(self, 'video_title'):
                self.video_title.delete(0, tk.END)
                self.video_title.insert(0, video_title)
                
            # åŠ è½½å®£ä¼ è§†é¢‘æ»šåŠ¨æŒç»­æ—¶é—´
            promo_scroll_duration = config_data.get('promo_scroll_duration', 7.0)
            self.promo_scroll_duration = promo_scroll_duration
            
            print(f"âœ… å·²å°†é…ç½®åº”ç”¨åˆ°GUI: é¢‘é“={channel}, è¯­è¨€={language}, PID={pid}")
            
        except Exception as e:
            print(f"âŒ åº”ç”¨é…ç½®åˆ°GUIæ—¶å‡ºé”™: {e}")

    def on_closing(self):
        """å¤„ç†çª—å£å…³é—­äº‹ä»¶"""
        try:
            # æ˜¾ç¤ºä¿å­˜ç¡®è®¤å¯¹è¯æ¡†
            if not self.show_save_confirmation_on_exit():
                return  # ç”¨æˆ·å–æ¶ˆäº†ï¼Œä¸å…³é—­åº”ç”¨
        
            print("ğŸ”„ æ­£åœ¨å…³é—­åº”ç”¨...")
            
            # åœæ­¢åå°è§†é¢‘æ£€æŸ¥çº¿ç¨‹
            self.stop_video_check_thread()
            
            # åœæ­¢çŠ¶æ€æ›´æ–°å®šæ—¶å™¨
            if hasattr(self, 'status_update_timer_id') and self.status_update_timer_id is not None:
                self.root.after_cancel(self.status_update_timer_id)
                self.status_update_timer_id = None
            
            # åœæ­¢è§†é¢‘æ’­æ”¾å¹¶é‡Šæ”¾èµ„æº
            if hasattr(self, 'video_cap') and self.video_cap:
                self.video_cap.release()
            if hasattr(self, 'video_after_id') and self.video_after_id:
                self.root.after_cancel(self.video_after_id)
                
            # æ¸…ç†ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
            self.cleanup_temp_audio_files()
            
            print("âœ… åº”ç”¨å·²æ­£å¸¸å…³é—­")
            
        except Exception as e:
            print(f"âŒ å…³é—­æ—¶å‡ºé”™: {e}")
        finally:
            self.root.destroy()
            
                
    def show_save_confirmation_on_exit(self):
        """é€€å‡ºæ—¶æ˜¾ç¤ºä¿å­˜ç¡®è®¤å¯¹è¯æ¡†"""
        try:
            pid = project_manager.PROJECT_CONFIG.get('pid', 'æœªçŸ¥PID')
            title = project_manager.PROJECT_CONFIG.get('video_title', 'æœªçŸ¥æ ‡é¢˜')
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœªä¿å­˜çš„æ›´æ”¹
            current_data = self.get_current_config_data()
            has_changes = current_data != project_manager.PROJECT_CONFIG
            
            if has_changes:
                result = messagebox.askyesnocancel(
                    "ä¿å­˜é¡¹ç›®é…ç½®", 
                    f"æ˜¯å¦ä¿å­˜å½“å‰é¡¹ç›®çš„é…ç½®ï¼Ÿ\n\né¡¹ç›®: {pid}\næ ‡é¢˜: {title}\n\nç‚¹å‡»'æ˜¯'ä¿å­˜å¹¶é€€å‡º\nç‚¹å‡»'å¦'ä¸ä¿å­˜ç›´æ¥é€€å‡º\nç‚¹å‡»'å–æ¶ˆ'è¿”å›åº”ç”¨",
                    icon='question'
                )
                
                if result is None:  # ç”¨æˆ·ç‚¹å‡»å–æ¶ˆ
                    return False  # ä¸å…³é—­åº”ç”¨
                elif result:  # ç”¨æˆ·ç‚¹å‡»æ˜¯
                    self.save_config()
                    print(f"âœ… å·²ä¿å­˜é¡¹ç›®é…ç½®: {pid} - {title}")
                else:  # ç”¨æˆ·ç‚¹å‡»å¦
                    print(f"âš ï¸ é¡¹ç›®é…ç½®æœªä¿å­˜: {pid} - {title}")
            else:
                print(f"ğŸ“‹ é¡¹ç›®é…ç½®æ— å˜åŒ–ï¼Œæ— éœ€ä¿å­˜: {pid} - {title}")
                
            return True  # ç»§ç»­å…³é—­åº”ç”¨
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç¡®è®¤å¯¹è¯æ¡†å‡ºé”™: {e}")
            return True  # å‡ºé”™æ—¶ç»§ç»­å…³é—­åº”ç”¨
    
    def get_current_config_data(self):
        """è·å–å½“å‰çš„é…ç½®æ•°æ®"""
        config_data = {
            'pid': self.get_pid(),
            'language': self.shared_language.cget('text'), 
            'channel': self.shared_channel.cget('text'),
            'video_title': getattr(self, 'video_title', None) and self.video_title.get() or 'é»˜è®¤è§†é¢‘æ ‡é¢˜',
            # video_width and video_height are read-only from project config, not saved
            'video_width': project_manager.PROJECT_CONFIG.get('video_width', '1920') if project_manager.PROJECT_CONFIG else '1920',
            'video_height': project_manager.PROJECT_CONFIG.get('video_height', '1080') if project_manager.PROJECT_CONFIG else '1080',
            'kernel': project_manager.PROJECT_CONFIG.get('kernel', ''),
            'promo': project_manager.PROJECT_CONFIG.get('promo', ''),
            'story': project_manager.PROJECT_CONFIG.get('story', '')
        }

        # Add audio_prepares data if available
        workflow = self.workflow
        if workflow and hasattr(workflow, 'audio_prepares'):
            config_data['audio_prepares'] = workflow.video_prepares

        return config_data


    def cleanup_temp_audio_files(self):
        """æ¸…ç†ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶"""
        try:
            import glob
            temp_files = glob.glob("temp_audio_*.wav")
            for temp_file in temp_files:
                try:
                    os.remove(temp_file)
                    print(f"ğŸ—‘ï¸ å·²æ¸…ç†ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶: {temp_file}")
                except:
                    pass
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}")

    def save_config(self):
        """ä¿å­˜å½“å‰é¡¹ç›®é…ç½®"""
        try:
            workflow = self.workflow
            
            config_data = {
                'pid': self.get_pid(),
                'language': self.shared_language.cget('text'),
                'channel': self.shared_channel.cget('text'),
                'video_title': getattr(self, 'video_title', None) and self.video_title.get() or 'è§†é¢‘æ ‡é¢˜',
                # video_width and video_height are read-only from project config, not saved
                'video_width': project_manager.PROJECT_CONFIG.get('video_width', '1920') if project_manager.PROJECT_CONFIG else '1920',
                'video_height': project_manager.PROJECT_CONFIG.get('video_height', '1080') if project_manager.PROJECT_CONFIG else '1080',
                'kernel': project_manager.PROJECT_CONFIG.get('kernel', ''),
                'promo': project_manager.PROJECT_CONFIG.get('promo', ''),
                'story': project_manager.PROJECT_CONFIG.get('story', '')
            }

            # Save audio_prepares data if available
            if workflow and hasattr(workflow, 'audio_prepares'):
                config_data['audio_prepares'] = workflow.video_prepares
            
            # Preserve video_id and other important fields from existing config
            if project_manager.PROJECT_CONFIG:
                if 'video_id' in project_manager.PROJECT_CONFIG:
                    config_data['video_id'] = project_manager.PROJECT_CONFIG['video_id']
                if 'generated_titles' in project_manager.PROJECT_CONFIG:
                    config_data['generated_titles'] = project_manager.PROJECT_CONFIG['generated_titles']
                if 'generated_tags' in project_manager.PROJECT_CONFIG:
                    config_data['generated_tags'] = project_manager.PROJECT_CONFIG['generated_tags']
                # Preserve kernel, story from existing config
                if 'kernel' in project_manager.PROJECT_CONFIG:
                    config_data['kernel'] = project_manager.PROJECT_CONFIG['kernel']
                if 'promo' in project_manager.PROJECT_CONFIG:
                    config_data['promo'] = project_manager.PROJECT_CONFIG['promo']
                if 'story' in project_manager.PROJECT_CONFIG:
                    config_data['story'] = project_manager.PROJECT_CONFIG['story']
            
            # æ›´æ–°å½“å‰é¡¹ç›®é…ç½®
            project_manager.PROJECT_CONFIG = config_data
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            config_manager = ProjectConfigManager(self.get_pid())
            config_manager.save_project_config(config_data)
                
        except Exception as e:
            print(f"âŒ ä¿å­˜é¡¹ç›®é…ç½®å¤±è´¥: {e}")



    def bind_edit_events(self):
        """ç»‘å®šç¼–è¾‘äº‹ä»¶"""
        # ç»‘å®šåœºæ™¯ä¿¡æ¯ç¼–è¾‘å­—æ®µçš„Enteré”®äº‹ä»¶ï¼Œç”¨äºè‡ªåŠ¨ä¿å­˜
        scene_fields = [
            self.scene_visual_image,
            self.scene_story,
            self.scene_era_time,
            self.scene_environment,
            self.scene_speaker,
            self.scene_speaker_action,
            self.scene_extra,
            self.scene_kernel,
            self.scene_cinematography,
            self.scene_subject,
            self.scene_person_action,
            self.scene_story_content,
            self.scene_promotion
        ]
        
        for field in scene_fields:
            # ç»‘å®šEnteré”®äº‹ä»¶ï¼ˆCtrl+Enteråœ¨ScrolledTextä¸­è§¦å‘ä¿å­˜ï¼‰
            field.bind('<Control-Return>', self.on_scene_field_enter)
            field.bind('<Control-Enter>', self.on_scene_field_enter)
            # ä¹Ÿç»‘å®šå¤±å»ç„¦ç‚¹äº‹ä»¶ä½œä¸ºå¤‡é€‰ä¿å­˜æœºåˆ¶
            field.bind('<FocusOut>', self.on_scene_field_focus_out)
        
        # ä¸ºEntryå’ŒComboboxå­—æ®µå•ç‹¬ç»‘å®šå¤±å»ç„¦ç‚¹äº‹ä»¶
        entry_combobox_fields = [
            self.scene_speaker,
            self.scene_mood,
            self.scene_speaker_position
        ]
        
        for field in entry_combobox_fields:
            field.bind('<FocusOut>', self.on_scene_field_focus_out)
            field.bind('<<ComboboxSelected>>', self.on_scene_field_change)
        
        print("ğŸ“ å·²ç»‘å®šåœºæ™¯ç¼–è¾‘å­—æ®µçš„è‡ªåŠ¨ä¿å­˜äº‹ä»¶ (Ctrl+Enter æˆ–å¤±å»ç„¦ç‚¹æ—¶ä¿å­˜)")
    

    def bind_config_change_events(self):
        """ç»‘å®šé…ç½®å˜åŒ–äº‹ä»¶"""
        # PID, è¯­è¨€å’Œé¢‘é“ç°åœ¨éƒ½æ˜¯åªè¯»çš„ï¼Œä¸éœ€è¦ç»‘å®šå˜åŒ–äº‹ä»¶
            
        # ç»‘å®švideo_titleå˜åŒ–äº‹ä»¶
        if hasattr(self, 'video_title'):
            self.video_title.bind('<KeyRelease>', self.on_video_title_change)
            self.video_title.bind('<FocusOut>', self.on_video_title_change)
        

    def on_video_title_change(self, event=None):
        """å½“è§†é¢‘æ ‡é¢˜å‘ç”Ÿå˜åŒ–æ—¶çš„å›è°ƒå‡½æ•°"""
        # å¦‚æœæ­£åœ¨åŠ è½½é…ç½®ï¼Œä¸è¦è‡ªåŠ¨ä¿å­˜
        if hasattr(self, '_loading_config') and self._loading_config:
            return
        
        # ç›´æ¥æ›´æ–°workflowçš„titleå±æ€§
        if hasattr(self, 'workflow') and self.workflow is not None:
            gui_title = self.video_title.get().strip()
            if gui_title and gui_title != "......":
                self.workflow.title = gui_title
                print(f"ğŸ·ï¸ Workflow title updated: {gui_title}")
        
        # ä¿å­˜é…ç½®
        self.save_config()



    def on_config_change(self, event=None):
        """å½“é…ç½®å‘ç”Ÿå˜åŒ–æ—¶çš„å›è°ƒå‡½æ•°"""
        # å¦‚æœæ­£åœ¨åŠ è½½é…ç½®ï¼Œä¸è¦è‡ªåŠ¨ä¿å­˜
        if hasattr(self, '_loading_config') and self._loading_config:
            return
        
        self.save_config()

    def on_scene_edit(self, event=None):
        """å½“åœºæ™¯ä¿¡æ¯è¢«ç¼–è¾‘æ—¶çš„å›è°ƒï¼ˆç°åœ¨ä¸éœ€è¦ï¼‰"""
        # ä¿å­˜æŒ‰é’®ç°åœ¨æ€»æ˜¯å¯ç”¨
        pass

    def on_scene_field_enter(self, event=None):
        """å½“åœ¨åœºæ™¯ç¼–è¾‘å­—æ®µä¸­æŒ‰ä¸‹Ctrl+Enteræ—¶çš„å›è°ƒ"""
        # ä¿å­˜å½“å‰åœºæ™¯ä¿¡æ¯åˆ°JSONå¹¶ä¼ æ’­åˆ°ç›¸åŒraw_scene_indexçš„åœºæ™¯
        self.update_current_scene()
        return "break"  # é˜»æ­¢é»˜è®¤çš„æ¢è¡Œè¡Œä¸º

    def on_scene_field_focus_out(self, event=None):
        """å½“åœºæ™¯ç¼–è¾‘å­—æ®µå¤±å»ç„¦ç‚¹æ—¶çš„å›è°ƒ"""
        # å»¶è¿Ÿä¿å­˜ä»¥é¿å…é¢‘ç¹æ“ä½œ
        if hasattr(self, '_save_timer'):
            self.root.after_cancel(self._save_timer)
        self._save_timer = self.root.after(500, lambda: self.update_current_scene())  # 500mså»¶è¿Ÿ

    def on_scene_field_change(self, event=None):
        """å½“åœºæ™¯å­—æ®µå€¼å‘ç”Ÿå˜åŒ–æ—¶çš„å›è°ƒï¼ˆå¦‚Comboboxé€‰æ‹©å˜åŒ–ï¼‰"""
        # ç«‹å³ä¿å­˜å½“å‰åœºæ™¯ä¿¡æ¯
        self.update_current_scene()
        print(f"âœ… åœºæ™¯ {self.current_scene_index + 1} æƒ…ç»ªå·²æ›´æ–°ä¸º: {self.scene_mood.get()}")

    def on_volume_change(self, *args):
        """å½“éŸ³é‡æ»‘å—å€¼å‘ç”Ÿå˜åŒ–æ—¶çš„å›è°ƒ"""
        volume = self.track_volume_var.get()
        self.volume_label.config(text=f"{volume:.1f}")

    def on_tab_changed(self, event):
        if not hasattr(self, 'workflow') or self.workflow is None:
            return
        self.refresh_gui_scenes()


    def setup_drag_and_drop(self):
        self.video_canvas.drop_target_register(DND_FILES)
        self.video_canvas.dnd_bind('<<Drop>>', self.on_media_drop)
        self.video_canvas.dnd_bind('<<DragEnter>>', self.on_video_drag_enter)
        self.video_canvas.dnd_bind('<<DragLeave>>', self.on_video_drag_leave)
        
        # æ·»åŠ åŒå‡»äº‹ä»¶ç»‘å®š
        self.video_canvas.bind('<Double-Button-1>', self.on_video_canvas_double_click)


    def handle_av_replacement(self, av_path, replace_media_audio, media_type):
        """å¤„ç†éŸ³é¢‘æ›¿æ¢"""
        try:
            current_scene = self.get_current_scene()
            previous_scene = self.get_previous_scene()
            next_scene = self.get_next_scene()
            scenes_same_story = self.workflow.scenes_in_story(current_scene)

            if not av_path:
                if media_type == 'clip':
                    av_path = get_file_path(current_scene, "clip")
                elif media_type == 'zero':
                    av_path = get_file_path(current_scene, "zero")
                elif media_type == 'one':
                    av_path = get_file_path(current_scene, "one")
                else:
                    av_path = get_file_path(current_scene, "second")
            else:
                current_scene[media_type + "_fps"] = self.workflow.ffmpeg_processor.get_video_fps(av_path)
                current_scene[media_type + "_status"] = "DND"
                av_path = self.workflow.ffmpeg_processor.resize_video(av_path, width=None, height=self.workflow.ffmpeg_processor.height)

            print(f"ğŸ¬ æ‰“å¼€åˆå¹¶ç¼–è¾‘å™¨ - åª’ä½“ç±»å‹: {media_type}, æ›¿æ¢éŸ³é¢‘: {replace_media_audio}")
            if media_type != "clip":
                replace_media_audio = "keep"
            review_dialog = AVReviewDialog(self, av_path, current_scene, previous_scene, next_scene, media_type, replace_media_audio)
            
            # ç­‰å¾…å¯¹è¯æ¡†å…³é—­
            self.root.wait_window(review_dialog.dialog)

            if media_type != "clip" :
                transcribe_way = "" if ('transcribe_way' not in review_dialog.result) else review_dialog.result['transcribe_way']
                if transcribe_way == "multiple" or media_type == "zero":
                    for sss in scenes_same_story:
                        sss[media_type] = current_scene[media_type]
                        sss[media_type+"_audio"]  = current_scene[media_type+"_audio"]
                        sss[media_type+"_image"]  = current_scene[media_type+"_image"]
                        if "camear_style" in current_scene:
                            sss["camear_style"] = current_scene["camear_style"]
                        if "camera_shot" in current_scene:
                            sss["camera_shot"] = current_scene["camera_shot"]
                        if "camera_angle" in current_scene:
                            sss["camera_angle"] = current_scene["camera_angle"]
                        if "camera_color" in current_scene:
                            sss["camera_color"] = current_scene["camera_color"]

                self.workflow.save_scenes_to_json()
                return

            self.workflow.save_scenes_to_json()

            # media_type == clip
            if (not review_dialog.result) or ('transcribe_way' not in review_dialog.result) or (review_dialog.result['transcribe_way'] == "none"):
                print("åœºæ™¯å†…å®¹æ— å˜åŒ–")
                return

            transcribe_way = review_dialog.result['transcribe_way']
            audio_json = review_dialog.result['audio_json']

            current_scene["clip_animation"] = ""

            if transcribe_way == "single":
                current_scene["content"] = "\n".join([segment["content"] for segment in audio_json])
                self.workflow.refresh_scene_visual(current_scene)
            elif transcribe_way == "multiple":
                self.workflow.prepare_scenes_from_json( raw_scene=current_scene, audio_json=audio_json )
                self.workflow.replace_scene_with_others(self.current_scene_index, audio_json)
            else: # transcribe_way == "multiple_merge":
                self.workflow.merge_scenes_from_json( raw_scene=current_scene, audio_json=audio_json )

            messagebox.showinfo("æˆåŠŸ", f"éŸ³é¢‘å·²æˆåŠŸæ›¿æ¢ï¼\n\n")
                
        except Exception as e:
            messagebox.showerror("é”™è¯¯", f"éŸ³é¢‘æ›¿æ¢å¤±è´¥: {str(e)}")


    def handle_image_replacement(self, source_image_path):
        """å¤„ç†å›¾åƒæ›¿æ¢"""
        try:
            # å¯¼å…¥å›¾åƒåŒºåŸŸé€‰æ‹©å¯¹è¯æ¡†
            from gui.image_area_selector_dialog import show_image_area_selector
            # æ˜¾ç¤ºå›¾åƒåŒºåŸŸé€‰æ‹©å¯¹è¯æ¡†
            selected_image_path, vertical_line_position, target_field = show_image_area_selector(
                self, source_image_path, self.workflow.ffmpeg_processor.width, self.workflow.ffmpeg_processor.height
            )
            
            if selected_image_path is None:
                return  # ç”¨æˆ·å–æ¶ˆäº†é€‰æ‹©
            
            field_names = {
                "clip_image": "å½“å‰åœºæ™¯å›¾ç‰‡",
                "clip_image_last": "æœ€ååœºæ™¯å›¾ç‰‡"
            }
            
            dialog = messagebox.askyesno("ç¡®è®¤æ›¿æ¢åœºæ™¯çš„å›¾åƒ/è§†é¢‘", 
                                       f"ç¡®å®šè¦æ›¿æ¢ {field_names.get(target_field, target_field)} å—ï¼Ÿ\nå‚ç›´åˆ†å‰²çº¿ä½ç½®: {vertical_line_position}")
            if not dialog:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.remove(selected_image_path)
                except:
                    pass
                return
            
            selected_image_path = self.workflow.ffmpeg_processor.resize_image_smart(selected_image_path)

            current_scene = self.get_current_scene()
            self.workflow.replace_scene_image(current_scene, selected_image_path, vertical_line_position, target_field)
            
            # åˆ·æ–°GUIæ˜¾ç¤º
            self.refresh_gui_scenes()
            
            # è®°å½•æ“ä½œ
            print(f"âœ… å›¾åƒå·²æ›¿æ¢åˆ° {field_names.get(target_field, target_field)}ï¼Œå‚ç›´åˆ†å‰²çº¿ä½ç½®: {vertical_line_position}")
            
        except Exception as e:
            messagebox.showerror("é”™è¯¯", f"å›¾åƒæ›¿æ¢å¤±è´¥: {str(e)}")


    # è§†é¢‘æ‹–æ‹½ç›¸å…³æ–¹æ³•
    def on_video_drag_enter(self, event):
        """è§†é¢‘æ‹–æ‹½è¿›å…¥æ—¶çš„è§†è§‰åé¦ˆ"""
        self.video_canvas.create_rectangle(0, 0, self.video_canvas.winfo_width(), 
                                         self.video_canvas.winfo_height(), 
                                         outline="blue", width=3, tags="drag_border")


    def on_video_drag_leave(self, event):
        """è§†é¢‘æ‹–æ‹½ç¦»å¼€æ—¶æ¢å¤è§†è§‰çŠ¶æ€"""
        self.video_canvas.delete("drag_border")


    def on_media_drop(self, event):
        self.video_canvas.delete("drag_border")
        
        files = self.root.tk.splitlist(event.data)
        if not files:
            return
        dropped_file = files[0]
        if not os.path.exists(dropped_file):
            return
        
        if is_image_file(dropped_file):
            self.handle_image_replacement(dropped_file)
        elif is_audio_file(dropped_file):
            # ask user if want to replace audio for just current scene, all scenes, or extend to all scenes
            choice = askchoice("ç¡®è®¤æ›¿æ¢éŸ³é¢‘", [
                "æ›¿æ¢å½“å‰åœºæ™¯éŸ³é¢‘",
                "æ›¿æ¢æ‰€æœ‰åœºæ™¯éŸ³é¢‘",
                "ç¼©æ”¾æ‰€æœ‰åœºæ™¯éŸ³é¢‘"
            ])
            if not choice:
                return  # ç”¨æˆ·å–æ¶ˆ


            if choice == "æ›¿æ¢å½“å‰åœºæ™¯éŸ³é¢‘":
                clip_duration = self.workflow.ffmpeg_processor.get_duration(self.get_current_scene()["clip_audio"])
                self.workflow.replace_scene_audio(self.get_current_scene(), dropped_file, 0, clip_duration)

            elif choice == "æ›¿æ¢æ‰€æœ‰åœºæ™¯éŸ³é¢‘":
                start_time = 0.0
                for scene in self.workflow.scenes:
                    clip_duration = self.workflow.ffmpeg_processor.get_duration(scene["clip_audio"])
                    self.workflow.replace_scene_audio(scene, dropped_file, start_time, clip_duration)
                    start_time += clip_duration

            elif choice == "ç¼©æ”¾æ‰€æœ‰åœºæ™¯éŸ³é¢‘":
                total_duration = self.workflow.ffmpeg_processor.get_duration(dropped_file)
                clip_duration = total_duration / len(self.workflow.scenes)
                # Extend audio to total duration and assign to each scene
                start_time = 0.0
                for scene in self.workflow.scenes:
                    self.workflow.replace_scene_audio(scene, dropped_file, start_time, clip_duration)
                    start_time += clip_duration

        elif is_video_file(dropped_file):
            from gui.media_type_selector import MediaTypeSelector
            selector = MediaTypeSelector(self.root, dropped_file, self.workflow.ffmpeg_processor.has_audio_stream(dropped_file), self.get_current_scene())
            replace_media_audio, media_type = selector.show()
            if not media_type:
                return  # ç”¨æˆ·å–æ¶ˆ
            self.handle_av_replacement(dropped_file, replace_media_audio, media_type)

        self.refresh_gui_scenes()


    def on_video_canvas_configure(self, event):
        """å½“video canvaså°ºå¯¸æ”¹å˜æ—¶ï¼ŒåŠ¨æ€è°ƒæ•´æç¤ºæ–‡æœ¬ä½ç½®"""
        canvas_width = event.width
        canvas_height = event.height
        center_x = canvas_width // 2
        center_y = canvas_height // 2
        
        # æ›´æ–°æ‹–æ‹½æç¤ºæ–‡æœ¬çš„ä½ç½®åˆ°canvasä¸­å¿ƒ
        self.video_canvas.coords("drag_hint", center_x, center_y)


    def on_video_canvas_double_click(self, event):
        current_scene = self.get_current_scene()
        from gui.media_type_selector import MediaTypeSelector
        selector = MediaTypeSelector(self.root, None, True, current_scene)
        replace_media_audio, media_type = selector.show()
        if not media_type:
            return  # ç”¨æˆ·å–æ¶ˆ
        elif media_type == 'clip':
            dropped_file = get_file_path(current_scene, "clip")
        elif media_type == 'zero':
            dropped_file = get_file_path(current_scene, "zero")
        elif media_type == 'one':
            dropped_file = get_file_path(current_scene, "one")
        else:
            dropped_file = get_file_path(current_scene, "second")

        self.handle_av_replacement(dropped_file, replace_media_audio, media_type)

        self.refresh_gui_scenes()


    def on_clip_animation_change(self, event=None):
        current_scene = self.get_current_scene()
        current_scene["clip_animation"] = self.scene_main_animate.get()
        self.workflow.save_scenes_to_json()

    def on_video_clip_animation_change(self, event=None):
        """å½“è§†é¢‘æ ‡ç­¾é¡µå®£ä¼ æ¨¡å¼å‘ç”Ÿå˜åŒ–æ—¶çš„å›è°ƒå‡½æ•°"""
        # ä¿å­˜å½“å‰åœºæ™¯çš„å®£ä¼ æ¨¡å¼åˆ°JSON
        current_scene = self.get_current_scene()
        current_scene["clip_animation"] = self.scene_main_animate.get()
        self.workflow.save_scenes_to_json()
        self.log_to_output(self.video_output, f"âœ… å®£ä¼ æ¨¡å¼å·²æ›´æ–°ä¸º: {self.scene_main_animate.get()}")


    def on_image_type_change(self, event=None):
        """å¤„ç†å›¾åƒç±»å‹é€‰æ‹©å˜åŒ–"""
        selected_image_type = self.scene_second_animation.get()
        print(f"âœ… åœºæ™¯ {self.current_scene_index + 1} å›¾åƒç±»å‹å·²è®¾ç½®ä¸º: {selected_image_type}")
        
        # ä¿å­˜å›¾åƒç±»å‹åˆ°scenes JSONæ–‡ä»¶
        self.save_second_animation_to_scenes_json(self.current_scene_index, selected_image_type)
        
        # æ ‡è®°é…ç½®å·²æ›´æ”¹
        self._config_changed = True


    def update_scene_field(self, scene_index, field_name, field_value):
        """æ›´æ–°å•ä¸ªåœºæ™¯çš„ç‰¹å®šå­—æ®µ"""
        try:
            workflow = self.workflow
            
            if scene_index >= len(workflow.scenes):
                print(f"âŒ åœºæ™¯ç´¢å¼• {scene_index} è¶…å‡ºèŒƒå›´")
                return False
            
            # è°ƒè¯•ï¼šæ˜¾ç¤ºæ›´æ–°å‰çš„çŠ¶æ€
            old_value = workflow.scenes[scene_index].get(field_name, "æœªè®¾ç½®")
            print(f"ğŸ” è°ƒè¯•: åœºæ™¯ {scene_index + 1} çš„ {field_name} ä» '{old_value}' æ›´æ–°ä¸º '{field_value}'")
            
            # æ›´æ–°workflowå†…å­˜ä¸­çš„æ•°æ®
            workflow.scenes[scene_index][field_name] = field_value
            
            # éªŒè¯æ›´æ–°
            new_value = workflow.scenes[scene_index].get(field_name)
            print(f"âœ… éªŒè¯: åœºæ™¯ {scene_index + 1} çš„ {field_name} ç°åœ¨æ˜¯ '{new_value}'")
            
            return self.workflow.save_scenes_to_json()
            
        except Exception as e:
            print(f"âŒ æ›´æ–°åœºæ™¯å­—æ®µå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False


    def update_scene_fields(self, scene_index, field_updates):
        """æ‰¹é‡æ›´æ–°å•ä¸ªåœºæ™¯çš„å¤šä¸ªå­—æ®µ"""
        try:
            workflow = self.workflow
            
            if scene_index >= len(workflow.scenes):
                print(f"âŒ åœºæ™¯ç´¢å¼• {scene_index} è¶…å‡ºèŒƒå›´")
                return False
            
            # æ‰¹é‡æ›´æ–°workflowå†…å­˜ä¸­çš„æ•°æ®
            for field_name, field_value in field_updates.items():
                workflow.scenes[scene_index][field_name] = field_value
            # ç«‹å³ä¿å­˜åˆ°JSONæ–‡ä»¶
            field_names = list(field_updates.keys())
            return self.workflow.save_scenes_to_json()
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡æ›´æ–°åœºæ™¯å­—æ®µå¤±è´¥: {str(e)}")
            return False

        
    def save_second_animation_to_scenes_json(self, scene_index, image_type):
        """ä¿å­˜å•ä¸ªåœºæ™¯çš„å›¾åƒç±»å‹åˆ°scenes JSONæ–‡ä»¶"""
        return self.update_scene_field(scene_index, "second_animation", image_type)
        

    def generate_video(self, scene, previous_scene, next_scene, track):
        image_path = get_file_path(scene, track+"_image")
        image_last_path = get_file_path(scene, track+"_image_last")

        animate_mode = scene.get(track+"_animation", "")
        if animate_mode not in config_prompt.ANIMATE_SOURCE or animate_mode.strip() == "":
            return

        wan_prompt = scene.get(track+"_prompt", "")
        
        # å¦‚æœ wan_prompt æ˜¯å­—ç¬¦ä¸²ï¼ˆJSONæ ¼å¼ï¼‰ï¼Œå°è¯•è§£æä¸ºå­—å…¸
        if isinstance(wan_prompt, str) and wan_prompt.strip():
            try:
                import json
                p = json.loads(wan_prompt)
                wan_prompt = p
            except:
                print("none json wan_prompt")
        
        # æ£€æŸ¥ prompt æ˜¯å¦ä¸ºç©ºï¼ˆæ”¯æŒå­—ç¬¦ä¸²å’Œå­—å…¸ä¸¤ç§æ ¼å¼ï¼‰
        if not wan_prompt or (isinstance(wan_prompt, str) and wan_prompt.strip() == "") or (isinstance(wan_prompt, dict) and len(wan_prompt) == 0):
            #wan_prompt = self.workflow.build_prompt(scene, "", "", track, animate_mode, False, self.workflow.language)
            wan_prompt = "..."
            scene[track+"_prompt"] = wan_prompt

        action_path = get_file_path(scene, self.selected_second_track)

        sound_path = get_file_path(scene, "clip_audio")
        next_sound_path = get_file_path(next_scene, "clip_audio")

        self.workflow.rebuild_scene_video(scene, track, animate_mode, image_path, image_last_path, sound_path, next_sound_path, action_path, wan_prompt)
        self.workflow.save_scenes_to_json()


    def regenerate_video(self, track):
        """æ‰“å¼€ WAN æç¤ºè¯ç¼–è¾‘å¯¹è¯æ¡†å¹¶ç”Ÿæˆä¸»è½¨é“è§†é¢‘"""
        if track == None:
            track = self.selected_second_track

        scene = self.get_current_scene()
        previous_scene = self.get_previous_scene()
        next_scene = self.get_next_scene()
        
        # å®šä¹‰ç”Ÿæˆè§†é¢‘çš„å›è°ƒå‡½æ•°
        def generate_callback(wan_prompt):
            # ä¿å­˜æç¤ºè¯
            scene[track+"_prompt"] = wan_prompt
            # ä½¿ç”¨ç¼–è¾‘åçš„ prompt ç”Ÿæˆè§†é¢‘
            self.generate_video(scene, previous_scene, next_scene, track)
            # ç›‘æ§å·²é›†æˆåˆ°åå°å®šæ—¶å™¨ä¸­ï¼Œæ— éœ€å•ç‹¬è°ƒç”¨ trace_scene_wan_video
            # åå°æ£€æŸ¥ä¼šè‡ªåŠ¨å¼€å§‹ç›‘æ§æœ‰ clip_animation çš„åœºæ™¯
            self.workflow.save_scenes_to_json()
            self.refresh_gui_scenes()
        
        # æ˜¾ç¤ºç¼–è¾‘å¯¹è¯æ¡†
        show_wan_prompt_editor(self, self.workflow, generate_callback, scene, track)
 

    def regenerate_audio(self):
        """éŸ³é¢‘é‡ç”Ÿ"""
        scene = self.get_current_scene()
        t, mix_audio = self.workflow.regenerate_audio_item(scene, 0, self.workflow.language)

        olda, clip_audio = refresh_scene_media(scene, "clip_audio", ".wav", mix_audio)

        clip_video = get_file_path(scene, "clip")
        if clip_video:
            clip_video = self.workflow.ffmpeg_processor.add_audio_to_video(clip_video, clip_audio)
            oldv, clip_video = refresh_scene_media(scene, "clip", ".mp4", clip_video)

        self.refresh_gui_scenes()



    def update_scene_buttons_state(self):
        """æ›´æ–°åœºæ™¯æ’å…¥æŒ‰é’®çš„çŠ¶æ€"""
        current_scene = self.get_current_scene()
        
        # æ›´æ–°å‰æ’æŒ‰é’®çŠ¶æ€
        if not current_scene or self.workflow.first_scene_of_story(current_scene):
            self.insert_scene_button.config(state="normal")
        else:
            self.insert_scene_button.config(state="disabled")
        
        # æ›´æ–°åæ’æŒ‰é’®çŠ¶æ€
        if current_scene and self.workflow.last_scene_of_story(current_scene):
            self.append_scene_button.config(state="normal")
        else:
            self.append_scene_button.config(state="disabled")




def main():
    root = TkinterDnD.Tk()

    app = WorkflowGUI(root)
    root.mainloop()

if __name__ == "__main__":
    main()

