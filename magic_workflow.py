from hashlib import new
from urllib.parse import urlparse, parse_qs

from torch._inductor.ir import NoneAsConstantBuffer
import config_prompt
from utility.audio_transcriber import AudioTranscriber
from utility.youtube_downloader import YoutubeDownloader
from utility.sd_image_processor import SDProcessor
from utility.ffmpeg_processor import FfmpegProcessor
from utility.ffmpeg_audio_processor import FfmpegAudioProcessor
import os, time
import json
import shutil
import threading
import requests
import re
from pathlib import Path
from config import azure_subscription_key, azure_region
import config
from typing import List, Dict, Optional, Union, Any, Generator
from datetime import datetime
from io import BytesIO
from utility.file_util import get_file_path, safe_remove, copy_file
from utility.minimax_speech_service import MinimaxSpeechService
from gui.image_prompts_review_dialog import IMAGE_PROMPT_OPTIONS, NEGATIVE_PROMPT_OPTIONS
from utility.llm_api import LLMApi
from project_manager import PROJECT_TYPE_STORY, PROJECT_TYPE_SONG, PROJECT_TYPE_MUSIC, PROJECT_TYPE_TALK, PROJECT_TYPE_LIST
import project_manager


class MagicWorkflow:

    def __init__(self, pid, project_type, language, channel, story_site, video_width=None, video_height=None):
        self.pid = pid
        self.project_type = project_type
        self.language = language
        self.channel = channel
        self.story_site = story_site
        self.media_count = 0
        
        # ÂÖ®Â±ÄÁ∫øÁ®ãÁÆ°ÁêÜ
        self.background_threads = []

        # Get video dimensions from parameters or load from project config
        if video_width is None or video_height is None:
            # Try to load from project config
            try:
                from project_manager import ProjectConfigManager
                config_manager = ProjectConfigManager(pid)
                if project_manager.PROJECT_CONFIG:
                    video_width = project_manager.PROJECT_CONFIG.get('video_width')
                    video_height = project_manager.PROJECT_CONFIG.get('video_height')
            except Exception as e:
                print(f"‚ö†Ô∏è  Could not load video dimensions from project config: {e}")
        
        self.ffmpeg_processor = FfmpegProcessor(pid, language, video_width, video_height)
        self.ffmpeg_audio_processor = FfmpegAudioProcessor(pid)
        self.sd_processor = SDProcessor(self)
        self.downloader = YoutubeDownloader(pid)
        self.llm_api = LLMApi()
        self.speech_service = MinimaxSpeechService(self.pid)
        self.transcriber = AudioTranscriber(self, model_size="small", device="cuda")

        config.create_project_path(pid)

        # Create project paths
        self.publish_path = config.PUBLISH_PATH + "/"
        self.project_path = config.get_project_path(pid)
        self.channel_path = config.get_channel_path(channel)
        self.effect_path = config.get_effect_path()

        self.short_conversation_path = f"{self.project_path}/{self.pid}_{self.language}_short_1.json"
        if os.path.exists(self.short_conversation_path):
            with open(self.short_conversation_path, "r", encoding="utf-8") as f:
                self.short_conversation = f.read()
        else:
            self.short_conversation = ""

        self.negative_prompt = NEGATIVE_PROMPT_OPTIONS[0]

        self.font_size = 14
        self.language = language
        if language == "tw":
            self.font_video = config.FONT_9
            self.font_title = config.FONT_8
        elif language == 'jp':
            self.font_video = config.FONT_11
            self.font_title = config.FONT_12
        elif language == 'kr':
            self.font_video = config.FONT_20
            self.font_title = config.FONT_20
        elif language == 'ar':
            self.font_video = config.FONT_13
            self.font_title = config.FONT_13
        elif language == 'th':
            self.font_video = config.FONT_14
            self.font_title = config.FONT_14
        elif language == 'ti':
            self.font_video = config.FONT_16
            self.font_title = config.FONT_16
        elif language == 'mo':
            self.font_video = config.FONT_17
            self.font_title = config.FONT_17
        else:
            self.font_video = config.FONT_4
            self.font_title = config.FONT_0

        self.title = ""
        self.program_keywords = None

        self.background_image = None
        self.background_music = None
        self.background_video = None

        self.load_scenes()

 
    def post_init(self, title, keywords):
        if title:
            self.title = self.transcriber.translate_text(title, self.language, self.language)

        keywords_list = keywords.split(',') if keywords else []
        self.program_keywords = [kw.strip() for kw in keywords_list if kw.strip()]


    def project_169_mode(self):
        return self.ffmpeg_processor.width > self.ffmpeg_processor.height


    def clean_folder(self, folder_name):
        folder = Path(folder_name)
        # Check if original folder exists
        if not folder.exists():
            return
        subdirs = [d for d in folder.iterdir() if d.is_dir()]
        if not subdirs:
            return

        for subdir in subdirs:
            shutil.rmtree(subdir)


    def clean_media(self):
        """Â™í‰ΩìÊ∏ÖÁêÜ"""
        valid_media_files = []
        for scene in self.scenes:
            zero = get_file_path(scene, "zero")
            if zero:
                valid_media_files.append(zero)
            zero_audio = get_file_path(scene, "zero_audio")
            if zero_audio:
                valid_media_files.append(zero_audio)

            second = get_file_path(scene, "second")
            if second:
                valid_media_files.append(second)
            second_audio = get_file_path(scene, "second_audio")
            if second_audio:
                valid_media_files.append(second_audio)

            clip_audio = get_file_path(scene, "clip_audio")
            if clip_audio:
                valid_media_files.append(clip_audio)
            clip_video = get_file_path(scene, "clip")
            if clip_video:
                valid_media_files.append(clip_video)

            clip_left = get_file_path(scene, "clip_left")
            if clip_left:
                valid_media_files.append(clip_left)
            clip_right = get_file_path(scene, "clip_right")
            if clip_right:
                valid_media_files.append(clip_right)

            second_left = get_file_path(scene, "second_left")
            if second_left:
                valid_media_files.append(second_left)
            second_right = get_file_path(scene, "second_right")
            if second_right:
                valid_media_files.append(second_right)

            zero_left = get_file_path(scene, "zero_left")
            if zero_left:
                valid_media_files.append(zero_left)
            zero_right = get_file_path(scene, "zero_right")
            if zero_right:
                valid_media_files.append(zero_right)

            back_video = get_file_path(scene, "back")
            if back_video:
                backs = back_video.split(',')
                for back in backs:
                    if back and os.path.exists(back):
                        valid_media_files.append(back)

            clip_image = get_file_path(scene, "clip_image")
            if clip_image:
                valid_media_files.append(clip_image)
            clip_image_last = get_file_path(scene, "clip_image_last")
            if clip_image_last:
                valid_media_files.append(clip_image_last)

            second_image = get_file_path(scene, "second_image")
            if second_image:
                valid_media_files.append(second_image)
            second_image_last = get_file_path(scene, "second_image_last")
            if second_image_last:
                valid_media_files.append(second_image_last)

            zero_image = get_file_path(scene, "zero_image")
            if zero_image:
                valid_media_files.append(zero_image)
            zero_image_last = get_file_path(scene, "zero_image_last")
            if zero_image_last:
                valid_media_files.append(zero_image_last)

            speak_audio = get_file_path(scene, "speak_audio")
            if speak_audio:
                valid_media_files.append(speak_audio)
            main_audio = get_file_path(scene, "main_audio")
            if main_audio:
                valid_media_files.append(main_audio)
            main_video = get_file_path(scene, "main_video")
            if main_video:
                valid_media_files.append(main_video)

        # list all files inside the project_path/media folder
        all_media_files = []
        for file in os.listdir(config.get_media_path(self.pid)):
            all_media_files.append(config.get_media_path(self.pid) + "/" + file)

        # list all temp files inside the project_path/temp folder
        for file in os.listdir(config.get_temp_path(self.pid)):
            all_media_files.append(config.get_temp_path(self.pid) + "/" + file)
            #safe_remove(config.get_temp_path(self.pid) + "/" + file)

        # try to remove all files in all_media_files that are not in valid_media_files
        for file in all_media_files:
            if file not in valid_media_files:
                safe_remove(file)


    def build_prompt(self, scene_data, style, extra, track, av_type, start, language):
        prompt_dict = {}

        # ÊèêÂèñÂΩìÂâçÂú∫ÊôØÁöÑÂÖ≥ÈîÆ‰ø°ÊÅØ
        speaker = scene_data.get("speaker", "")
        speaker_position = scene_data.get("speaker_position", "")
        person_mood = scene_data.get("mood", "calm")

        if ("clip" in track or "zero" in track) and "IMAGE" in av_type:
            if style:
                prompt_dict["IMAGE_STYLE"] = style

        if "second" in track or "zero" in track:
            if "content" in scene_data:
                content = scene_data['content']
                if speaker:
                    if speaker_position and speaker_position == "left":
                        prompt_dict["SPEAKING"] = f"the person on left-side is in {person_mood} mood to say ({content}),  while acting like ({scene_data['speaker_action']})"
                        if av_type == "WS2V":
                            prompt_dict["LISTENING"] = f"the person on right-side is listening this content : {content}, while engaging with lots of reactions, expressions, and hand movements."
                    elif speaker_position and speaker_position == "right":
                        prompt_dict["SPEAKING"] = f"the person on right-side is in {person_mood} mood to say ({content}),  while acting like ({scene_data['speaker_action']})"
                        if av_type == "WS2V":
                            prompt_dict["LISTENING"] = f"the person on left-side is listening this content : ({content}), while engaging with lots of reactions, expressions, and hand movements."
                    else:
                        prompt_dict["SPEAKING"] = f"the person is in {person_mood} mood to say ({content}),  while acting like ({scene_data['speaker_action']})"
                else:
                    prompt_dict["SPEAKING"] = content

        if "clip" in track or "zero" in track:
            person_lower = scene_data['subject'].lower()
            if person_lower:
                has_negative_pattern = (
                    re.search(r'\bno\b.*\bpersons?\b', person_lower) or  # matches "no person", "no persons", "no specific person"
                    re.search(r'\bno\b.*\bcharacters?\b', person_lower) or  # matches "no character", "no characters", "no other character"
                    re.search(r'\bn/a\b', person_lower) or  # matches "n/a"
                    re.search(r'\bnone\b', person_lower)  # matches "none"
                )
                if not has_negative_pattern:
                    prompt_dict["PERSON"] = ""
                    if av_type in config.ANIMATE_S2V and self.project_type == PROJECT_TYPE_SONG:
                        prompt_dict["PERSON"] = prompt_dict["PERSON"]+"Singing with body/hand movements.\n"
                    if person_mood and person_mood != "":
                        prompt_dict["PERSON"] = prompt_dict["PERSON"]+f"The person is in {person_mood} mood.\n"

        if language == "en":
            prompt_dict["subject"] = self.transcriber.translate_text(scene_data['subject'], self.language, "en")
        else:
            prompt_dict["subject"] = scene_data['subject']

        if language == "en":
            visual_start = self.transcriber.translate_text(scene_data['visual_start'], self.language, "en")
            visual_end = self.transcriber.translate_text(scene_data['visual_end'], self.language, "en")
        else:
            visual_start = scene_data['visual_start']
            visual_end = scene_data['visual_end']

        if start:
            prompt_dict["VISUAL"] = visual_start
        else:
            prompt_dict["VISUAL"] = visual_end + "\n this image is following with the image like: " + visual_start

        if "era_time" in scene_data:
            prompt_dict["ERA_TIME"] = scene_data['era_time']
        if "environment" in scene_data:
            prompt_dict["ENVIRONMENT"] = scene_data['environment']
        if "cinematography" in scene_data:
            prompt_dict["CINEMATOGRAPHY"] = scene_data['cinematography']
        #if "sound_effect" in scene_data:
        #    prompt_dict["SOUND_EFFECT"] = scene_data['sound_effect']

        if extra:
            prompt_dict["FYI"] = extra

        return prompt_dict


    def create_story_images(self, story_json_content, image_style, extra_description, negative, image_folder):
        """ÂàõÂª∫ÊïÖ‰∫ãÂõæÂÉè"""
        print(f"üé≠ ÂºÄÂßãÂàõÂª∫Ê≤âÊµ∏ÂºèÊïÖ‰∫ãÈü≥È¢ë...")
        # ËØªÂèñÊ≤âÊµ∏ÊïÖ‰∫ãJSON
        story_json = json.loads(story_json_content)
        
        if not story_json:
            print(f"‚ùå Ê≤âÊµ∏ÊïÖ‰∫ãÂÜÖÂÆπ‰∏∫Á©∫")
            return None
        
        print(f"üìù Ê≤âÊµ∏ÊïÖ‰∫ãÂåÖÂê´ {len(story_json)} ‰∏™ÂØπËØùÁâáÊÆµ")

        for i,conversation in enumerate(story_json):
            scene = {}
            scene["clip_image"] = f"{config.get_project_path(self.pid)}/{image_folder}/{i}.png"
            scene["summary"] = conversation["english_explanation"],

            self._create_image(   self.workflow.sd_processor.gen_config["Story"], 
                                                scene, 
                                                "clip",
                                                image_style, 
                                                extra_description, 
                                                negative,
                                                int(time.time())    )
        return  True 


    def create_story_audio(self, story_json_path, audio_path, video_duration):
        """ÂàõÂª∫Ê≤âÊµ∏ÂºèÊïÖ‰∫ãÈü≥È¢ë"""
        try:
            print(f"üé≠ ÂºÄÂßãÂàõÂª∫Ê≤âÊµ∏ÂºèÊïÖ‰∫ãÈü≥È¢ë...")
            with open(story_json_path, 'r', encoding='utf-8') as f:
                immersive_story_json = json.load(f)
            
            if not immersive_story_json:
                print(f"‚ùå Ê≤âÊµ∏ÊïÖ‰∫ãÂÜÖÂÆπ‰∏∫Á©∫")
                return None
            
            print(f"üìù Ê≤âÊµ∏ÊïÖ‰∫ãÂåÖÂê´ {len(immersive_story_json)} ‰∏™ÂØπËØùÁâáÊÆµ")
            
            speed_percentage = self.calculate_speed_percentage(7.0, video_duration)

            # ‰∏∫ÊØè‰∏™ÂØπËØùÁâáÊÆµËÆæÁΩÆÈÄüÂ∫¶ÂíåÈü≥Ë∞É
            for item in immersive_story_json:
                item["speed"] = speed_percentage  # Ê≠£Â∏∏ÈÄüÂ∫¶
                item["pitch"] = speed_percentage  # Ê≠£Â∏∏Èü≥Ë∞É
            
            # ÁîüÊàêSSMLÂπ∂ËΩ¨Êç¢‰∏∫ËØ≠Èü≥
            # ssml = self.tts_service.make_ssml("250ms", immersive_story_json)
            #print(f"üéµ Ê≤âÊµ∏ÊïÖ‰∫ãSSML:\n {ssml}")
            
            # ÁîüÊàêÈü≥È¢ëÊñá‰ª∂
            temp_audio_path = self.tts_service.generate_audio(immersive_story_json, audio_path)
            if temp_audio_path and os.path.exists(temp_audio_path):
                os.replace(temp_audio_path, audio_path)
                print(f"üéµ Ê≤âÊµ∏ÊïÖ‰∫ãÈü≥È¢ëÊñá‰ª∂Â∑≤ÁîüÊàê: {audio_path}")
            else:
                print(f"‚ùå Ê≤âÊµ∏ÊïÖ‰∫ãÈü≥È¢ëÁîüÊàêÂ§±Ë¥•")
            
        except Exception as e:
            print(f"‚ùå ÂàõÂª∫Ê≤âÊµ∏ÂºèÊïÖ‰∫ãÈü≥È¢ëÊó∂ÂèëÁîüÈîôËØØ: {str(e)}")
            import traceback
            traceback.print_exc()

        return audio_path


    # style: choices of  '1 male & 1 female) hosts', '1 male host', '1 female host', '1 host & 2 actors', '2 hosts & 2 actors'
    # topic: the topic of the dialogue (text field)
    # avoid_content: the content to avoid in the dialogue (text field)
    # location: the location of the dialogue (text field)
    # general_location: the name of the large site (from existing ÊïÖ‰∫ãÂú∫Âú∞ field)
    # dialogue_openning: the opening of the dialogue (text field)
    # dialogue_ending: the ending of the dialogue (text field)
    # previous_dialogue: the previous dialogue (drag/drop mp3 or txt file, to the image area [media/wave_sound.png]) in left side 
    # introducation_story: the introducation story (drag/drop mp3 or txt file, to the image area [media/wave_sound.png]) in right side 
    def prepare_notebooklm_for_project(self, style, topic, avoid_content, location, general_location, previous_dialogue, introduction_story, introduction_type):
        if avoid_content and avoid_content.strip() != "":
            avoid_content = f"""
            "Avoid_Content" : "Try to avoid content like '{avoid_content}'",
            """
        else:
            avoid_content = ""
            
        if introduction_story and introduction_story.strip() != "":
            if introduction_story.endswith(".mp3") or introduction_story.endswith(".wav"):
                introduction_story = self.transcriber.transcribe_to_file(introduction_story, "zh", 10, 26)
            if not introduction_story:
                return None

            user_prompt = self.transcriber.fetch_text_from_json(introduction_story)
            
            introduction_story = self.llm_api.generate_text(config_prompt.STORY_SUMMARY_SYSTEM_PROMPT, user_prompt)

            introduction_story = f"""
            "Introducation_story" : "The hosts start the dialogue just after they {introduction_type}, that talks about : '{introduction_story}'.     (the dialogue is carried out immediately after this talk)",
            """
        else:
            introduction_story = ""

        if previous_dialogue and previous_dialogue.strip() != "":
            if previous_dialogue.endswith(".mp3") or previous_dialogue.endswith(".wav"):
                previous_dialogue = self.transcriber.transcribe_to_file(previous_dialogue, "zh", 10, 26)
            if not previous_dialogue:
                return None

            user_prompt = self.transcriber.fetch_text_from_json(previous_dialogue)

            previous_dialogue = self.llm_api.generate_text(config_prompt.STORY_SUMMARY_SYSTEM_PROMPT, user_prompt)

            previous_dialogue = f"""
                "Previous_Dialogue" : "This dialogue follows the previous story-telling-dialogue, talking about : '{previous_dialogue}'.    !!! This dialogue may mention the previous content quickly, but DO NOT talking the details again !!!",
            """
        else:
            previous_dialogue = ""

        user_prompt = config.fetch_story_extract_text_content(self.pid, self.language)


        if general_location:
            dialogue_opening = self.llm_api.generate_text(config_prompt.NOTEBOOKLM_OPENING_DIALOGUE_PROMPT.format(location=location), user_prompt)
            dialogue_opening = f"""
                "Dialogue_Openning" : "The dialogue should open with Immersive-Narrative like : '{dialogue_opening}'.  (Don't directly use, please re-organize / re-phrase the content in more infectious way)",
                """
    
            dialogue_ending = self.llm_api.generate_text(config_prompt.NOTEBOOKLM_ENDING_DIALOGUE_PROMPT.format(location=location), user_prompt)
            dialogue_ending = f"""
                "Dialogue_Ending" : "The dialogue should end like : '{dialogue_ending}'.     (Don't directly use, please re-organize / re-phrase the content in more infectious way)",
                """

            immersive_env_scene = self.llm_api.generate_text(config_prompt.NOTEBOOKLM_LOCATION_ENVIRONMENT_PROMPT.format(location=location, general_location=general_location))
            # replace all new line characters with space
            immersive_env_scene = immersive_env_scene.replace("\n", " ").replace("\r", " ")
            location = f"""
                "Location" : "the dialogue happens at: '{location}'; The enviroment is like '{immersive_env_scene}'",
                """
        else:
            dialogue_opening = ""
            dialogue_ending = ""
            location = ""


        return config_prompt.NOTEBOOKLM_PROMPT.format(
            style=style,
            topic=topic,
            avoid_content=avoid_content,
            location=location,
            previous_dialogue=previous_dialogue,
            introduction_story=introduction_story,
            dialogue_openning=dialogue_opening,
            dialogue_ending=dialogue_ending
        )
 
 
    def create_titles_and_tags(self):
        system_prompt = config_prompt.TITLE_SUMMARIZATION_SYSTEM_PROMPT.format(
            language=config.LANGUAGES[self.language],
            length=5
        )
        user_prompt = self.transcriber.fetch_text_from_json(config.get_project_path(self.pid)+"/main.srt.json")
        
        title_json_path = config.get_titles_path(self.pid, self.language)
        return self.llm_api.generate_json_summary(system_prompt, user_prompt, title_json_path)


    def prepare_suno_music(self, suno_lang, content, atmosphere, expression, structure, 
                                leading_melody, instruments, rhythm_groove):
        
        content = self.llm_api.generate_text(config_prompt.SUNO_CONTENT_ENHANCE_SYSTEM_PROMPT, content)
        suno_style_prompt = config_prompt.SUNO_STYLE_PROMPT.format(
            target=suno_lang,
            atmosphere=atmosphere,
            expression=expression+" ("+config.SUNO_CONTENT[expression]+")",
            structure=structure,
            melody=leading_melody,
            instruments=instruments,
            rhythm=rhythm_groove
        )
        # Build enhanced system prompt with new parameters
        music_prompt = self.llm_api.generate_json_summary(config_prompt.SUNO_MUSIC_SYSTEM_PROMPT.format(language_style=suno_lang), content)

        if music_prompt and len(music_prompt) > 0:
            content += "\n*** " + music_prompt[0]["music_expression"]
            content += "\n\n\n***" + suno_style_prompt
            content += "\n\n\n[LYRICS]\n" + music_prompt[0]["lyrics_suggestion"]

        suno_prompt_path = config.get_project_path(self.pid) + "/suno_prompt.txt"
        with open(suno_prompt_path, "w", encoding="utf-8") as f:
            f.write(content)
        return content


    def find_clip_duration(self, current_scene):
        clip_audio = get_file_path(current_scene, "clip_audio")
        duration = None
        if clip_audio:
            duration = self.ffmpeg_audio_processor.get_duration(clip_audio)

        if not duration:
            clip_video = get_file_path(current_scene, "clip")
            duration = self.ffmpeg_processor.get_duration(clip_video)

        current_scene["duration"] = duration
        return duration


    def get_scene_detail(self, scene):
        indx = -1
        for i in range(len(self.scenes)):
            if self.scenes[i] is scene:
                indx = i
                break

        if indx < 0:
            return 0.0, 0.0, 0.0, -1, 0, False # not found

        clip_duration = self.find_clip_duration(scene)

        ss = self.scenes_in_story(scene)
        story_duration = 0.0
        for s in ss:
            story_duration += self.find_clip_duration(s)
        start_time_in_story = 0.0
        for s in ss:
            if s == scene:
                break
            start_time_in_story += self.find_clip_duration(s)

        return start_time_in_story, clip_duration, story_duration, indx, len(ss), s == ss[-1]


    def merge_scene(self, from_index, to_index):
        if not (from_index-to_index==1 or from_index-to_index==-1):
            return False
        if from_index > len(self.scenes) - 1 or from_index < 0:
            return False
        if to_index > len(self.scenes) - 1 or to_index < 0:
            return False

        from_scene = self.scenes[from_index]
        to_scene = self.scenes[to_index]

        # merged_duration = self.find_duration(from_scene) + self.find_duration(to_scene)
        if from_index > to_index:
            from_scene["content"] = to_scene["content"] + ". " + from_scene["content"]
            audio_list = [get_file_path(to_scene, "clip_audio"), get_file_path(from_scene, "clip_audio")]
            video_list = [get_file_path(to_scene, "clip"), get_file_path(from_scene, "clip")]
        else:
            from_scene["content"] = from_scene["content"] + ". " + to_scene["content"]
            audio_list = [get_file_path(from_scene, "clip_audio"), get_file_path(to_scene, "clip_audio")]
            video_list = [get_file_path(from_scene, "clip"), get_file_path(to_scene, "clip")]

        same_main_scenes = self.scenes_in_story(from_scene)
        if len(same_main_scenes) > 1:
            self.refresh_scene_media(from_scene, "clip_audio", ".wav",  self.ffmpeg_audio_processor.concat_audios(audio_list))
            self.refresh_scene_media(from_scene, "clip", ".mp4",  self.ffmpeg_processor.concat_videos(video_list, True))
        else:
            self.refresh_scene_media(from_scene, "clip_audio", ".wav", get_file_path(from_scene, "main_audio"), True)
            self.refresh_scene_media(from_scene, "clip", ".mp4", get_file_path(from_scene, "main_video"), True)

        del self.scenes[to_index]

        self.refresh_scene_visual(from_scene)

        #self._generate_video_from_image(from_scene)
        return True


    def clone_scene(self, current_index, is_append=False):
        if current_index < 0 or current_index >= len(self.scenes):
            return False

        if is_append:
            new_scenes = [self.scenes[current_index], self.scenes[current_index].copy()]
            self.replace_scene_with_others(current_index+1, new_scenes)
        else:
            new_scenes = [self.scenes[current_index].copy(), self.scenes[current_index]]
            self.replace_scene_with_others(current_index, new_scenes)


    def replace_scene(self, current_index, new_scene=None):
        if current_index >= len(self.scenes):
            return None

        old_scene = self.scenes[current_index]
        ss = self.scenes_in_story(old_scene)
        
        if new_scene:
            self.scenes[current_index] = new_scene
        else:
            del self.scenes[current_index]

        if len(ss) == 1:
            return None

        # delete old_scene from ss
        ss.remove(old_scene)
        return ss


    def replace_scene_with_others(self, current_index, new_scenes):
        if current_index < 0 or current_index >= len(self.scenes):
            return False  # invalid index

        # Replace the single item with the list of new scenes
        self.scenes = (
            self.scenes[:current_index] +
            new_scenes +
            self.scenes[current_index + 1:]
        )
        self.save_scenes_to_json()
        return True


    def split_scene_at_position(self, n, position):
        """ÂàÜÁ¶ªÂΩìÂâçÂú∫ÊôØ"""
        if n<0  or n >= len(self.scenes):
            return False

        current_scene = self.scenes[n]

        original_duration = self.find_clip_duration(current_scene)
        if position<=0 or position >= original_duration:
            return False

        original_content = current_scene.get("content", "")
        original_audio_clip = get_file_path(current_scene, "clip_audio")
        original_video_clip = get_file_path(current_scene, "clip")

        next_scene = current_scene.copy()

        self.replace_scene_with_others(n, [current_scene, next_scene])

        current_ratio = position / original_duration

        first, second = self.ffmpeg_audio_processor.split_audio(original_audio_clip, position)
        self.refresh_scene_media(current_scene, "clip_audio", ".wav", first)
        self.refresh_scene_media(next_scene, "clip_audio", ".wav", second)

        first, second = self.ffmpeg_processor.split_video(original_video_clip, position)
        self.refresh_scene_media(current_scene, "clip", ".mp4", first)
        self.refresh_scene_media(next_scene, "clip", ".mp4", second)

        # max section id -->  raw_id = int((raw_scene["id"]/100)*100)
        # every 100 is a section of id, so we need to find the max section id out of same section
        max_section_id = current_scene["id"]
        current_section = current_scene["id"] // 100
        for s in self.scenes:
            # Ê£ÄÊü•ÊòØÂê¶Âú®Âêå‰∏Äsection
            if s["id"] // 100 != current_section:
                continue
            # Âú®Âêå‰∏ÄsectionÂÜÖÔºåÊâæÊúÄÂ§ßÁöÑid
            if s["id"] > max_section_id:
                max_section_id = s["id"]

        current_scene["content"] = original_content[:int(len(original_content)*current_ratio)]
        current_scene["id"] = max_section_id + 1
        next_scene["content"] = original_content[int(len(original_content)*(1.0-current_ratio)):]
        next_scene["id"] = max_section_id + 2

        #self._generate_video_from_image(current_scene)
        #self._generate_video_from_image(next_scene)

        #self.refresh_scene(current_scene)
        #self.refresh_scene(next_scene)

        self.save_scenes_to_json()

        return True


    def shift_scene(self, n, m, position):
        """ÂàÜÁ¶ª‰∏∫nÂº†ÂõæÁâá"""
        if n<0  or n > len(self.scenes) or m<0  or m > len(self.scenes):
            return False

        if abs(n-m) != 1:
            return False
        
        current_scene = self.scenes[n]
        next_scene = self.scenes[m]

        original_audio_clip = get_file_path(current_scene, "clip_audio")
        original_video_clip = get_file_path(current_scene, "clip")     
        original_duration = self.ffmpeg_audio_processor.get_duration(original_audio_clip)
        if position<=0 or position >= original_duration:
            return False

        firsta, seconda = self.ffmpeg_audio_processor.split_audio(original_audio_clip, position)

        if n < m: 
            self.refresh_scene_media(current_scene, "clip_audio", ".wav", firsta)
            mergeda = self.ffmpeg_audio_processor.concat_audios([seconda, get_file_path(next_scene, "clip_audio")])
            self.refresh_scene_media(next_scene, "clip_audio", ".wav", mergeda)
        else:
            self.refresh_scene_media(current_scene, "clip_audio", ".wav", seconda)
            mergeda = self.ffmpeg_audio_processor.concat_audios([get_file_path(next_scene, "clip_audio"), firsta])
            self.refresh_scene_media(next_scene, "clip_audio", ".wav", mergeda)

        current_video = get_file_path(current_scene, "clip")
        current_video = self.ffmpeg_processor.add_audio_to_video(current_video, current_scene["clip_audio"])
        self.refresh_scene_media(current_scene, "clip", ".mp4", current_video)

        next_video = get_file_path(next_scene, "clip")
        next_video = self.ffmpeg_processor.add_audio_to_video(next_video, next_scene["clip_audio"])  
        self.refresh_scene_media(next_scene, "clip", ".mp4", next_video)

        self.save_scenes_to_json()

        return True


    def _create_image(self, sd_config, new_image_path, figures, positive, negative, seed):
        if self.ffmpeg_processor.width >= self.ffmpeg_processor.height:
            sd_width, sd_height = 1920, 1080
        else:
            sd_width, sd_height = 1080, 1920

        if sd_config["model"] == "sd":
            image = self.sd_processor.text2Image_sd(
                positive,
                negative, 
                sd_config["url"], 
                sd_config["cfg"], 
                seed or sd_config["seed"], 
                sd_config["steps"], 
                sd_width, 
                sd_height
            )
        elif sd_config["model"] == "banana":
            image = self.sd_processor.text2Image_banana(
                url = sd_config["url"], 
                workflow = sd_config["workflow"],
                positive = positive,
                negative = negative,
                image_list = [figures] if figures else [],
                width = sd_width,
                height = sd_height,
                cfg = sd_config["cfg"],
                seed = seed or sd_config["seed"],
                steps = sd_config["steps"],
            )
            
        # Ê£ÄÊü•ÂõæÂÉèÁîüÊàêÊòØÂê¶ÊàêÂäü
        if image is None:
            print("‚ùå ÂõæÂÉèÁîüÊàêÂ§±Ë¥•ÔºåËøîÂõû None")
            return
            
        print(f"üîÑ Áº©ÊîæÂõæÂÉè‰ªé {sd_width}x{sd_height} Âà∞HDÂ∞∫ÂØ∏ {self.ffmpeg_processor.width}x{self.ffmpeg_processor.height}")
        hd_image = self.sd_processor.resize_image(image, self.ffmpeg_processor.width, self.ffmpeg_processor.height)
        # Convert back to binary format
        buffer = BytesIO()
        hd_image.save(buffer, format="PNG")
        hd_image_data = buffer.getvalue()

        self.sd_processor.save_image(hd_image_data, new_image_path)


    def load_scenes(self):
        self.scenes = []
        scenes_file = config.get_scenes_path(self.pid)
        if os.path.exists(scenes_file):
            with open(scenes_file, "r", encoding="utf-8") as f:
                self.scenes = json.load(f)
            return

        system_prompt = config_prompt.PROJECT_STORY_SCENES_PROMPT.format( 
                                            type_name=project_manager.PROJECT_CONFIG.get('project_type', 'story'), 
                                            topic=config.channel_config[project_manager.PROJECT_CONFIG.get('channel', 'default')]["topic"], 
                                            language=config.LANGUAGES[project_manager.PROJECT_CONFIG.get('language', 'zh')]
                                    )
        user_prompt = config_prompt.INITIAL_CONTENT_USER_PROMPT.format(
                                            type_name=project_manager.PROJECT_CONFIG.get('project_type', 'story'), 
                                            topic=config.channel_config[project_manager.PROJECT_CONFIG.get('channel', 'default')]["topic"], 
                                            story=project_manager.PROJECT_CONFIG.get('story', ''), 
                                            inspiration=project_manager.PROJECT_CONFIG.get('inspiration', '')
                                    )

        raw_json_path = f"{config.get_project_path(self.pid)}/scenes.json"
        self.scenes = self.llm_api.generate_json_summary(system_prompt, user_prompt, raw_json_path)
        for scene in self.scenes:
            self.initialize_default_root_scene(scene)

        self.save_scenes_to_json()



    def get_image_main_scenes(self):
        """Ëé∑ÂèñÊâÄÊúâÊ†áËÆ∞‰∏∫IMAGE_MAINÁöÑÂú∫ÊôØÔºåÁî®‰∫éÂà∂‰ΩúÁº©Áï•Âõæ"""
        image_main_scenes = []
        for i, scene in enumerate(self.scenes):
            if scene.get("clip_animation", "") == "IMAGE_MAIN":
                image_main_scenes.append({
                    "index": i,
                    "scene": scene,
                    "image_path": scene.get("clip_image", "")
                })
        return image_main_scenes


    def make_conversation_srt(self, conversation_json, duration):
        content = []
        addup = 2
        for i, item in enumerate(conversation_json):
            #if (i+1) * duration + addup > video_duration:
            #    break
            addup = addup + 1

            content.append(str(i+1))

            start = i * duration + addup
            end = (i+1) * duration + addup
            content.append(f"{start} --> {end}")

            content.append(item["keywords"])
            content.append("\n")

        return self.transcriber.chinese_convert('\n'.join(content), self.language)


    def calculate_speed_percentage(self, default_duration, actual_duration):
        if default_duration <= 0:
            return "+0%"
        
        # Calculate percentage: (default - actual) / default * 100
        percentage = ((default_duration - actual_duration) / default_duration) * 100
        
        # Round to nearest integer
        percentage = int(round(percentage))
        
        # Format as string with + or - sign
        if percentage > 0:
            return f"+{percentage}%"
        elif percentage < 0:
            return f"{percentage}%"  # negative sign is already included
        else:
            return "+0%"


    def prepare_srt(self, subtitle, start_duration, audio_duration):
        #if subtitle is json file, load it, then for each json item, get the 'content' field, concat them by \n, as srt_content
        script_lines = []

        try:
            json_content = json.loads(subtitle)
            srt_content = ""
            for item in json_content:
                script_lines.append(item["content"])
        except Exception as e: 
            for line in subtitle.split('\n'):
                line = line.strip()
                # Skip empty lines (even with spaces)
                if not line:
                    continue
                # Skip lines starting with [ or (
                if line.startswith('[') or line.startswith('('):
                    continue
                # Skip lines with very few characters (under 5)
                if len(line) < 5:
                    continue
                # Skip timestamp lines (format: HH:MM:SS.mmm --> HH:MM:SS.mmm)
                if '-->' in line and ':' in line:
                    # Check if it looks like a timestamp
                    parts = line.split('-->')
                    if len(parts) == 2:
                        left = parts[0].strip()
                        right = parts[1].strip()
                        # Check if both parts contain colons (time format)
                        if ':' in left and ':' in right:
                            continue
                script_lines.append(line)
        
        # Create SRT content
        start_seconds = start_duration
        line_duration = audio_duration / len(script_lines) if script_lines else 0
        srt_content = ""
        
        for i, line in enumerate(script_lines):
            end_seconds = start_seconds + line_duration
            start_seconds_str = start_seconds
            end_seconds_str = end_seconds
            srt_content += f"{i+1}\n{start_seconds_str} --> {end_seconds_str}\n{line}\n\n"
            start_seconds = end_seconds
        
        # Convert content using transcriber
        return self.transcriber.chinese_convert(srt_content, self.language)


    def save_scenes_to_json(self):
        # config.clear_temp_files()
        try:
            with open(config.get_scenes_path(self.pid), "w", encoding="utf-8") as f:
                json.dump(self.scenes, f, ensure_ascii=False, indent=2)
            return True
        except Exception as e:
            print(f"‚ùå ‰øùÂ≠òscenesÂà∞JSONÂ§±Ë¥•: {str(e)}")
            return False


    def scenes_in_story(self, scene):
        if len(self.scenes) == 0 or scene is None:
            return []

        root_id = int(scene["id"]/10000)
        scenes = []
        for s in self.scenes:
            if int(s["id"]/10000) == root_id:
                scenes.append(s)

        scenes.sort(key=lambda x: x["id"])
        return scenes


    def next_scene_of_story(self, scene):
        ss = self.scenes_in_story(scene)
        if len(ss) < 2:
            return None
        # get the index of scene in ss
        index = ss.index(scene)
        if index == len(ss) - 1:
            return None
        return ss[index + 1]


    def first_scene_of_story(self, scene):
        ss = self.scenes_in_story(scene)
        if len(ss) == 0:
            return True
        return ss[0] == scene


    def last_scene_of_story(self, scene):
        ss = self.scenes_in_story(scene)
        if len(ss) == 0:
            return True
        return ss[-1] == scene

        
 
    def replace_scene_second(self, current_scene, source_video_path, source_audio_path):
        oldv, secondv = self.refresh_scene_media(current_scene, "second", ".mp4", source_video_path)
        olda, seconda = self.refresh_scene_media(current_scene, "second_audio", ".wav", source_audio_path)

        for s in self.scenes_in_story(current_scene):
            s["second"] = secondv
            s["second_audio"] = seconda

        self.save_scenes_to_json()


    def is_last_scene(self, scene, scenes):
        if len(scenes) <= 1 or scene is scenes[-1]:
            return True
        try:
            next_scene = scenes[scenes.index(scene) + 1]
            return next_scene["main_audio"] != scene["main_audio"]
        except:
            return True
        

    def refresh_scene_media(self, scene, media_type, media_postfix, replacement=None, make_replacement_copy=False):
        new_media_stem = media_type + "_" + str(scene["id"]) + "_" + str(int(datetime.now().timestamp()*100 + self.media_count%100))
        self.media_count = (self.media_count + 1) % 100

        old_media_path = scene.get(media_type, None)
        scene[media_type] = config.get_media_path(self.pid) + "/" + new_media_stem + media_postfix

        if replacement:
            copy_file(replacement, scene[media_type])
            if not make_replacement_copy:
                safe_remove(replacement)
        return old_media_path, scene[media_type]

 
    def ask_replace_scene_info_from_image(self, current_scene, image_path):
        """ËØ¢ÈóÆÁî®Êà∑ÊòØÂê¶Ë¶ÅÂàÜÊûêÂõæÂÉèÂπ∂Êõ¥Êñ∞Âú∫ÊôØÊï∞ÊçÆ"""
        import tkinter.messagebox as messagebox

        dialog_result = messagebox.askyesno(
            "ÂàÜÊûêÂõæÂÉè", 
            f"ÊòØÂê¶Ë¶ÅÂàÜÊûêÂõæÂÉèÂπ∂Êõ¥Êñ∞Âú∫ÊôØÊï∞ÊçÆÔºü\nÂõæÂÉèË∑ØÂæÑ: {image_path}"
        )
        if not dialog_result:
            return
        
        print(f"üîÑ Ê≠£Âú®ÂàÜÊûêÂõæÂÉè: {image_path}")
        scene_data = self.sd_processor.describe_image(image_path)
        
        if not scene_data:
            messagebox.showerror("ÈîôËØØ", "ÂõæÂÉèÂàÜÊûêÂ§±Ë¥•ÔºåÊó†Ê≥ïËé∑ÂèñÂú∫ÊôØÊï∞ÊçÆ")
            return
        
        # Êõ¥Êñ∞Âú∫ÊôØÊï∞ÊçÆ
        self.try_update_scene_visual_fields(current_scene, scene_data)


    def replace_scene_image(self, current_scene, source_image_path, vertical_line_position, target_field):
        oldi, image_path = self.refresh_scene_media(current_scene, target_field, ".webp", source_image_path)

        current_scene[target_field + "_split"] = vertical_line_position
        clip_image_last = get_file_path(current_scene, target_field + "_last")
        if not clip_image_last:
            current_scene[target_field + "_last"] = image_path

        self.ask_replace_scene_info_from_image(current_scene, image_path)
        #target_image = get_file_path(current_scene, target_field)
        #target_image_last = get_file_path(current_scene, target_field + "_last")
         
        #ss = self.scenes_in_story(current_scene)
        #for scene in ss:
        #    if scene == current_scene:
        #        continue
        #    image = get_file_path(scene, target_field)
        #    if not image and target_image:
        #        self.refresh_scene_media(scene, target_field, ".webp", target_image, True)
        #    image_last = get_file_path(scene, target_field + "_last")
        #    if not image_last and target_image_last:
        #        self.refresh_scene_media(scene, target_field + "_last", ".webp", target_image_last, True)
        self.save_scenes_to_json()


    def upload_video(self, title):
        title_cvt = self.transcriber.chinese_convert(title, self.language)
        title_used = title_cvt.replace("_", " ")
        title_used = title_used.replace("\n", " ")
        self.title = title_used

        for scene in self.scenes:
            if scene.get("clip_animation", "") == "IMAGE_MAIN":
                image_main_scene = scene
                break

        if not image_main_scene:
            for scene in self.scenes:
                clip_animation = scene.get("clip_animation", "")
                if clip_animation == "VIDEO" or clip_animation == "IMAGE" or clip_animation == "IMAGE_MAIN":
                    image_main_scene = scene
                    break

        if not image_main_scene:
            print(f"‚ùå Ê≤°ÊúâÊâæÂà∞IMAGE_MAINÂú∫ÊôØ")
            return

        thumbnail_path = f"{config.get_project_path(self.pid)}/thumbnail.png"
        final_video_path = f"{self.publish_path}/{self.title.replace(' ', '_')}_final.mp4"

        sums = config.fetch_main_summary_content(self.pid, self.language)
        if not sums:
            sums = "„Ää"+config.channel_config[self.channel]["channel_name"]+"„Äã "+self.title
        sums = self.transcriber.chinese_convert(sums, self.language)

        final_srt_path = f"{self.publish_path}/{self.title.replace(' ', '_')}_final.srt"

        video_id = self.downloader.upload_video(final_video_path, 
                                     thumbnail_path, 
                                     title=title, 
                                     description=sums, 
                                     language=self.language, 
                                     script_path=final_srt_path, 
                                     secret_key=config.channel_config[self.channel]["channel_key"],
                                     channel_id=self.channel,
                                     categoryId=config.channel_config[self.channel]["channel_category_id"][0], 
                                     tags=config.channel_config[self.channel]["channel_tags"], 
                                     privacy="unlisted")
        # save video_id to the project_manager.PROJECT_CONFIG
        try:
            from project_manager import ProjectConfigManager
            config_manager = ProjectConfigManager(self.pid)
            # Load existing config or create new one
            existing_config = config_manager.load_config(self.pid) or {}
            # Add video_id to config
            existing_config["video_id"] = video_id
            # Save the updated config
            ProjectConfigManager.set_global_config(existing_config)
            config_manager.save_project_config(existing_config)
            print(f"‚úÖ Video ID saved to project config: {video_id}")
        except Exception as e:
            print(f"‚ùå Failed to save video_id to project config: {e}")



    def upload_promo_video(self, title, description):
        # need to get video_id from the project_manager.PROJECT_CONFIG
        try:
            from project_manager import ProjectConfigManager
            config_manager = ProjectConfigManager(self.pid)
            existing_config = config_manager.load_config(self.pid) or {}
            video_id = existing_config.get("video_id", None)
        except Exception as e:
            print(f"‚ùå Failed to get video_id from project config: {e}")

        promo_video_path = f"{self.publish_path}/{title.replace(' ', '_')}_promo.mp4"
        if os.path.exists(promo_video_path):
            channel_name = self.transcriber.chinese_convert(config.channel_config[self.channel]["channel_name"], self.language)
            self.downloader.upload_video(promo_video_path, 
                            None, 
                            title=f"„Ää{channel_name}„ÄãÔºö{title}", 
                            description=channel_name,
                            language=self.language, 
                            script_path=None, 
                            secret_key=config.channel_config[self.channel]["channel_key"],
                            channel_id=self.channel,
                            categoryId=config.channel_config[self.channel]["channel_category_id"][0],
                            tags=config.channel_config[self.channel]["channel_tags"], 
                            privacy="unlisted")
        return promo_video_path


    def build_single_mv(self, background_music_mp3, background_images):
        whole_duration = self.ffmpeg_audio_processor.get_duration(background_music_mp3)
        duration_count = 0
        video_list = [] 

        for repeat in range(1, 10):
            if whole_duration and duration_count > whole_duration:
                break
            for background_image_path in background_images:
                background_video_path = f"{self.channel_path}/{Path(background_image_path).stem}.mp4"
                if self.project_169_mode():
                    duration = 30
                else:
                    duration = 60

                if not os.path.exists(background_video_path):
                    if self.project_169_mode():
                        v = self.ffmpeg_processor.create_scroll_background_video_169(background_image_path, duration, duration)
                    else:
                        v = self.ffmpeg_processor.create_scroll_background_video_916(background_image_path, duration, duration)
                    os.replace(v, background_video_path)

                video_list.append( {"path":background_video_path, "transition":"fade", "duration":1.0} )
                duration_count += duration
                if duration_count > whole_duration:
                    break

        vv = self.ffmpeg_processor.concat_videos_demuxer(video_list)
        vv = self.ffmpeg_processor.add_audio_to_video(vv, background_music_mp3)
        project_background_video = f"{config.get_media_path(self.pid)}/background_{datetime.now().strftime('%Y%m%d%H%M%S')}.mp4"
        os.replace(vv, project_background_video)

        return project_background_video


    def build_full_music_video(self, mv_name, item_list, rebuild=False):
        mv_list = []
        for i, item in enumerate(item_list):
            mv_temp = self.build_single_mv(item)
            mv_list.append( {"path":mv_temp, "transition":"fade", "duration":1.0} )

        whole_video_path = f"{self.publish_path}/full_music_video_{mv_name}.mp4"
        vv = self.ffmpeg_processor.concat_videos_demuxer(mv_list, True)
        os.replace(vv, whole_video_path)  
        config.clear_temp_files()


    def transcript_youtube_video(self, url, source_lang, translated_language):
        try:
            query = urlparse(url).query
            params = parse_qs(query)
            vid = params.get("v", [None])[0]
            if not vid:
                raise ValueError("Invalid YouTube URL, could not find video ID.")
            print(f"üîç Ëß£ÊûêÈìæÊé•Ôºö{url} - {vid}")

            script_prefix = f"{config.get_project_path(self.pid)}/Youtbue_download/__script_{vid}"

            script_lang = self.downloader.download_captions(url, translated_language, script_prefix)
            if not script_lang:
                mp3_path = self.downloader.download_audio(url)
                print("ÂºÄÂßãËΩ¨ÂΩïÈü≥È¢ë...")
                script_path = self.transcriber.transcribe_to_file(mp3_path, source_lang, 10, 26)
                if not script_path:
                    return None
                text_path = f"{config.get_project_path(self.pid)}/Youtbue_download/__text_{vid}.{source_lang}.txt"
                self.transcriber.fetch_text_from_json(script_path)
            else:
                script_path = f"{script_prefix}.{script_lang}.srt"
                text_path = f"{config.get_project_path(self.pid)}/Youtbue_download/__text_{vid}.{script_lang}.txt"
                self.transcriber.src_to_text(script_path, text_path)

            if translated_language != source_lang:
                translated_srt_path = f"{config.get_project_path(self.pid)}/Youtbue_download/__script_{vid}.{translated_language}.srt.json"
                self.transcriber.translate_srt_file(script_path, source_lang, translated_language, translated_srt_path)

            return script_path
        except Exception as e:
            print(f"Ëé∑ÂèñÂ≠óÂπïÂ§±Ë¥•: {str(e)}")
            return None



    def prepare_final_script(self, base_seconds, final_script_path):
        content = []
        i = 0
        subtitle_index = 1
        
        while i < len(self.scenes):
            current_scene = self.scenes[i]
            current_story = current_scene.get("visual_start", "")
            
            # Find all consecutive scenes with the same story content
            start_index = i
            end_index = i
            
            start_formatted = base_seconds
            scene_duration = self.get_scene_duration(current_scene)
            base_seconds += scene_duration
            # Look ahead to find consecutive scenes with same story
            while (end_index + 1 < len(self.scenes) and 
                   self.scenes[end_index + 1].get("visual_start", "") == current_story and
                   current_story.strip() != ""):  # Only combine non-empty content
                end_index += 1
                current_scene = self.scenes[end_index]
                base_seconds += scene_duration

            end_formatted = base_seconds

            # Add to content (only if story content is not empty)
            if current_story.strip():
                content.append(str(subtitle_index))
                content.append(f"{start_formatted} --> {end_formatted}")
                content.append(current_story)
                content.append("\n")
                subtitle_index += 1
                
                # Log the combination for debugging
                if end_index > start_index:
                    print(f"üìù Combined scenes {start_index+1}-{end_index+1} with same content: '{current_story[:50]}{'...' if len(current_story) > 50 else ''}'")
            
            # Move to next unique content
            i = end_index + 1

        with open(final_script_path, "w", encoding="utf-8") as f:
            f.write('\n'.join(content))
            
        print(f"üìã Final script created with {subtitle_index-1} subtitle entries (combined duplicates): {final_script_path}")


    def wait_for_background_threads(self, timeout=300):
        """Á≠âÂæÖÊâÄÊúâÂêéÂè∞Á∫øÁ®ãÂÆåÊàê"""
        if not self.background_threads:
            return
            
        print(f"‚è≥ Á≠âÂæÖ {len(self.background_threads)} ‰∏™ÂêéÂè∞Á∫øÁ®ãÂÆåÊàê...")
        
        for i, thread in enumerate(self.background_threads):
            if thread.is_alive():
                print(f"‚è≥ Á≠âÂæÖÁ∫øÁ®ã {i+1}/{len(self.background_threads)} ÂÆåÊàê...")
                thread.join(timeout=timeout//len(self.background_threads))
                
        # Ê∏ÖÁêÜÂ∑≤ÂÆåÊàêÁöÑÁ∫øÁ®ã
        alive_threads = [t for t in self.background_threads if t.is_alive()]
        completed = len(self.background_threads) - len(alive_threads)
        
        print(f"‚úÖ {completed} ‰∏™ÂêéÂè∞Á∫øÁ®ãÂÆåÊàêÔºå{len(alive_threads)} ‰∏™‰ªçÂú®ËøêË°å")
        self.background_threads = alive_threads



    def regenerate_audio(self, fresh_json, language):
        lang = "chinese" if language == "zh" or language == "tw" else "english"

        start_time = 0.0
        for json_item in fresh_json:
            start_time, json_item["speak_audio"] = self.regenerate_audio_item(json_item, start_time, language)

        return fresh_json, self.ffmpeg_audio_processor.concat_audios([json_item["speak_audio"] for json_item in fresh_json])


    def regenerate_audio_item(self, json_item, start_time, language):
        lang = "chinese" if language == "zh" or language == "tw" else "english"

        speaker = json_item["speaker"]
        content = json_item["content"]
        mood = json_item["mood"]

        ss = speaker.split(",")
        if len(ss) > 1:
            print(f"üé§ Ê£ÄÊµãÂà∞Â§öËØ¥ËØù‰∫∫Ê®°Âºè: {len(ss)} ‰∏™ËØ¥ËØù‰∫∫ - {', '.join([s.strip() for s in ss if s.strip()])}")

        voices = []
        for s in ss:
            s = s.strip()  # ÂéªÈô§ÂèØËÉΩÁöÑÁ©∫Ê†º
            if not s:  # Ë∑≥ËøáÁ©∫Â≠óÁ¨¶‰∏≤
                continue
            try:
                voice = self.speech_service.get_voice(s, lang)
                ssml = self.speech_service.create_ssml(text=content, voice=voice, mood=mood)
                audio_file = self.speech_service.synthesize_speech(ssml)
                if audio_file:  # Âè™Ê∑ªÂä†ÊàêÂäüÁîüÊàêÁöÑÈü≥È¢ëÊñá‰ª∂
                    voices.append(audio_file)
                else:
                    print(f"‚ö†Ô∏è ËØ¥ËØù‰∫∫ '{s}' ÁöÑËØ≠Èü≥ÂêàÊàêÂ§±Ë¥•")
            except Exception as e:
                print(f"‚ùå ËØ¥ËØù‰∫∫ '{s}' ÁöÑËØ≠Èü≥ÂêàÊàêÈîôËØØ: {str(e)}")

        json_item["speak_audio"] = self.ffmpeg_audio_processor.audio_list_mix(voices)

        json_item["start"] = start_time
        json_item["duration"] = self.ffmpeg_audio_processor.get_duration(json_item["speak_audio"])
        json_item["end"] = start_time + json_item["duration"]

        return json_item["end"], json_item["speak_audio"]


    def make_scene_name(self, scene, av_type, postfix):
        if not scene:
            return av_type + "_" + self.pid + "_" + "0000" + postfix
        return av_type + "_" + self.pid + "_" + str(scene.get("id", "")) + postfix


    def check_generated_clip_video(self, scene, video_type, audio_type):
        clip_animation = scene.get("clip_animation", "")
        second_animation = scene.get("second_animation", "")
        if clip_animation not in config.ANIMATE_SOURCE and second_animation not in config.ANIMATE_SOURCE:
            return None

        output_mp4_folder = "Z:/wan_video/output_mp4"

        # ÁîüÊàêÂü∫Á°ÄÊñá‰ª∂ÂêçÂâçÁºÄÔºà‰∏çÂåÖÊã¨Á±ªÂûãÂêéÁºÄÔºâ
        # Ê†ºÂºè: av_type + "_" + pid + "_" + scene_id
        base_name = self.make_scene_name(scene, video_type, "")
        
        # ÂÆö‰πâÊñá‰ª∂Á±ªÂûãÊ®°ÂºèÂàóË°®ÔºåÂåπÈÖçÊ†ºÂºè: base_name + type_suffix + _timestamp.mp4
        # timestampÊ†ºÂºè‰∏∫ %d%H%M%S (8‰ΩçÊï∞Â≠ó: Êó•Êúü+Â∞èÊó∂+ÂàÜÈíü+Áßí)
        files = []
        for file in os.listdir(output_mp4_folder):
            # Ê£ÄÊü•Êñá‰ª∂ÊòØÂê¶‰ª•Âü∫Á°ÄÂêçÁß∞ÂºÄÂ§¥
            if file.startswith(base_name):
                # Ê£ÄÊü•ÊòØÂê¶ÂåπÈÖç‰ªª‰Ωï‰∏Ä‰∏™Á±ªÂûãÊ®°Âºè
                for pattern, type_suffix in config.ANIMATE_TYPE_PATTERNS:
                    if re.search(pattern, file):
                        files.append(file)
                        break

        if len(files) == 0:
            return None

        choose_file_stem = Path(files[0]).stem
        has_audio = any(item in choose_file_stem for item in config.ANIMATE_WITH_AUDIO)

        enhanced_video = output_mp4_folder + "/" + choose_file_stem + ".mp4"
        if not os.path.exists(enhanced_video):
            print(f"‚ö†Ô∏è Êñá‰ª∂Â∑≤ÁªèÂ§ÑÁêÜËøáÔºåË∑≥Ëøá: {enhanced_video}")
            return
        os.replace(enhanced_video, enhanced_video + ".bak.mp4")
        enhanced_video = enhanced_video + ".bak.mp4"

        enhanced_video = self.ffmpeg_processor.resize_video(enhanced_video, width=None, height=self.ffmpeg_processor.height)

        audio = get_file_path(scene, audio_type)
        if audio: # always cut to the same duration as the audio
            enhanced_video = self.ffmpeg_processor.add_audio_to_video(enhanced_video, audio)
        elif has_audio:
            audio = self.ffmpeg_audio_processor.extract_audio_from_video(enhanced_video)
            olda, audio = self.refresh_scene_media(scene, audio_type, ".wav", audio)

        if "_L_WS2V" in choose_file_stem:
            self.refresh_scene_media(scene, video_type+"_left", ".mp4", enhanced_video, True)
            #if scene[video_type+"_left"] and scene[video_type+"_right"]:
            #    scene["clip_animation"] = ""
        elif "_R_WS2V" in choose_file_stem:
            self.refresh_scene_media(scene, video_type+"_right", ".mp4", enhanced_video, True)
            #if scene[video_type+"_left"] and scene[video_type+"_right"]:
            #    scene["clip_animation"] = ""
        else:
            oldv, enhanced_video = self.refresh_scene_media(scene, video_type, ".mp4", enhanced_video, True)
            #scene["clip_animation"] = ""

        self.save_scenes_to_json()


    def rebuild_scene_video(self, scene, video_type, animate_mode, image_path, image_last_path, sound_path, action_path, wan_prompt):
        if not sound_path or not image_path:
            return
        if not image_last_path:
            image_last_path = image_path

        file_prefix = video_type + "_" + self.pid + "_" + str(scene.get("id", ""))
        
        #if animate_mode == "IMAGE":
        #    v = self.ffmpeg_processor.image_audio_to_video(image_path, sound_path, 1)
        #    self.refresh_scene_media(scene, video_type, ".mp4", v, True)

        if animate_mode in config.ANIMATE_I2V:
            self.sd_processor.image_to_video( prompt=wan_prompt, file_prefix=file_prefix, image_path=image_path, sound_path=sound_path, animate_mode=animate_mode )

        elif animate_mode in config.ANIMATE_2I2V:
            self.sd_processor.two_image_to_video( prompt=wan_prompt, file_prefix=file_prefix, first_frame=image_path, last_frame=image_last_path, sound_path=sound_path, animate_mode=animate_mode )

        elif animate_mode in config.ANIMATE_S2V:
            self.sd_processor.sound_to_video(prompt=wan_prompt, file_prefix=file_prefix, image_path=image_path, sound_path=sound_path, animate_mode=animate_mode, silence=False)

        elif animate_mode in config.ANIMATE_AI2V:
            if not action_path:
                action_path = f"{config.DEFAULT_MEDIA_PATH}/default_action.mp4"
            self.sd_processor.action_transfer_video(prompt=wan_prompt, file_prefix=file_prefix, image_path=image_path, sound_path=sound_path, action_path=action_path, animate_mode=animate_mode)

        elif animate_mode in config.ANIMATE_WS2V:
            vertical_line_position = scene.get("clip_image_split", 0)
            if vertical_line_position == 0:
                return

            speaker = scene.get("speaker", "")
            speaker_position = scene.get("speaker_position", "")
            if not speaker or not speaker_position:
                return

            left_image, right_image = self.ffmpeg_processor.split_image(image_path, vertical_line_position)
            
            left_prompt = wan_prompt.copy()
            right_prompt = wan_prompt.copy()
            if speaker_position == "left":
                left_prompt.pop("LISTENING", None)
                right_prompt.pop("SPEAKING", None)
                self.sd_processor.sound_to_video(prompt=left_prompt, file_prefix=file_prefix+"_L", image_path=left_image, sound_path=sound_path, animate_mode=animate_mode, silence=False)
                self.sd_processor.sound_to_video(prompt=right_prompt, file_prefix=file_prefix+"_R", image_path=right_image, sound_path=sound_path, animate_mode=animate_mode, silence=True)
            elif speaker_position == "right":
                left_prompt.pop("SPEAKING", None)
                right_prompt.pop("LISTENING", None)
                self.sd_processor.sound_to_video(prompt=left_prompt, file_prefix=file_prefix+"_L", image_path=left_image, sound_path=sound_path, animate_mode=animate_mode, silence=True)
                self.sd_processor.sound_to_video(prompt=right_prompt, file_prefix=file_prefix+"_R", image_path=right_image, sound_path=sound_path, animate_mode=animate_mode, silence=False)


    def _process_single_files(self, scene, found_files):
        """Â§ÑÁêÜ S2V Ê®°ÂºèÁöÑÂçï‰∏™ËßÜÈ¢ëÊñá‰ª∂"""
        try:
            # Á≠âÂæÖ30ÁßíÁ°Æ‰øùÊñá‰ª∂ÂÆåÂÖ®ÁîüÊàê
            print("‚è≥ Á≠âÂæÖ30ÁßíÁ°Æ‰øùÊñá‰ª∂ÁîüÊàêÂÆåÊàê...")
            time.sleep(30)
            
            for new_mp4 in found_files:
                print(f"üìπ Â§ÑÁêÜ S2V ËßÜÈ¢ëÊñá‰ª∂: {os.path.basename(new_mp4)}")
                
                # ÁßªÂä®Êñá‰ª∂Âà∞È°πÁõÆÂ™í‰ΩìÊñá‰ª∂Â§π
                oldv, clip_raw_video = self.refresh_scene_media(scene, "clip_raw_video", ".mp4", new_mp4)

                temp_video =  config.get_temp_path(self.pid) + "/" + self.make_scene_name(scene, "clip", ".mp4")
                copy_file(clip_raw_video, temp_video)
                
                print(f"‚úÖ Êñá‰ª∂Â∑≤ÁßªÂä®Âà∞: {clip_raw_video}")
                
                # Ë∞ÉÁî® REST API Â¢ûÂº∫ËßÜÈ¢ë
                self._enhance_single_video(temp_video)
            
        except Exception as e:
            print(f"‚ùå Â§ÑÁêÜ S2V Êñá‰ª∂Êó∂Âá∫Èîô: {str(e)}")


    def _process_dual_files(self, scene, found_files):
        """Â§ÑÁêÜ WS2V Ê®°ÂºèÁöÑÂèåËßÜÈ¢ëÊñá‰ª∂"""
        try:
            # Á≠âÂæÖ30ÁßíÁ°Æ‰øùÊñá‰ª∂ÂÆåÂÖ®ÁîüÊàê
            print("‚è≥ Á≠âÂæÖ60ÁßíÁ°Æ‰øùÊñá‰ª∂ÁîüÊàêÂÆåÊàê...")
            time.sleep(60)
            
            temp_video =  config.get_temp_path(self.pid) + "/" + self.make_scene_name(scene, "clip", ".mp4")
            # ÂÅáËÆæÂâç‰∏§‰∏™Êñá‰ª∂ÂàÜÂà´ÊòØÂ∑¶Âè≥ËßÜÈ¢ë
            left_mp4 = found_files[0]
            right_mp4 = found_files[1]
            
            print(f"üìπ Â§ÑÁêÜ WS2V Â∑¶ËßÜÈ¢ë: {os.path.basename(left_mp4)}")
            print(f"üìπ Â§ÑÁêÜ WS2V Âè≥ËßÜÈ¢ë: {os.path.basename(right_mp4)}")
            
            # ÁßªÂä®Êñá‰ª∂Âà∞È°πÁõÆÂ™í‰ΩìÊñá‰ª∂Â§π
            oldv1, clip_raw_video = self.refresh_scene_media(scene, "clip_raw_video", ".mp4", left_mp4)
            copy_file(clip_raw_video, temp_video)
            
            oldv2, clip_raw_video2 = self.refresh_scene_media(scene, "clip_raw_video2", ".mp4", right_mp4)
            
            print(f"‚úÖ Â∑¶ËßÜÈ¢ëÂ∑≤ÁßªÂä®Âà∞: {clip_raw_video}")
            print(f"‚úÖ Âè≥ËßÜÈ¢ëÂ∑≤ÁßªÂä®Âà∞: {clip_raw_video2}")
            
            # Ë∞ÉÁî® REST API Â¢ûÂº∫ÂèåËßÜÈ¢ë
            self._enhance_dual_video(temp_video, clip_raw_video2)
            
        except Exception as e:
            print(f"‚ùå Â§ÑÁêÜ WS2V Êñá‰ª∂Êó∂Âá∫Èîô: {str(e)}")
    

    def _enhance_single_video(self, video_path):
        """Ë∞ÉÁî® REST API Â¢ûÂº∫Âçï‰∏™ËßÜÈ¢ë"""
        try:
            url = "http://10.0.0.179:5000/process/single"
            
            with open(video_path, 'rb') as video_file:
                files = {'video': video_file}
                
                print(f"üöÄ Ê≠£Âú®Ë∞ÉÁî®ËßÜÈ¢ëÂ¢ûÂº∫API: {url}")
                response = requests.post(url, files=files, timeout=300)
                
                if response.status_code >= 200 and response.status_code < 300:
                    print("‚úÖ ÂçïËßÜÈ¢ëÂ¢ûÂº∫ÊàêÂäü")
                    print(f"üìÑ ÂìçÂ∫î: {response.text}")
                else:
                    print(f"‚ùå ÂçïËßÜÈ¢ëÂ¢ûÂº∫Â§±Ë¥•ÔºåÁä∂ÊÄÅÁ†Å: {response.status_code}")
                    print(f"üìÑ ÈîôËØØ‰ø°ÊÅØ: {response.text}")
                    
        except requests.exceptions.RequestException as e:
            print(f"‚ùå REST API Ë∞ÉÁî®Â§±Ë¥•: {str(e)}")
        except Exception as e:
            print(f"‚ùå Â¢ûÂº∫ÂçïËßÜÈ¢ëÊó∂Âá∫Èîô: {str(e)}")
    

    def _enhance_dual_video(self, left_video_path, right_video_path):
        """Ë∞ÉÁî® REST API Â¢ûÂº∫ÂèåËßÜÈ¢ë"""
        try:
            url = "http://10.0.0.179:5000/process/dual"
            
            with open(left_video_path, 'rb') as left_file, open(right_video_path, 'rb') as right_file:
                files = {
                    'left_video': left_file,
                    'right_video': right_file
                }
                
                print(f"üöÄ Ê≠£Âú®Ë∞ÉÁî®ÂèåËßÜÈ¢ëÂ¢ûÂº∫API: {url}")
                response = requests.post(url, files=files, timeout=300)
                
                if response.status_code >= 200 and response.status_code < 300:
                    print("‚úÖ ÂèåËßÜÈ¢ëÂ¢ûÂº∫ÊàêÂäü")
                    print(f"üìÑ ÂìçÂ∫î: {response.text}")
                else:
                    print(f"‚ùå ÂèåËßÜÈ¢ëÂ¢ûÂº∫Â§±Ë¥•ÔºåÁä∂ÊÄÅÁ†Å: {response.status_code}")
                    print(f"üìÑ ÈîôËØØ‰ø°ÊÅØ: {response.text}")
                    
        except requests.exceptions.RequestException as e:
            print(f"‚ùå REST API Ë∞ÉÁî®Â§±Ë¥•: {str(e)}")
        except Exception as e:
            print(f"‚ùå Â¢ûÂº∫ÂèåËßÜÈ¢ëÊó∂Âá∫Èîô: {str(e)}")


    def replace_scene_audio(self, scene, audio_path, audio_start_time):
        audio_duration = self.ffmpeg_processor.get_duration(scene["clip_audio"])
        extended_audio = self.ffmpeg_audio_processor.extend_audio(audio_path, audio_start_time, audio_duration)
        olda, newa = self.refresh_scene_media(scene, "clip_audio", ".wav", extended_audio)
        newv = self.ffmpeg_processor.add_audio_to_video(scene["clip"], newa)
        self.refresh_scene_media(scene, "clip", ".mp4", newv)


    def promotion_video(self, title, program_keywords):
        self.post_init(title, program_keywords)

        promotion_scenes = []
        for s in self.scenes:
            promotion_info = s.get("promotion_info", None)
            if promotion_info and len(promotion_info) > 0:
                promotion_scenes.append(s)
        
        if len(promotion_scenes) == 0:
            return

        video_segments = []
        zero_audio = None
        zero_offset = None
        for s in promotion_scenes:
            promotion_info = s.get("promotion_info", "")
            clip = s.get("clip", None)
            if zero_offset is None:
                zero_offset, clip_duration, story_duration, indx, count, is_story_last_clip = self.get_scene_detail(s)
            if zero_audio is None:
                zero_audio = get_file_path(s, "zero_audio")

            clip_lang = s.get("content_language", "")
            if not clip_lang or clip_lang not in config.FONT_LIST:
                font = self.font_video
            else:
                font = config.FONT_LIST[clip_lang]
        
            promotion_info = "hl_" + self.transcriber.translate_text(promotion_info, self.language, self.language)
            clip_temp = self.ffmpeg_processor.add_script_to_video(clip, promotion_info, font)
            video_segments.append({"path":clip_temp, "transition":"fade", "duration":1.0})

        video_temp = self.ffmpeg_processor._concat_videos_with_transitions(video_segments, keep_audio_if_has=True)
        if zero_audio is not None and zero_offset is not None:
            audio_zero = self.ffmpeg_audio_processor.audio_cut_fade(zero_audio, zero_offset, self.ffmpeg_processor.get_duration(video_temp))
            video_temp = self.ffmpeg_processor.add_audio_to_video(video_temp, audio_zero)
            
        promotion_video_path = f"{self.publish_path}/{self.title.replace(' ', '_')}_promotion.mp4"
        os.replace(video_temp, promotion_video_path)

        config.clear_temp_files()
        print(f"‚úÖ Promotion video created: {promotion_video_path}")


    def finalize_video(self, title, program_keywords):
        self.post_init(title, program_keywords)
        
        #start = 0.0
        #if self.video_prepares["starting"]["video_path"] and os.path.exists(self.video_prepares["starting"]["video_path"]):
        #    video_segments.append({"path":self.video_prepares["starting"]["video_path"], "transition":"fade", "duration":1.0})
        #    start = start + self.ffmpeg_processor.get_duration(self.video_prepares["starting"]["video_path"])

        #if self.video_prepares["pre_video"]["video_path"] and os.path.exists(self.video_prepares["pre_video"]["video_path"]):
        #    video_segments.append({"path":self.video_prepares["pre_video"]["video_path"], "transition":"fade", "duration":1.0})
        #    start = start + self.ffmpeg_processor.get_duration(self.video_prepares["pre_video"]["video_path"])

        video_segments = []
        for s in self.scenes:
            clip= s["clip"]
            video_segments.append({"path":clip, "transition":"fade", "duration":1.0})

        video_temp = self.ffmpeg_processor._concat_videos_with_transitions(video_segments, keep_audio_if_has=True)

        audio_segments = []
        current_zero = None
        for s in self.scenes:
            if not current_zero or current_zero != s["zero"]:
                current_zero = s["zero"]
                audio_segments.append(current_zero)
        if audio_segments and len(audio_segments) > 0:
            audio_temp = self.ffmpeg_audio_processor.concat_audios(audio_segments)
            video_temp = self.ffmpeg_processor.add_audio_to_video(video_temp, audio_temp, False)

        final_video_path = f"{self.publish_path}/{self.title.replace(' ', '_')}_final.mp4"
        os.replace(video_temp, final_video_path)

        config.clear_temp_files()

        # prepare final srt file
        #final_srt_path = f"{self.publish_path}/{title.replace(' ', '_')}_final.srt"
        #self.prepare_final_script(start, final_srt_path)
 
        # add subtitle to the final video
        # self.ffmpeg_processor.add_subtitle(final_video_path, mp4_path, final_srt_path)
        print(f"‚úÖ Final video with audio created: {final_video_path}")


    def make_background_audio(self):
        audio_segments = []
        started = None
        last_end = 0.0
        for s in self.scenes:
            duration = self.find_clip_duration(s)

            zero = get_file_path(s, "zero")
            zero_clip_position = s.get("zero_clip_position", -1.0)
            zero_volume = s.get("zero_clip_volume", None)
            zero_ending = s.get("zero_ending", False)
            zero_end = s.get("zero_end", None)

            if not zero_end or not zero_volume or zero_clip_position < 0.0 or zero_clip_position >= duration-0.1:
                audio_segments.append( self.ffmpeg_audio_processor.make_silence(duration) )
                continue

            if not started:
                started = s.get("zero_start", None)
                if zero_clip_position > 0.0:
                    audio_segments.append( self.ffmpeg_audio_processor.make_silence(zero_clip_position) )

            if started:
                if zero_ending:
                    audio_segments.append( self.ffmpeg_audio_processor.audio_cut_fade(zero, started, zero_end-started, 1.0, 1.0, zero_volume) )
                    started = None

            if not zero_end:
                last_end = 0.0
            else:
                last_end = zero_end

        audio_temp = self.ffmpeg_audio_processor.concat_audios(audio_segments)
        return audio_temp


    def prepare_scenes_from_json(self, raw_scene, raw_index, audio_json, style, shot, angle, color):
        # keep raw_id as integer    
        raw_id = int((raw_scene["id"]/100)*100)

        start_time = 0.0
        for audio_scene in audio_json:
            raw_id += 100
            # ‰øùÂ≠ò audio_scene ‰∏≠ÈúÄË¶Å‰øùÁïôÁöÑÂ≠óÊÆµ
            preserved_fields = {}
            for field in ["start", "end", "duration", "speaker", "content"]:
                if field in audio_scene:
                    preserved_fields[field] = audio_scene[field]
            
            # ÂÖãÈöÜ raw_scene ÁöÑÊâÄÊúâÂ≠óÊÆµÂà∞ audio_scene
            audio_scene.update(raw_scene.copy())
            
            # ÊÅ¢Â§ç‰øùÁïôÁöÑÂ≠óÊÆµÔºàË¶ÜÁõñ‰ªé raw_scene Â§çÂà∂ÁöÑÂÄºÔºâ
            audio_scene.update(preserved_fields)
            
            # Êõ¥Êñ∞ÁâπÂÆöÂ≠óÊÆµ
            audio_scene.update({
                "id": raw_id,
                "wan_style": style,
                "wan_shot": shot,
                "wan_angle": angle,
                "wan_color": color,
                "clip_animation": ""
            })
            self.refresh_scene_visual(audio_scene)

            clip_wav = self.ffmpeg_audio_processor.audio_cut_fade(audio_scene["clip_audio"], audio_scene["start"], audio_scene["duration"])
            olda, clip_audio = self.refresh_scene_media(audio_scene, "clip_audio", ".wav", clip_wav)

            v = self.ffmpeg_processor.trim_video(raw_scene["clip"], audio_scene["start"], audio_scene["end"])
            #v = self.ffmpeg_processor.add_audio_to_video(v, clip_audio)
            self.refresh_scene_media(audio_scene, "clip", ".mp4", v)

        self.replace_scene_with_others(raw_index, audio_json)

        return audio_json


    def swap_scene(self, current_index, next_index):
        if current_index < 0 or current_index >= len(self.scenes):
            return False
        if next_index < 0 or next_index >= len(self.scenes):
            return False
        self.scenes[current_index], self.scenes[next_index] = self.scenes[next_index], self.scenes[current_index]
        temp = self.scenes[current_index]["id"]
        self.scenes[current_index]["id"] = self.scenes[next_index]["id"]
        self.scenes[next_index]["id"] = temp
        self.save_scenes_to_json()
        return True


    def refresh_scene_visual(self, scene, script_content=None):
        if script_content:
            scene["content"] = script_content
        else:
            script_content = scene.get("content", "")
 
        if not script_content or script_content == "":
            return

        script_content = "This scens is about:\n--------\n" + script_content + "\n\n--------\nAnd it is a scene insidethe whole story as below:\n--------\n" + project_manager.PROJECT_CONFIG.get("story", "")
        system_prompt = config_prompt.SCENE_SUMMARY_SYSTEM_PROMPT
        wan_style = scene.get("wan_style", None)
        wan_color = scene.get("wan_color", None)
        wan_shot = scene.get("wan_shot", None)
        wan_angle = scene.get("wan_angle", None)
        if wan_style and wan_color and wan_shot and wan_angle:
            system_prompt = system_prompt + f'\n***FYI*** Generally, video/image is in \'{wan_style}\' style &  \'{wan_color}\' colors; the camera using \'{wan_shot}\' shot, in \'{wan_angle}\' angle.'

        new_scene = self.llm_api.generate_json_summary(system_prompt, script_content, None, False)
        if isinstance(new_scene, list):
            if len(new_scene) == 0:
                return
            new_scene = new_scene[0]

        self.try_update_scene_visual_fields(scene, new_scene)


    def max_id(self):
        max_id = 0
        for s in self.scenes:
            id = s.get("id", 0)
            if id > max_id:
                max_id = id
        return max_id


    def initialize_default_root_scene(self, scene):
        prefix, keywords = config.fetch_resource_prefix("", ["default"])

        if not self.background_image:
            self.background_image = config.find_matched_file(config.get_background_image_path()+"/"+self.channel, prefix+"/", "jpeg", keywords)
            self.background_image = self.ffmpeg_processor.to_webp(self.background_image) 

        if not self.background_music:
            self.background_music = config.find_matched_file(config.get_background_music_path()+"/"+self.channel, prefix+"/", "wav", keywords)
            self.background_music = self.ffmpeg_audio_processor.to_wav(self.background_music)

        if not self.background_video:
            self.background_video = self.ffmpeg_processor.image_audio_to_video(self.background_image, self.background_music, 1)

        next_root_id = (int(self.max_id()/10000) + 1)*10000
        scene["id"] = next_root_id

        oldv, zero =self.refresh_scene_media(scene, "zero", ".mp4", self.background_video, True)
        olda, zero_audio = self.refresh_scene_media(scene, "zero_audio", ".wav", self.background_music, True)
        oldi, zero_image = self.refresh_scene_media(scene, "zero_image", ".webp", self.background_image, True)

        self.refresh_scene_media(scene, "clip", ".mp4", zero, True)
        self.refresh_scene_media(scene, "clip_audio", ".wav", zero_audio, True)
        self.refresh_scene_media(scene, "clip_image", ".webp", zero_image, True)


    def add_default_root_scene(self, scene_index, site, is_append=False):
        next_root_id = (int(self.max_id()/10000) + 1)*10000
        scene = {
                "id": next_root_id,
                "environment": site,
                "clip_animation": "",
            }
        self.initialize_default_root_scene(scene)

        if not self.scenes:
            self.scenes = [scene]
        else:
            if is_append:
                self.scenes.insert(scene_index+1, scene)
            else:
                self.scenes.insert(scene_index, scene) 



    def try_update_scene_visual_fields(self, current_scene, scene_data):
        # ÊòæÁ§∫ÂØπÊØîÂØπËØùÊ°ÜÔºåËÆ©Áî®Êà∑ÊØîËæÉÂíåÁºñËæë
        updated_data = self._show_scene_comparison_dialog(current_scene, scene_data)
        if updated_data is None:
            return  # Áî®Êà∑ÂèñÊ∂à

        content = scene_data.get("content", "")
        if content:
            current_scene["content"] = content
        story = scene_data.get("visual_start", "")
        subject = scene_data.get("subject", "")
        if subject:
            current_scene["subject"] = subject
        if story:
            current_scene["visual_start"] = story
        visual_end = scene_data.get("visual_end", "")
        if visual_end:
            current_scene["visual_end"] = visual_end
        era = scene_data.get("era_time", "")
        if era:
            current_scene["era_time"] = era
        environment = scene_data.get("environment", "")
        if environment:
            current_scene["environment"] = environment
        cinematography = scene_data.get("cinematography", "")
        if cinematography:
            current_scene["cinematography"] = cinematography
        sound = scene_data.get("sound_effect", "")
        if sound:
            current_scene["sound_effect"] = sound
        speaker = scene_data.get("speaker", "")
        if speaker:
            current_scene["speaker"] = speaker
        speaker_action = scene_data.get("speaker_action", "")
        if speaker_action:
            current_scene["speaker_action"] = speaker_action
        keywords = scene_data.get("keywords", "")
        if keywords:
            current_scene["keywords"] = keywords
        mood = scene_data.get("mood", "")
        if mood:
            current_scene["mood"] = mood

        self.save_scenes_to_json()


    def _show_scene_comparison_dialog(self, current_scene, new_scene_data):
        """ÊòæÁ§∫Âú∫ÊôØÊï∞ÊçÆÂØπÊØîÂØπËØùÊ°ÜÔºåËÆ©Áî®Êà∑ÊØîËæÉÂíåÁºñËæëÂ≠óÊÆµÂÄº"""
        import tkinter as tk
        import tkinter.ttk as ttk
        import tkinter.scrolledtext as scrolledtext
        
        # Â≠óÊÆµÂêçÁß∞Êò†Â∞Ñ
        field_labels = {
            "content": "ÂÜÖÂÆπ",
            "subject": "‰∏ª‰Ωì",
            "visual_start": "ÂºÄÂú∫ÁîªÈù¢",
            "visual_end": "ÁªìÊùüÁîªÈù¢",
            "era_time": "Êó∂‰ª£",
            "environment": "ÁéØÂ¢É",
            "cinematography": "ÁîµÂΩ±ÊëÑÂΩ±",
            "sound_effect": "Èü≥Êïà",
            "speaker": "ËÆ≤Âëò",
            "speaker_action": "ËÆ≤ÂëòÂä®‰Ωú",
            "keywords": "ÂÖ≥ÈîÆËØç",
            "mood": "ÊÉÖÁª™"
        }
        
        # ÈúÄË¶ÅÂØπÊØîÁöÑÂ≠óÊÆµÂàóË°®
        fields_to_compare = list(field_labels.keys())
        
        # ÂàõÂª∫ÂØπËØùÊ°Ü
        try:
            root = tk._default_root
            if root is None:
                root = tk.Tk()
                root.withdraw()
        except:
            root = tk.Tk()
            root.withdraw()
        
        dialog = tk.Toplevel(root)
        dialog.title("ÂØπÊØîÂíåÁºñËæëÂú∫ÊôØÊï∞ÊçÆ")
        dialog.geometry("1200x800")
        dialog.transient(root)
        dialog.grab_set()
        
        # Â±Ö‰∏≠ÊòæÁ§∫
        dialog.update_idletasks()
        x = (dialog.winfo_screenwidth() - 1200) // 2
        y = (dialog.winfo_screenheight() - 800) // 2
        dialog.geometry(f"1200x800+{x}+{y}")
        
        # ‰∏ªÊ°ÜÊû∂
        main_frame = ttk.Frame(dialog, padding=10)
        main_frame.pack(fill=tk.BOTH, expand=True)
        
        # Ê†áÈ¢ò
        title_label = ttk.Label(main_frame, text=f"Êñ∞ÊóßÂú∫ÊôØÊï∞ÊçÆÂØπÊØî",  font=('TkDefaultFont', 11, 'bold'))
        title_label.pack(pady=(0, 10))
        
        # ÂàõÂª∫ÊªöÂä®Ê°ÜÊû∂
        canvas = tk.Canvas(main_frame)
        scrollbar = ttk.Scrollbar(main_frame, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas)
        
        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # Â≠òÂÇ®ÁºñËæëÊéß‰ª∂ÂíåÂ§çÈÄâÊ°Ü
        comparison_widgets = {}
        
        # ‰∏∫ÊØè‰∏™Â≠óÊÆµÂàõÂª∫ÂØπÊØîË°å
        for field in fields_to_compare:
            field_frame = ttk.LabelFrame(scrollable_frame, text=field_labels[field], padding=5)
            field_frame.pack(fill=tk.X, pady=5, padx=5)
            
            # Â§çÈÄâÊ°ÜÔºöÊòØÂê¶Êõ¥Êñ∞Ê≠§Â≠óÊÆµ
            checkbox_frame = ttk.Frame(field_frame)
            checkbox_frame.pack(fill=tk.X, pady=(0, 5))
            
            update_var = tk.BooleanVar(value=False)
            checkbox = ttk.Checkbutton(checkbox_frame, text="Êõ¥Êñ∞Ê≠§Â≠óÊÆµ", variable=update_var)
            checkbox.pack(side=tk.LEFT)
            
            # ‰∏§ÂàóÂ∏ÉÂ±ÄÔºöÂΩìÂâçÂÄº vs Êñ∞ÂÄº
            comparison_frame = ttk.Frame(field_frame)
            comparison_frame.pack(fill=tk.BOTH, expand=True)
            
            # ÂΩìÂâçÂÄºÂàó
            current_frame = ttk.Frame(comparison_frame)
            current_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(0, 5))
            ttk.Label(current_frame, text="ÂΩìÂâçÂÄº:", font=('TkDefaultFont', 9, 'bold')).pack(anchor='w')
            current_text = scrolledtext.ScrolledText(current_frame, wrap=tk.WORD, height=3, width=50)
            current_text.pack(fill=tk.BOTH, expand=True)
            current_value = str(current_scene.get(field, ""))
            current_text.insert('1.0', current_value)
            current_text.config(state=tk.DISABLED)  # ÂΩìÂâçÂÄºÂè™ËØª
            
            # Êñ∞ÂÄºÂàóÔºàÂèØÁºñËæëÔºâ
            new_frame = ttk.Frame(comparison_frame)
            new_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(5, 0))
            ttk.Label(new_frame, text="Êñ∞ÂÄºÔºàÂèØÁºñËæëÔºâ:", font=('TkDefaultFont', 9, 'bold')).pack(anchor='w')
            new_text = scrolledtext.ScrolledText(new_frame, wrap=tk.WORD, height=3, width=50)
            new_text.pack(fill=tk.BOTH, expand=True)
            new_value = str(new_scene_data.get(field, ""))
            new_text.insert('1.0', new_value)
            
            # ‰øùÂ≠òÊéß‰ª∂ÂºïÁî®
            comparison_widgets[field] = {
                'update_var': update_var,
                'new_text': new_text
            }
        
        canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        
        # ÊåâÈíÆÊ°ÜÊû∂
        button_frame = ttk.Frame(main_frame)
        button_frame.pack(fill=tk.X, pady=(10, 0))
        
        result = [None]  # ‰ΩøÁî®ÂàóË°®‰ª•‰æøÂú®Èó≠ÂåÖ‰∏≠‰øÆÊîπ
        
        def on_ok():
            # Êî∂ÈõÜÁî®Êà∑ÈÄâÊã©ÁöÑÊï∞ÊçÆ
            updated_data = {}
            for field, widgets in comparison_widgets.items():
                if widgets['update_var'].get():
                    # Áî®Êà∑ÈÄâÊã©‰∫ÜÊõ¥Êñ∞Ê≠§Â≠óÊÆµ
                    new_value = widgets['new_text'].get('1.0', tk.END).strip()
                    if new_value:  # Âè™Ê∑ªÂä†ÈùûÁ©∫ÂÄº
                        updated_data[field] = new_value
            result[0] = updated_data if updated_data else None
            dialog.destroy()
        
        def on_cancel():
            result[0] = None
            dialog.destroy()
        
        ttk.Button(button_frame, text="ÂèñÊ∂à", command=on_cancel).pack(side=tk.RIGHT, padx=5)
        ttk.Button(button_frame, text="Á°ÆÂÆö", command=on_ok).pack(side=tk.RIGHT, padx=5)
        
        # Á≠âÂæÖÂØπËØùÊ°ÜÂÖ≥Èó≠
        dialog.wait_window()
        
        return result[0]


