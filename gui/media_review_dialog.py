import tkinter as tk
from tkinter import ttk, messagebox, simpledialog
import os, time, threading
import subprocess
import tempfile
import copy  # æ·»åŠ  copy æ¨¡å—å¯¼å…¥
from PIL import Image, ImageTk
from utility.file_util import get_file_path, safe_remove, safe_file
from utility.ffmpeg_audio_processor import FfmpegAudioProcessor
from utility.ffmpeg_processor import FfmpegProcessor
from utility.audio_transcriber import AudioTranscriber
import config
from utility.llm_api import LLMApi
import json
from utility.file_util import is_audio_file, is_video_file, is_image_file

# å°è¯•å¯¼å…¥æ‹–æ”¾æ”¯æŒ
try:
    from tkinterdnd2 import DND_FILES
    DND_AVAILABLE = True
except ImportError:
    DND_AVAILABLE = False
    print("è­¦å‘Š: tkinterdnd2 ä¸å¯ç”¨ï¼Œæ‹–æ”¾åŠŸèƒ½å°†è¢«ç¦ç”¨")

# Audio recording imports
try:
    import sounddevice as sd
    import soundfile as sf
    import numpy as np
    RECORDING_AVAILABLE = True
except ImportError:
    RECORDING_AVAILABLE = False
    print("è­¦å‘Š: sounddevice æˆ– soundfile ä¸å¯ç”¨ï¼Œå½•éŸ³åŠŸèƒ½å°†è¢«ç¦ç”¨")

# Video playback imports
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    print("è­¦å‘Š: cv2 ä¸å¯ç”¨ï¼Œè§†é¢‘æ’­æ”¾åŠŸèƒ½å°†è¢«ç¦ç”¨")


# Audio playback imports
try:
    import pygame
    PYGAME_AVAILABLE = True
except ImportError:
    PYGAME_AVAILABLE = False
    print("è­¦å‘Š: pygame ä¸å¯ç”¨ï¼ŒéŸ³é¢‘æ’­æ”¾åŠŸèƒ½å°†è¢«ç¦ç”¨")


class AVReviewDialog:
    """Dialog for reviewing and configuring audio replacement with drag-and-drop support"""
    
    def __init__(self, parent, av_path, current_scenario, previous_scenario, next_scenario, media_type, replace_media_audio, initial_start_time, initial_end_time):
        self.parent = parent
        self.current_scenario = current_scenario
        self.previous_scenario = previous_scenario
        self.next_scenario = next_scenario
        self.language = parent.workflow.language
        self.pid = parent.workflow.pid
        self.workflow = parent.workflow
        # Get video dimensions from workflow's ffmpeg_processor
        video_width = self.workflow.ffmpeg_processor.width
        video_height = self.workflow.ffmpeg_processor.height
        self.transcriber = AudioTranscriber(self.workflow, model_size="small", device="cuda")
        self.summarizer = LLMApi(model=LLMApi.GEMINI_2_0_FLASH)

        self.media_type_names = {
            "clip": "åœºæ™¯åª’ä½“ (clip)",
            "one": "ç¬¬ä¸€è½¨é“ (one)",
            "second": "ç¬¬äºŒè½¨é“ (second)",
            "zero": "èƒŒæ™¯è½¨é“ (zero)"
        }

        # åª’ä½“ç±»å‹é€‰æ‹© ("clip", "second", "zero")
        self.media_type = media_type
        self.replace_media_audio = replace_media_audio
        
        # åª’ä½“å­—æ®µåæ˜ å°„
        if media_type == "clip":
            self.video_field = "clip"
            self.audio_field = "clip_audio"
            self.image_field = "clip_image"
        elif media_type == "second":
            self.video_field = "second"
            self.audio_field = "second_audio"
            self.image_field = "second_image"
        elif media_type == "zero":
            self.video_field = "zero"
            self.audio_field = "zero_audio"
            self.image_field = "zero_image"
        elif media_type == "one":
            self.video_field = "one"
            self.audio_field = "one_audio"
            self.image_field = "one_image"
        # Initialize source paths
        self.source_audio_path = get_file_path(self.current_scenario, self.audio_field)
        self.source_video_path = get_file_path(self.current_scenario, self.video_field)
        self.source_image_path = get_file_path(self.current_scenario, self.image_field)
        
        self.transcribe_way = "single"
        
        # æ–°å¢æ‹–æ”¾åª’ä½“
        self.animation_choice = 1

        # only keep "content", "speaker", "story_expression", "mood", "era_time" fields of each element
        #self.audio_json = [{"content": item["content"], "speaker": item["speaker"], "story_expression": item["story_expression"], "mood": item["mood"]} for item in [self.current_scenario]]
        self.audio_json = [self.current_scenario]
        self.audio_regenerated = False

        self.current_playback_time = 0.0
        self.av_playing = False
        self.av_paused = False
        self.playback_start_time = None  # Time when playback started
        self.pause_accumulated_time = 0.0  # Total time played before pausing

        self.start_time = initial_start_time if initial_start_time else 0.0
        if initial_end_time:
            self.end_time = initial_end_time
        elif replace_media_audio=="replace" or replace_media_audio=="trim" or is_image_file(av_path):
            self.end_time = self.workflow.ffmpeg_audio_processor.get_duration(self.source_audio_path)
        elif is_audio_file(av_path):
            self.end_time = self.workflow.ffmpeg_audio_processor.get_duration(av_path)
        else:
            self.end_time = self.workflow.ffmpeg_processor.get_duration(av_path)

        self.result = None  # Will store the result when dialog is closed
        
        # Recording state variables
        self.recording = False
        self.recorded_audio = None
        self.recording_thread = None

        # Initialize video-specific states if in video mode
        self.av_playing = False
        self.video_cap = None
        self.video_after_id = None
        # Keep a list of image references to prevent garbage collection
        self.image_references = []
        
        # Crop selection variables
        self.crop_start_x = 0
        self.crop_start_y = 0
        self.crop_width = None  # None means use full width
        self.crop_height = None  # None means use full height
        self.selection_rect = None  # Canvas rectangle ID for selection
        self.selecting = False  # Whether user is currently selecting
        self.selection_start_x = 0
        self.selection_start_y = 0
        self.video_original_width = None
        self.video_original_height = None
        
        self.create_dialog()

        self.handle_new_media(av_path)
        # Load video first frame after dialog is fully created
        self.dialog.after(100, self.init_load)


    def init_load(self):
        try:
            self.audio_json_text.delete(1.0, tk.END)
            self.audio_json_text.insert(1.0, json.dumps(self.audio_json, indent=2, ensure_ascii=False))

            # Draw simple waveform representation
            self.draw_waveform_placeholder()
            
            self.display_image_on_canvas()
            # åŠ è½½å½“å‰åœºæ™¯çš„å›¾ç‰‡
            self.load_video_first_frame()
        except:
            print("error: audio_json is not valid json")


    def update_dialog_title(self, transcribe_audio):
        self.transcribe_way = transcribe_audio
        self.dialog.title( f"{self.media_type_names.get(self.media_type)} - {self.transcribe_way}" )


    def create_dialog(self):
        """Create the review dialog window"""
        self.dialog = tk.Toplevel(self.parent.root)
        
        # æ ¹æ®åª’ä½“ç±»å‹æ˜¾ç¤ºæ ‡é¢˜
        self.update_dialog_title("none")

        self.dialog.geometry("1800x1000")
        self.dialog.resizable(True, True)
        self.dialog.transient(self.parent.root)
        self.dialog.grab_set()
        
        # Main container
        main_frame = ttk.Frame(self.dialog)
        main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
        
        # Media info section
        info_frame = ttk.LabelFrame(main_frame, text="", padding=10)
        info_frame.pack(fill=tk.X, pady=(0, 10))
        
        # Media info row
        info_row = ttk.Frame(info_frame)
        info_row.pack(fill=tk.X, pady=2)
        
        ttk.Label(info_row, text=f"è§†é¢‘æ—¶é•¿: { (self.end_time-self.start_time):.2f}ç§’").pack(side=tk.LEFT)
        ttk.Separator(info_row, orient=tk.VERTICAL).pack(side=tk.LEFT, fill=tk.Y, padx=20)
        if self.source_video_path:
            if self.workflow.ffmpeg_processor.has_audio_stream(self.source_video_path):
                audio_status = "æœ‰éŸ³é¢‘"
                audio_color = "green"
            else:
                audio_status = "æ— éŸ³é¢‘"
                audio_color = "red"
            audio_label = ttk.Label(info_row, text=f"éŸ³é¢‘çŠ¶æ€: {audio_status}", foreground=audio_color)
            audio_label.pack(side=tk.LEFT)
            ttk.Separator(info_row, orient=tk.VERTICAL).pack(side=tk.LEFT, fill=tk.Y, padx=20)

        av_path = self.source_video_path if self.source_video_path else self.source_audio_path
        ttk.Label(info_row, text=f"æºåª’ä½“: {av_path}").pack(side=tk.LEFT)
        
        # Media visualization section - ä¸‰æ å¸ƒå±€ï¼šè§†é¢‘ | å›¾ç‰‡ | éŸ³é¢‘
        media_container = ttk.Frame(main_frame)
        media_container.pack(fill=tk.BOTH, expand=True, pady=(0, 10))
        
        # å·¦æ ï¼šè§†é¢‘é¢„è§ˆ + æ‹–æ”¾
        video_frame = ttk.LabelFrame(media_container, text="è§†é¢‘é¢„è§ˆ (å¯æ‹–æ”¾è§†é¢‘æ–‡ä»¶)", padding=10)
        video_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(0, 5))
        
        # Video preview canvas (æ”¯æŒæ‹–æ”¾)
        self.preview_canvas = tk.Canvas(video_frame, bg='black', height=300, highlightthickness=2, highlightbackground='blue')
        self.preview_canvas.pack(fill=tk.BOTH, expand=True)
        if DND_AVAILABLE:
            self.preview_canvas.drop_target_register(DND_FILES)
            self.preview_canvas.dnd_bind('<<Drop>>', self.on_video_dnd_drop)
        
        # Bind mouse events for crop selection
        self.preview_canvas.bind("<Button-1>", self.on_canvas_click)
        self.preview_canvas.bind("<B1-Motion>", self.on_canvas_drag)
        self.preview_canvas.bind("<ButtonRelease-1>", self.on_canvas_release)
        
        # Add crop controls below video preview
        crop_control_frame = ttk.Frame(video_frame)
        crop_control_frame.pack(fill=tk.X, pady=(5, 0))
        
        ttk.Label(crop_control_frame, text="è£å‰ªåŒºåŸŸ:").pack(side=tk.LEFT, padx=(0, 5))
        ttk.Label(crop_control_frame, text="X:").pack(side=tk.LEFT, padx=(0, 2))
        self.crop_x_var = tk.IntVar(value=0)
        ttk.Spinbox(crop_control_frame, from_=0, to=9999, textvariable=self.crop_x_var, width=8, command=self.on_crop_params_changed).pack(side=tk.LEFT, padx=(0, 5))
        
        ttk.Label(crop_control_frame, text="Y:").pack(side=tk.LEFT, padx=(0, 2))
        self.crop_y_var = tk.IntVar(value=0)
        ttk.Spinbox(crop_control_frame, from_=0, to=9999, textvariable=self.crop_y_var, width=8, command=self.on_crop_params_changed).pack(side=tk.LEFT, padx=(0, 5))
        
        ttk.Label(crop_control_frame, text="å®½åº¦:").pack(side=tk.LEFT, padx=(0, 2))
        self.crop_width_var = tk.IntVar(value=0)  # 0 means auto
        crop_width_spinbox = ttk.Spinbox(crop_control_frame, from_=0, to=9999, textvariable=self.crop_width_var, width=8, command=self.on_crop_params_changed)
        crop_width_spinbox.pack(side=tk.LEFT, padx=(0, 5))
        ttk.Label(crop_control_frame, text="(0=è‡ªåŠ¨)").pack(side=tk.LEFT, padx=(0, 5))
        
        ttk.Button(crop_control_frame, text="æ¸…é™¤é€‰æ‹©", command=self.clear_crop_selection).pack(side=tk.LEFT, padx=(5, 0))
        
        # ä¸­æ ï¼šå›¾ç‰‡æ˜¾ç¤º + æ‹–æ”¾
        image_frame = ttk.LabelFrame(media_container, text="å›¾ç‰‡ (å¯æ‹–æ”¾å›¾ç‰‡æ–‡ä»¶)", padding=10)
        image_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5)
        
        # Image canvas (æ”¯æŒæ‹–æ”¾)
        self.image_canvas = tk.Canvas(image_frame, bg='gray20', height=300, highlightthickness=2, highlightbackground='green')
        self.image_canvas.pack(fill=tk.BOTH, expand=True)
        if DND_AVAILABLE:
            self.image_canvas.drop_target_register(DND_FILES)
            self.image_canvas.dnd_bind('<<Drop>>', self.on_image_dnd_drop)
        
        # åŠ¨ç”»é€‰æ‹©
        anim_frame = ttk.Frame(image_frame)
        anim_frame.pack(fill=tk.X, pady=2)
        ttk.Label(anim_frame, text="åŠ¨ç”»:").pack(side=tk.LEFT, padx=2)
        
        self.animation_var = tk.IntVar(value=4)
        for value, text in [(1, "é™æ­¢"), (2, "å·¦"), (3, "å³"), (4, "åŠ¨ç”»")]:
            ttk.Radiobutton(anim_frame, text=text, variable=self.animation_var, value=value).pack(side=tk.LEFT, padx=2)

        # ç»‘å®š animation_var å˜åŒ–äº‹ä»¶
        self.animation_var.trace('w', self.on_animation_changed)
        
        # å³æ ï¼šéŸ³é¢‘æ³¢å½¢ + æ‹–æ”¾
        waveform_frame = ttk.LabelFrame(media_container, text="éŸ³é¢‘æ³¢å½¢ (å¯æ‹–æ”¾éŸ³é¢‘æ–‡ä»¶)", padding=10)
        waveform_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(5, 0))
        
        # Waveform canvas (æ”¯æŒæ‹–æ”¾)
        self.waveform_canvas = tk.Canvas(waveform_frame, bg='black', height=300, highlightthickness=2, highlightbackground='orange')
        self.waveform_canvas.pack(fill=tk.BOTH, expand=True)
        if DND_AVAILABLE:
            self.waveform_canvas.drop_target_register(DND_FILES)
            self.waveform_canvas.dnd_bind('<<Drop>>', self.on_audio_dnd_drop)
        
        # Media controls (placed below the media visualization)
        control_container = ttk.Frame(main_frame)
        control_container.pack(fill=tk.X, pady=(0, 10))
        
        # Media controls
        control_frame = ttk.Frame(control_container)
        control_frame.pack(fill=tk.X, pady=5)
        
        self.play_button = ttk.Button(control_frame, text="â–¶ æ’­æ”¾", command=self.toggle_playback)
        self.play_button.pack(side=tk.LEFT, padx=15)
        
        self.stop_button = ttk.Button(control_frame, text="â¹ åœæ­¢", command=self.stop_playback)
        self.stop_button.pack(side=tk.LEFT, padx=15)

        separator = ttk.Separator(control_frame, orient='vertical')
        separator.pack(side=tk.LEFT, fill=tk.Y, padx=15)

        ttk.Button(control_frame, text="è·³è½¬å¼€å§‹", command=self.jump_to_start).pack(side=tk.LEFT, padx=15)
        ttk.Button(control_frame, text="æ’­æ”¾é€‰å®š", command=self.play_selected_range).pack(side=tk.LEFT, padx=15)
        
        separator = ttk.Separator(control_frame, orient='vertical')
        separator.pack(side=tk.LEFT, fill=tk.Y, padx=15)

        self.play_time_label = ttk.Label(control_frame, text="0.00 / 0.00", foreground="blue")
        self.play_time_label.pack(side=tk.LEFT, padx=15)

        separator = ttk.Separator(control_frame, orient='vertical')
        separator.pack(side=tk.LEFT, fill=tk.Y, padx=15)

        ttk.Label(control_frame, text="å¼€å§‹(ç§’):").pack(side=tk.LEFT, padx=(0, 5))
        self.start_time_var = tk.DoubleVar(value=self.start_time)
        max_duration = self.workflow.ffmpeg_processor.get_duration(self.source_video_path) if self.source_video_path else self.workflow.ffmpeg_audio_processor.get_duration(self.source_audio_path)
        start_spinbox = ttk.Spinbox(control_frame, from_=0, to=max_duration, 
                                   textvariable=self.start_time_var, increment=0.1, width=8)
        start_spinbox.pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(control_frame, text="è®¾ä¸ºå½“å‰", command=self.set_start_to_current).pack(side=tk.LEFT, padx=(0, 10))
        
        # åˆ†éš”ç¬¦
        ttk.Separator(control_frame, orient=tk.VERTICAL).pack(side=tk.LEFT, fill=tk.Y, padx=20)
        
        # ç»“æŸæ—¶é—´ç»„
        ttk.Label(control_frame, text="ç»“æŸæ—¶é—´ (ç§’):").pack(side=tk.LEFT, padx=(5, 5))
        self.end_time_var = tk.DoubleVar(value=self.end_time)
        end_spinbox = ttk.Spinbox(control_frame, from_=0, to=max_duration, 
                                 textvariable=self.end_time_var, increment=0.1, width=8)
        end_spinbox.pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(control_frame, text="è®¾ä¸ºå½“å‰", command=self.set_end_to_current).pack(side=tk.LEFT)
        
        # åˆ†éš”ç¬¦
        ttk.Separator(control_frame, orient=tk.VERTICAL).pack(side=tk.LEFT, fill=tk.Y, padx=20)

        self.selected_duration_label = ttk.Label(control_frame, text="", foreground="blue")
        self.selected_duration_label.pack(side=tk.LEFT)

        # åˆ†éš”ç¬¦
        ttk.Separator(control_frame, orient=tk.VERTICAL).pack(side=tk.LEFT, fill=tk.Y, padx=20)
        
        # add a button to let user record the audio from microphone, then put it as source_audio_path, then transcribe it, set self.regenerate_audio to True
        ttk.Button(control_frame, text="å½•éŸ³", command=self.record_audio).pack(side=tk.LEFT, padx=(0, 10))

        # Initialize play time display
        self.update_play_time_display()
        
        # Bind changes to update duration display
        self.start_time_var.trace('w', self.update_duration_display)
        self.end_time_var.trace('w', self.update_duration_display)
        self.update_duration_display()
        
        # Text editors section for JSON data
        editors_frame = ttk.LabelFrame(main_frame, text="JSONç¼–è¾‘å™¨", padding=10)
        editors_frame.pack(fill=tk.BOTH, expand=True, pady=(0, 10))
        
        # Create horizontal frame for side-by-side editors
        editors_container = ttk.Frame(editors_frame)
        editors_container.pack(fill=tk.BOTH, expand=True)
        
        # Editor 1: Fresh JSON (left side)
        fresh_frame = ttk.Frame(editors_container)
        fresh_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(0, 5))
        
        fresh_label = ttk.Label(fresh_frame, text="Fresh JSON")
        fresh_label.pack(anchor="w", pady=(0, 5))
        
        # Fresh JSON text editor with scrollbar
        fresh_text_frame = ttk.Frame(fresh_frame)
        fresh_text_frame.pack(fill=tk.BOTH, expand=True)
        
        self.fresh_json_text = tk.Text(fresh_text_frame, wrap=tk.WORD, width=40, height=15)
        fresh_scrollbar = ttk.Scrollbar(fresh_text_frame, orient="vertical", command=self.fresh_json_text.yview)
        self.fresh_json_text.configure(yscrollcommand=fresh_scrollbar.set)
        
        self.fresh_json_text.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        fresh_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        
        # ç»‘å®š Alt+Enter å¿«æ·é”®åˆ° fresh_json_text
        self.fresh_json_text.bind('<Alt-Return>', self.copy_fresh_to_audio_json)
        
        # Buttons for fresh JSON editor
        fresh_buttons_frame = ttk.Frame(fresh_frame)
        fresh_buttons_frame.pack(fill=tk.X, pady=(5, 0))
        
        # æç¤ºè¯æ¨¡æ¿ç»„
        ttk.Label(fresh_buttons_frame, text="æ¨¡æ¿:").pack(side=tk.LEFT, padx=(0, 5))
        self.prompt_selector = ttk.Combobox(fresh_buttons_frame, values=config.SPEAKING_PROMPTS_LIST, state="readonly", width=25)
        self.prompt_selector.pack(side=tk.LEFT, padx=(0, 10))
        self.prompt_selector.current(0)  # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ª

        # æ—ç™½è¯­éŸ³ç»„
        ttk.Label(fresh_buttons_frame, text="ä¸»æŒ").pack(side=tk.LEFT, padx=(0, 5))
        self.narrators = ttk.Combobox(fresh_buttons_frame, values=config.HOSTS, state="normal", width=30)
        self.narrators.pack(side=tk.LEFT, padx=(0, 10))
        self.narrators.current(0)

        # æ—ç™½è¯­éŸ³ç»„
        ttk.Label(fresh_buttons_frame, text="æ¼”å‘˜").pack(side=tk.LEFT, padx=(0, 5))
        self.actors = ttk.Combobox(fresh_buttons_frame, values=config.ACTORS, state="normal", width=30)
        self.actors.pack(side=tk.LEFT, padx=(0, 10))
        self.actors.current(0)

        ttk.Label(fresh_buttons_frame, text="è¡¥å……").pack(side=tk.LEFT, padx=(0, 5))
        self.speaking_addon = ttk.Combobox(fresh_buttons_frame, values=config.SPEAKING_ADDON, state="readonly", width=15)
        self.speaking_addon.pack(side=tk.LEFT, padx=(0, 10))
        self.speaking_addon.current(0)

        ttk.Button(fresh_buttons_frame, text="REMIX JSON", command=self.remix_json).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(fresh_buttons_frame, text="é‡ç”ŸéŸ³é¢‘", command=self.regenerate_audio).pack(side=tk.LEFT)
        
        # Editor 2: Audio JSON (right side)
        audio_frame = ttk.Frame(editors_container)
        audio_frame.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True, padx=(5, 0))
        
        audio_label = ttk.Label(audio_frame, text="Audio JSON")
        audio_label.pack(anchor="w", pady=(0, 5))
        
        # Audio JSON text editor with scrollbar
        audio_text_frame = ttk.Frame(audio_frame)
        audio_text_frame.pack(fill=tk.BOTH, expand=True)
        
        self.audio_json_text = tk.Text(audio_text_frame, wrap=tk.WORD, width=40, height=15)
        audio_scrollbar = ttk.Scrollbar(audio_text_frame, orient="vertical", command=self.audio_json_text.yview)
        self.audio_json_text.configure(yscrollcommand=audio_scrollbar.set)
        
        self.audio_json_text.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        audio_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)

        # Button for audio JSON editor
        audio_buttons_frame = ttk.Frame(audio_frame)
        audio_buttons_frame.pack(fill=tk.X, pady=(5, 0))
        
        ttk.Button(audio_buttons_frame, text="æ¸è¿›", command=self.audio_fade).pack(side=tk.LEFT)
        ttk.Button(audio_buttons_frame, text="å‰ªéŸ³è§†", command=self.trim_media).pack(side=tk.LEFT)
        ttk.Button(audio_buttons_frame, text="å‰ªè§†é¢‘", command=self.trim_video).pack(side=tk.LEFT)
        ttk.Button(audio_buttons_frame, text="å•è½¬å½•", command=self.trim_transcribe_single).pack(side=tk.LEFT)
        ttk.Button(audio_buttons_frame, text="å¤šè½¬å½•", command=self.trim_transcribe_multiple).pack(side=tk.LEFT)
        
        # Audio transcription options section
        transcribe_frame = ttk.LabelFrame(main_frame, text="éŸ³é¢‘è½¬å½•é€‰é¡¹", padding=10)
        transcribe_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(0, 5))
        
        # Video-specific controls (only show in video mode)
        self.track_mode = tk.IntVar(value=1)  # Default to mode 2
        if self.source_video_path:
            video_control_frame = ttk.LabelFrame(main_frame, text="å¤„ç†æ¨¡å¼", padding=10)
            video_control_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(5, 0))
            
            ttk.Radiobutton(video_control_frame, text="æ­£å¸¸", variable=self.track_mode, value=1).pack(side=tk.LEFT, padx=5)
            ttk.Radiobutton(video_control_frame, text="æ¸å…¥", variable=self.track_mode, value=2).pack(side=tk.LEFT, padx=5)
            ttk.Radiobutton(video_control_frame, text="æ¸å‡º", variable=self.track_mode, value=3).pack(side=tk.LEFT, padx=5)
            ttk.Radiobutton(video_control_frame, text="å‡ºå…¥", variable=self.track_mode, value=4).pack(side=tk.LEFT, padx=5)

        # Buttons
        button_frame = ttk.Frame(main_frame)
        button_frame.pack(fill=tk.X, pady=(10, 0))
        
        ttk.Button(button_frame, text="ç¡®è®¤æ›¿æ¢", command=self.confirm_replacement).pack(side=tk.RIGHT, padx=(5, 0))
        ttk.Button(button_frame, text="å–æ¶ˆ", command=self.cancel).pack(side=tk.RIGHT)
        
        if not pygame.mixer.get_init():
            pygame.mixer.init(frequency=44100, size=-16, channels=2, buffer=2048)
        

    def draw_waveform_placeholder(self):
        """Draw a simple waveform placeholder"""
        if not self.source_audio_path:
            return

        width = 750
        height = 180
        center_y = height // 2
        
        # Clear canvas
        self.waveform_canvas.delete("all")
        
        # Draw simple waveform simulation
        import math
        for x in range(0, width, 2):
            # Create random-looking waveform
            amplitude = 50 * math.sin(x * 0.05) * (0.5 + 0.5 * math.sin(x * 0.01))
            y1 = center_y - amplitude
            y2 = center_y + amplitude
            self.waveform_canvas.create_line(x, y1, x, y2, fill="green", width=1)
        
        # Draw time markers  
        display_duration = self.workflow.ffmpeg_audio_processor.get_duration(self.source_audio_path)
        if display_duration > 0:
            for i in range(0, int(display_duration) + 1, max(1, int(display_duration) // 10)):
                x = (i / display_duration) * width
                self.waveform_canvas.create_line(x, 0, x, height, fill="gray", width=1)
                self.waveform_canvas.create_text(x, height - 10, text=f"{i}s", fill="white", anchor="n")
    

    def update_duration_display(self, *args):
        """Update the selected duration display"""
        if not self.source_audio_path:
            return
        try:
            start = self.start_time_var.get()
            end = self.end_time_var.get()
            if end > start:
                duration = end - start
                self.selected_duration_label.config(text=f"{duration:.2f}ç§’")

                # Update waveform selection visualization
                self.waveform_canvas.delete("selection")
                # Draw selection overlay
                width = self.waveform_canvas.winfo_width()
                height = self.waveform_canvas.winfo_height()
                
                display_duration = self.workflow.ffmpeg_processor.get_duration(self.source_video_path) if self.source_video_path else self.workflow.ffmpeg_audio_processor.get_duration(self.source_audio_path)
                if display_duration > 0:
                    start_x = (start / display_duration) * width
                    end_x = (end / display_duration) * width
                else:
                    start_x = end_x = 0
                # Draw selection rectangle
                self.waveform_canvas.create_rectangle(start_x, 0, end_x, height, 
                                                    fill="yellow", stipple="gray50", tags="selection")

            else:
                self.selected_duration_label.config(text="æ— æ•ˆæ—¶é—´æ®µ")
        except:
            self.selected_duration_label.config(text="--")
    

    def update_play_time_display(self):
        """Update the play time display"""
        try:
            current_time = self.current_playback_time
            
            # Get total duration from video or audio
            if self.source_video_path:
                total_duration = self.workflow.ffmpeg_processor.get_duration(self.source_video_path)
            elif self.source_audio_path:
                total_duration = self.workflow.ffmpeg_audio_processor.get_duration(self.source_audio_path)
            else:
                total_duration = 0.0
            
            # Ensure we have valid values
            if total_duration is None or total_duration <= 0:
                total_duration = 0.0
            if current_time is None or current_time < 0:
                current_time = 0.0
                
            current_str = f"{current_time:.2f}"
            total_str = f"{total_duration:.2f}"
            
            self.play_time_label.config(text=f"{current_str} / {total_str}")
            
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°æ—¶é—´æ˜¾ç¤ºå¤±è´¥: {e}")
            self.play_time_label.config(text="0.00 / 0.00")
    

    def set_start_to_current(self):
        """Set start time to current playback position"""
        self.start_time_var.set(self.current_playback_time)
        self.update_play_time_display()
    

    def set_end_to_current(self):
        """Set end time to current playback position"""
        self.end_time_var.set(self.current_playback_time)
        self.update_play_time_display()
    

    def audio_fade(self):
        """Audio fade"""
        if self.source_audio_path:
            self.source_audio_path = self.workflow.ffmpeg_audio_processor.audio_change(self.source_audio_path, 1.5, 1.5, 1.0, 0.0)
        if self.source_video_path:
            self.source_video_path = self.workflow.ffmpeg_processor.add_audio_to_video(self.source_video_path, self.source_audio_path)


    def trim_video(self):
        self.update_dialog_title("none")

        video_path = safe_file(self.source_video_path)
        audio_path = safe_file(self.source_audio_path)
        if not audio_path or not video_path:
            return

        start_time = float(self.start_time_var.get())
        end_time = float(self.end_time_var.get())
        duration = self.workflow.ffmpeg_processor.get_duration(video_path)
        
        if end_time <= start_time:
            messagebox.showerror("é”™è¯¯", "ç»“æŸæ—¶é—´å¿…é¡»å¤§äºå¼€å§‹æ—¶é—´")
            return
        
        if start_time < 0 or end_time > duration:
            messagebox.showerror("é”™è¯¯", "æ—¶é—´é€‰æ‹©è¶…å‡ºéŸ³é¢‘èŒƒå›´")
            return

        if start_time > 0 or end_time != duration:
            # Get crop parameters if selection exists
            crop_width = self.crop_width if self.crop_width else None
            crop_height = self.crop_height if self.crop_height else None
            
            video_path = self.workflow.ffmpeg_processor.resize_video( video_path, crop_width, start_time, end_time, 
                                                                      volume=1.0, start_x=self.crop_start_x, start_y=self.crop_start_y )
            self.source_video_path = self.workflow.ffmpeg_processor.add_audio_to_video(video_path, audio_path, True)



    def trim_media(self):
        self.update_dialog_title("none")

        """Trim media"""
        """Trim and transcribe audio"""
        video_path = safe_file(self.source_video_path)
        audio_path = safe_file(self.source_audio_path)
        if not audio_path or not video_path:
            return

        start_time = float(self.start_time_var.get())
        end_time = float(self.end_time_var.get())
        duration = self.workflow.ffmpeg_processor.get_duration(video_path) if video_path else self.workflow.ffmpeg_audio_processor.get_duration(audio_path)
        
        if end_time <= start_time:
            messagebox.showerror("é”™è¯¯", "ç»“æŸæ—¶é—´å¿…é¡»å¤§äºå¼€å§‹æ—¶é—´")
            return
        
        if start_time < 0 or end_time > duration:
            messagebox.showerror("é”™è¯¯", "æ—¶é—´é€‰æ‹©è¶…å‡ºéŸ³é¢‘èŒƒå›´")
            return
        
        if start_time > 0 or end_time != duration:
            self.source_audio_path = self.workflow.ffmpeg_audio_processor.audio_cut_fade( audio_path, start_time, end_time-start_time )
            
            # Get crop parameters if selection exists
            crop_width = self.crop_width if self.crop_width else None
            crop_height = self.crop_height if self.crop_height else None
            
            self.source_video_path = self.workflow.ffmpeg_processor.resize_video( video_path, crop_width, start_time, end_time, 
                                                                                  volume=1.0, start_x=self.crop_start_x, start_y=self.crop_start_y )


    def trim_transcribe_single(self):
        self.trim_media()
        self.update_dialog_title("single")
        if not self.audio_regenerated or self.media_type=="clip":
            self._transcribe_recorded_audio()


    def trim_transcribe_multiple(self):
        self.trim_media()
        self.update_dialog_title("multiple")
        if not self.audio_regenerated and self.media_type=="clip":
            self._transcribe_recorded_audio()


    def _transcribe_recorded_audio(self):
        """è½¬å½•å½•åˆ¶çš„éŸ³é¢‘"""
        if not self.source_audio_path:
            return
        
        print("ğŸ”„ å¼€å§‹è½¬å½•å½•éŸ³...")
        
        # ä½¿ç”¨éŸ³é¢‘è½¬å½•å™¨è½¬å½•
        self.audio_json = self.transcriber.transcribe_with_whisper(
            self.source_audio_path, 
            self.language, 
            10,  # min_sentence_duration
            28   # max_sentence_duration
        )
        
        if self.audio_json:
            try:
                formatted_json = json.dumps(self.audio_json, indent=2, ensure_ascii=False)
                self.fresh_json_text.delete(1.0, tk.END)
                self.fresh_json_text.insert(1.0, formatted_json)
                self.audio_json_text.delete(1.0, tk.END)
                self.audio_json_text.insert(1.0, formatted_json)
            except Exception as e:
                print(f"JSONæ ¼å¼é”™è¯¯: {str(e)}")
        else:
            print("âš ï¸ å½•éŸ³è½¬å½•å¤±è´¥")


    def toggle_playback(self):
        """Toggle media playback (audio or video+audio)"""
        if not self.av_playing:
            self.start_playback()
        else:
            self.pause_playback()


    def start_playback(self):
        self.av_playing = True
        self.av_paused = False
        self.play_button.config(text="â¸ æš‚åœ")
        
        if self.source_video_path:
            # å¦‚æœè§†é¢‘æ•è·å¯¹è±¡ä¸å­˜åœ¨ï¼Œæˆ–è€…éœ€è¦ä»å¤´å¼€å§‹æ’­æ”¾ï¼Œå°±é‡æ–°åˆ›å»º
            if self.video_cap is None or self.current_playback_time == 0.0:
                # é‡Šæ”¾æ—§çš„è§†é¢‘æ•è·å¯¹è±¡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if self.video_cap:
                    self.video_cap.release()
                # åˆ›å»ºæ–°çš„è§†é¢‘æ•è·å¯¹è±¡
                self.video_cap = cv2.VideoCapture(self.source_video_path)
                if not self.video_cap.isOpened():
                    print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {self.source_video_path}")
                    self.av_playing = False
                    self.play_button.config(text="â–¶ æ’­æ”¾")
                    return
                print(f"âœ“ å·²åŠ è½½è§†é¢‘: {os.path.basename(self.source_video_path)}")
            
            # å¦‚æœéœ€è¦ä»æŒ‡å®šä½ç½®å¼€å§‹æ’­æ”¾ï¼Œè®¾ç½®è§†é¢‘ä½ç½®
            if self.current_playback_time > 0:
                fps = self.video_cap.get(cv2.CAP_PROP_FPS) or 30
                start_frame = int(self.current_playback_time * fps)
                self.video_cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
            
            # Start video frame updates
            self.update_video_frame()
            
        if self.source_audio_path:
            pygame.mixer.music.stop()
            pygame.mixer.music.load(self.source_audio_path)
            pygame.mixer.music.play(start=self.current_playback_time)
            
        # Record the start time for tracking
        self.playback_start_time = time.time()
    
        self.start_time_update_thread()


    def pause_playback(self):
        """Pause audio-only playback"""
        if self.playback_start_time is not None:
            elapsed_since_start = time.time() - self.playback_start_time
            self.pause_accumulated_time += elapsed_since_start
            self.current_playback_time = self.pause_accumulated_time

        self.av_playing = False
        self.play_button.config(text="â–¶ æ’­æ”¾")
        
        # Cancel video frame updates
        if self.video_after_id:
            self.dialog.after_cancel(self.video_after_id)
            
        # Pause audio BEFORE changing the state
        if self.source_audio_path:
            print("ğŸ”„ æ­£åœ¨åœæ­¢éŸ³é¢‘æ’­æ”¾...")
            pygame.mixer.music.stop()
            self.av_paused = True


    def stop_playback(self):
        """Stop media playback (audio or video+audio)"""
        self.av_playing = False
        self.av_paused = False
        self.play_button.config(text="â–¶ æ’­æ”¾")
        self.current_playback_time = 0.0
        
        # Cancel video frame updates
        if self.source_video_path:
            if self.video_after_id:
                self.dialog.after_cancel(self.video_after_id)
            # Reset video to beginning
            if self.video_cap:
                self.video_cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
        
        if self.source_audio_path:
            pygame.mixer.music.stop()
            print(f"â¹ éŸ³é¢‘æ’­æ”¾åœæ­¢")
        
        # Reset playback time tracking
        self.pause_accumulated_time = 0.0
        self.playback_start_time = None
        
        # Update play time display to show 0.00 / total_duration
        self.update_play_time_display()
    

    def remix_json(self):
        refresh_content = self.fresh_json_text.get(1.0, tk.END).strip()
        if not refresh_content or refresh_content.strip() == "":
            return

        # try to format formatted_user_prompt as json , if success, take 'content' field of each elemet, concat together as whole content
        try:
            refresh_json = json.loads(refresh_content)
            refresh_content = " ".join([item["content"] for item in refresh_json])
        except:
            print(f"âš ï¸ åˆ·æ–°å†…å®¹æ ¼å¼åŒ–å¤±è´¥")


        previous_scenario_content = self.previous_scenario["content"] if self.previous_scenario and hasattr(self.previous_scenario, "content") else ""
        previous_story_content = self.previous_scenario["story_summary"] if self.previous_scenario and hasattr(self.previous_scenario, "story_summary") else ""

        next_scenario_content = self.next_scenario["content"] if self.next_scenario and hasattr(self.next_scenario, "content") else ""
        next_story_content = self.next_scenario["story_summary"] if self.next_scenario and hasattr(self.next_scenario, "story_summary")  else ""

        prompt_name = self.prompt_selector.get()
        if prompt_name == "Reorganize-Text":
            engaging = ""
            selected_prompt = config.SPEAKING_PROMPTS["Reorganize-Text"]
        elif prompt_name == "Reorganize-Text-with-Previous-Scenario":
            engaging = "[the conversation is following the previous speaking like: " + previous_scenario_content + "]"
            selected_prompt = config.SPEAKING_PROMPTS["Reorganize-Text"]
        elif prompt_name == "Reorganize-Text-with-Previous-Story":
            engaging = "[the conversation is following the previous story : " + previous_story_content + "]"
            selected_prompt = config.SPEAKING_PROMPTS["Reorganize-Text"]
        elif prompt_name == "Reorganize-Text-with-Next-Scenario":
            engaging = "[the conversation will be followed by the next speaking like: " + next_scenario_content + "]"
            selected_prompt = config.SPEAKING_PROMPTS["Reorganize-Text"]
        elif prompt_name == "Reorganize-Text-with-Next-Story":
            engaging = "[the conversation will be followed by the next story : " + next_story_content + "]"
            selected_prompt = config.SPEAKING_PROMPTS["Reorganize-Text"]
        else:
            selected_prompt = config.SPEAKING_PROMPTS[prompt_name]
            engaging = ""

        format_args = selected_prompt.get("format_args", {}).copy()  # å¤åˆ¶é¢„è®¾å‚æ•°
        format_args.update({
            "speaker_style": f"with {self.narrators.get()} and {self.actors.get()}",
            "language": "Chinese" if self.language == "zh" or self.language == "tw" else "English",
            "engaging": engaging
        })

        formatted_system_prompt = selected_prompt["system_prompt"].format(**format_args)
        print("ğŸ¤– ç³»ç»Ÿæç¤º:")
        print(formatted_system_prompt)
        
        # pop up a dialog to show the system prompt, user can edit the system prompt, then click confirm to continue  to self.summarizer.generate_json_summary ..
        system_prompt_dialog = tk.Toplevel(self.dialog)
        system_prompt_dialog.title("ç³»ç»Ÿæç¤º")
        system_prompt_dialog.geometry("600x400")
        system_prompt_dialog.resizable(True, True)
        system_prompt_dialog.transient(self.dialog)
        system_prompt_dialog.grab_set()

        # æ·»åŠ æ ‡ç­¾è¯´æ˜
        instruction_label = tk.Label(system_prompt_dialog, text="è¯·ç¼–è¾‘ç³»ç»Ÿæç¤ºï¼ˆå¯ä¿®æ”¹åç‚¹å‡»ç¡®è®¤ï¼‰ï¼š")
        instruction_label.pack(pady=(10, 5))

        # ä½¿ç”¨Textå°éƒ¨ä»¶ä»£æ›¿Labelï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥ç¼–è¾‘
        system_prompt_text = tk.Text(system_prompt_dialog, wrap=tk.WORD, height=15, width=70)
        system_prompt_text.pack(pady=10, padx=10, fill=tk.BOTH, expand=True)
        
        # æ’å…¥å½“å‰çš„ç³»ç»Ÿæç¤ºæ–‡æœ¬
        system_prompt_text.insert(1.0, formatted_system_prompt)

        # æ·»åŠ æ»šåŠ¨æ¡
        scrollbar = tk.Scrollbar(system_prompt_dialog, command=system_prompt_text.yview)
        system_prompt_text.config(yscrollcommand=scrollbar.set)
        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)

        # ç”¨äºå­˜å‚¨ç”¨æˆ·ç¼–è¾‘åçš„æç¤º
        edited_prompt = [formatted_system_prompt]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨é—­åŒ…ä¸­ä¿®æ”¹

        def confirm_and_close():
            # è·å–ç¼–è¾‘åçš„æ–‡æœ¬
            edited_prompt[0] = system_prompt_text.get(1.0, tk.END).strip()
            system_prompt_dialog.destroy()

        confirm_button = tk.Button(system_prompt_dialog, text="ç¡®è®¤", command=confirm_and_close)
        confirm_button.pack(pady=10)

        system_prompt_dialog.wait_window()

        # ä½¿ç”¨ç¼–è¾‘åçš„ç³»ç»Ÿæç¤º
        formatted_system_prompt = edited_prompt[0]

        new_scenarios = self.summarizer.llm.generate_json_summary(
            system_prompt=formatted_system_prompt,
            user_prompt=refresh_content,
            expect_list=True
        )

        formatted_json = json.dumps(new_scenarios, indent=2, ensure_ascii=False)
        # clean self.fresh_json_text, then insert formatted_json
        self.fresh_json_text.delete(1.0, tk.END)
        self.fresh_json_text.insert(1.0, formatted_json)
        self.audio_regenerated = False
         

    def copy_fresh_to_audio_json(self, event=None):
        fresh_text = self.fresh_json_text.get(1.0, tk.END).strip()
        # éªŒè¯JSONæ ¼å¼
        try:
            self.audio_json = json.loads(fresh_text)
            # JSONæ ¼å¼æœ‰æ•ˆï¼Œæ¸…ç©ºaudio_json_textå¹¶å¤åˆ¶å†…å®¹
            self.audio_json_text.delete(1.0, tk.END)
            self.audio_json_text.insert(1.0, fresh_text)
        except Exception as e:
            # å…¶ä»–é”™è¯¯
            messagebox.showerror("é”™è¯¯", f"å¤åˆ¶è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")


    def regenerate_audio(self):
        fresh_text = self.fresh_json_text.get(1.0, tk.END).strip()
        # check if fresh_json_text is valid json
        fresh_json = None
        try:
            fresh_json = json.loads(fresh_text)
        except:
            messagebox.showerror("é”™è¯¯", "Fresh JSONæ ¼å¼ä¸æ­£ç¡®")
            return

        fresh_json, self.source_audio_path = self.parent.workflow.regenerate_audio(fresh_json, self.language)
        if self.source_video_path:
            duration = self.workflow.ffmpeg_audio_processor.get_duration(self.source_audio_path)
            self.source_video_path = self.workflow.ffmpeg_processor.adjust_video_to_duration( self.source_video_path, duration )

        self.audio_json = fresh_json
        audio_text = json.dumps(fresh_json, indent=2, ensure_ascii=False)

        self.audio_json_text.delete(1.0, tk.END)
        self.audio_json_text.insert(1.0, audio_text)

        self.fresh_json_text.delete(1.0, tk.END)
        self.fresh_json_text.insert(1.0, audio_text)

        self.audio_regenerated = True
        self.update_dialog_title("multiple")


    def record_audio(self):
        """å½•éŸ³åŠŸèƒ½ï¼šä»éº¦å…‹é£å½•éŸ³å¹¶è®¾ç½®ä¸ºæºéŸ³é¢‘"""
        if not RECORDING_AVAILABLE:
            messagebox.showerror("é”™è¯¯", "å½•éŸ³åŠŸèƒ½ä¸å¯ç”¨ã€‚è¯·å®‰è£… sounddevice å’Œ soundfile åº“ã€‚")
            return
        
        if self.recording:
            self.stop_recording()
        else:
            self.start_recording()


    def start_recording(self):
        """å¼€å§‹å½•éŸ³"""
        try:
            # åˆ›å»ºå½•éŸ³å¯¹è¯æ¡†
            self.recording_dialog = tk.Toplevel(self.dialog)
            self.recording_dialog.title("å½•éŸ³ä¸­...")
            self.recording_dialog.geometry("400x200")
            self.recording_dialog.resizable(False, False)
            self.recording_dialog.transient(self.dialog)
            self.recording_dialog.grab_set()
            
            # å½•éŸ³å¯¹è¯æ¡†å¸ƒå±€
            main_frame = ttk.Frame(self.recording_dialog, padding=20)
            main_frame.pack(fill=tk.BOTH, expand=True)
            
            # å½•éŸ³çŠ¶æ€æ˜¾ç¤º
            self.recording_status_label = ttk.Label(main_frame, text="æ­£åœ¨å½•éŸ³...", 
                                                  font=("Arial", 14), foreground="red")
            self.recording_status_label.pack(pady=10)
            
            # å½•éŸ³æ—¶é•¿æ˜¾ç¤º
            self.recording_time_label = ttk.Label(main_frame, text="00:00", 
                                                font=("Arial", 12))
            self.recording_time_label.pack(pady=5)
            
            # åœæ­¢å½•éŸ³æŒ‰é’®
            ttk.Button(main_frame, text="åœæ­¢å½•éŸ³", command=self.stop_recording).pack(pady=20)
            
            # å½•éŸ³å‚æ•°
            self.sample_rate = 44100  # é‡‡æ ·ç‡
            self.channels = 1  # å•å£°é“
            self.recording = True
            self.recorded_audio = []
            self.recording_start_time = time.time()
            
            # å¼€å§‹å½•éŸ³çº¿ç¨‹
            self.recording_thread = threading.Thread(target=self._recording_worker, daemon=True)
            self.recording_thread.start()
            
            # å¼€å§‹æ—¶é—´æ›´æ–°çº¿ç¨‹
            self._update_recording_time()
            
            print("ğŸ¤ å¼€å§‹å½•éŸ³...")
            
        except Exception as e:
            messagebox.showerror("é”™è¯¯", f"å¯åŠ¨å½•éŸ³å¤±è´¥: {str(e)}")
            self.recording = False


    def _recording_worker(self):
        """å½•éŸ³å·¥ä½œçº¿ç¨‹"""
        try:
            # å›è°ƒå‡½æ•°ç”¨äºæ¥æ”¶éŸ³é¢‘æ•°æ®
            def audio_callback(indata, frames, time, status):
                if status:
                    print(f"å½•éŸ³çŠ¶æ€: {status}")
                if self.recording:
                    self.recorded_audio.append(indata.copy())
            
            # å¼€å§‹éŸ³é¢‘æµ
            with sd.InputStream(samplerate=self.sample_rate, 
                              channels=self.channels, 
                              callback=audio_callback,
                              dtype='float32'):
                while self.recording:
                    time.sleep(0.1)
                    
        except Exception as e:
            print(f"å½•éŸ³çº¿ç¨‹é”™è¯¯: {e}")
            self.recording = False
            # åœ¨ä¸»çº¿ç¨‹ä¸­æ˜¾ç¤ºé”™è¯¯
            self.dialog.after(0, lambda: messagebox.showerror("é”™è¯¯", f"å½•éŸ³å¤±è´¥: {str(e)}"))


    def _update_recording_time(self):
        """æ›´æ–°å½•éŸ³æ—¶é—´æ˜¾ç¤º"""
        if self.recording and hasattr(self, 'recording_dialog') and self.recording_dialog.winfo_exists():
            elapsed = time.time() - self.recording_start_time
            minutes = int(elapsed // 60)
            seconds = int(elapsed % 60)
            time_str = f"{minutes:02d}:{seconds:02d}"
            self.recording_time_label.config(text=time_str)
            
            # æ¯100msæ›´æ–°ä¸€æ¬¡
            self.recording_dialog.after(100, self._update_recording_time)


    def stop_recording(self):
        """åœæ­¢å½•éŸ³å¹¶ä¿å­˜æ–‡ä»¶"""
        if not self.recording:
            return
            
        self.recording = False
        
        try:
            # å…³é—­å½•éŸ³å¯¹è¯æ¡†
            if hasattr(self, 'recording_dialog') and self.recording_dialog.winfo_exists():
                self.recording_dialog.destroy()
            
            if not self.recorded_audio:
                messagebox.showwarning("è­¦å‘Š", "æ²¡æœ‰å½•åˆ¶åˆ°éŸ³é¢‘æ•°æ®")
                return
            
            # åˆå¹¶å½•éŸ³æ•°æ®
            audio_data = np.concatenate(self.recorded_audio, axis=0)
            
            # ä¿å­˜å½•éŸ³æ–‡ä»¶
            recorded_file_path = config.get_temp_file(self.parent.workflow.pid, "wav")
            sf.write(recorded_file_path, audio_data, self.sample_rate)
            
            print(f"âœ“ å½•éŸ³ä¿å­˜åˆ°: {recorded_file_path}")
            print(f"âœ“ å½•éŸ³æ—¶é•¿: {len(audio_data) / self.sample_rate:.2f} ç§’")
            
            # è®¾ç½®ä¸ºæºéŸ³é¢‘è·¯å¾„
            if self.source_audio_path:
                safe_remove(self.source_audio_path)  # æ¸…ç†ä¹‹å‰çš„éŸ³é¢‘æ–‡ä»¶
            
            self.source_audio_path = recorded_file_path
            
            # æ›´æ–°éŸ³é¢‘æ—¶é•¿å’Œæ—¶é—´é€‰æ‹©å™¨
            duration = self.workflow.ffmpeg_audio_processor.get_duration(self.source_audio_path)
            self.start_time = 0.0
            self.end_time = duration
            self.start_time_var.set(0.0)
            self.end_time_var.set(duration)
            
            # æ›´æ–°æ—¶é—´é€‰æ‹©å™¨çš„æœ€å¤§å€¼
            for widget in self.dialog.winfo_children():
                if isinstance(widget, ttk.Frame):
                    for child in widget.winfo_children():
                        if isinstance(child, ttk.Spinbox):
                            try:
                                child.configure(to=duration)
                            except:
                                pass
            
            # é‡æ–°ç»˜åˆ¶æ³¢å½¢
            self.draw_waveform_placeholder()
            
            # è‡ªåŠ¨è½¬å½•å½•éŸ³
            self._transcribe_recorded_audio()
            
            # è®¾ç½®éŸ³é¢‘é‡æ–°ç”Ÿæˆæ ‡å¿—
            self.audio_regenerated = False
            
            messagebox.showinfo("æˆåŠŸ", f"å½•éŸ³å®Œæˆï¼\næ–‡ä»¶ä¿å­˜åˆ°: {os.path.basename(recorded_file_path)}\næ—¶é•¿: {duration:.2f} ç§’")
            
        except Exception as e:
            messagebox.showerror("é”™è¯¯", f"ä¿å­˜å½•éŸ³å¤±è´¥: {str(e)}")
            print(f"ä¿å­˜å½•éŸ³é”™è¯¯: {e}")


    def confirm_replacement(self):
        """Confirm the media replacement with selected parameters"""
        try:
            # Validate source paths
            audio_path = self.source_audio_path
            video_path = self.source_video_path
            
            if self.track_mode.get() == 2:
                if audio_path:
                    audio_path = self.workflow.ffmpeg_audio_processor.audio_change(audio_path, 1.0, 0.0, 1.0, 0.0)
                if video_path:
                    video_path = self.workflow.ffmpeg_processor.fade_video(video_path, 1.0, 0.0)
            elif self.track_mode.get() == 3:
                if audio_path:
                    audio_path = self.workflow.ffmpeg_audio_processor.audio_change(audio_path, 0.0, 1.0, 1.0, 0.0)
                if video_path:
                    video_path = self.workflow.ffmpeg_processor.fade_video(video_path, 0.0, 1.0)
            elif self.track_mode.get() == 4:
                if audio_path:
                    audio_path = self.workflow.ffmpeg_audio_processor.audio_change(audio_path, 1.0, 1.0, 1.0, 0.0)
                if video_path:
                    video_path = self.workflow.ffmpeg_processor.fade_video(video_path, 1.0, 1.0)

            v = self.current_scenario.get(self.video_field, None)
            a = self.current_scenario.get(self.audio_field, None)
            i = self.current_scenario.get(self.image_field, None)

            if v != video_path:
                self.parent.workflow.refresh_scenario_media(self.current_scenario, self.video_field, ".mp4", video_path, True)
            if a != audio_path:
                self.parent.workflow.refresh_scenario_media(self.current_scenario, self.audio_field, ".wav", audio_path, True)
            if i != self.source_image_path:
                self.parent.workflow.refresh_scenario_media(self.current_scenario, self.image_field, ".webp", self.source_image_path, True)

            self.result = {
                'audio_json': self.audio_json,
                'transcribe_way': self.transcribe_way
            }
            
            self.close_dialog()
            
        except Exception as e:
            messagebox.showerror("é”™è¯¯", f"å‚æ•°éªŒè¯å¤±è´¥: {str(e)}")
    

    def cancel(self):
        """Cancel the operation"""
        self.result = {'confirmed': False}
        self.close_dialog()
    

    def close_dialog(self):
        """Close the dialog and cleanup"""
        # Stop recording if in progress
        if self.recording:
            self.recording = False
            if hasattr(self, 'recording_dialog') and self.recording_dialog.winfo_exists():
                self.recording_dialog.destroy()
        
        # Stop all playback and reset states
        self.av_playing = False
        self.av_playing = False
        self.av_paused = False
        self.playback_start_time = None
        self.pause_accumulated_time = 0.0
        # Stop audio
        if self.source_audio_path:
            pygame.mixer.music.stop()
        
        # Cleanup video resources
        if self.source_video_path:
            # Cancel video frame updates
            if self.video_after_id:
                self.dialog.after_cancel(self.video_after_id)
            
            # Release video capture
            if self.video_cap:
                self.video_cap.release()
            
        self.image_references.clear()
        
        # Close dialog
        self.dialog.destroy()
    

    def jump_to_start(self):
        """Jump to the start of selected time range"""
        try:
            start_time = self.start_time_var.get()
            
            if self.source_video_path:
                # Video mode: jump video and stop audio
                if self.video_cap:
                    fps = self.video_cap.get(cv2.CAP_PROP_FPS)
                    start_frame = int(start_time * fps) if fps > 0 else 0
                    self.video_cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
                    
            # Stop video audio and reset state
            if self.source_audio_path:
                if self.pygame_initialized:
                    pygame.mixer.music.stop()
                
            # Reset time tracking
            self.pause_accumulated_time = start_time
            self.playback_start_time = None
            self.av_paused = False
            
            self.current_playback_time = start_time
            print(f"âœ“ è·³è½¬åˆ°å¼€å§‹ä½ç½®: {start_time:.2f}ç§’")
            
            # Update play time display
            self.update_play_time_display()
            
        except Exception as e:
            print(f"âš ï¸ è·³è½¬å¤±è´¥: {e}")
   
    
    def play_selected_range(self):
        """Play only the selected time range"""
        try:
            # Jump to start first
            self.jump_to_start()
            
            # Start playback based on mode
            if self.source_video_path:
                if not self.av_playing:
                    self.start_video_playback()
            else:
                if not self.av_playing:
                    self.start_playback()
            
            print(f"â–¶ å¼€å§‹æ’­æ”¾é€‰å®šèŒƒå›´")
            
        except Exception as e:
            print(f"âš ï¸ æ’­æ”¾é€‰å®šèŒƒå›´å¤±è´¥: {e}")


    def start_time_update_thread(self):
        """Start a thread to update playback time"""
        def update_time():
            while self.av_playing and not self.av_paused:
                try:
                    if self.playback_start_time is not None:
                        elapsed_since_start = time.time() - self.playback_start_time
                        self.current_playback_time = self.pause_accumulated_time + elapsed_since_start
                        
                        # Update display in main thread
                        self.dialog.after(0, self.update_play_time_display)
                        
                        # Check if we've reached the end of audio
                        if self.current_playback_time >= self.audio_duration:
                            self.dialog.after(0, self.stop_playback)
                            break
                    
                    time.sleep(0.1)  # Update every 100ms
                except:
                    break
        
        # Start the update thread
        if self.av_playing:
            threading.Thread(target=update_time, daemon=True).start()

    def get_fresh_json_from_editor(self):
        """Get JSON data from fresh JSON editor"""
        import json
        try:
            content = self.fresh_json_text.get(1.0, tk.END).strip()
            if content:
                return json.loads(content)
            return None
        except json.JSONDecodeError as e:
            messagebox.showerror("é”™è¯¯", f"Fresh JSONæ ¼å¼æ— æ•ˆ: {str(e)}")
            return None


    def get_audio_json_from_editor(self):
        """Get JSON data from audio JSON editor"""
        import json
        try:
            content = self.audio_json_text.get(1.0, tk.END).strip()
            if content:
                return json.loads(content)
            return None
        except json.JSONDecodeError as e:
            messagebox.showerror("é”™è¯¯", f"Audio JSONæ ¼å¼æ— æ•ˆ: {str(e)}")
            return None
        
    
    def load_video_first_frame(self):
        """Load and display the first frame of the video in preview canvas"""
        if not self.source_video_path:
            return
            
        try:
            # Clear canvas first
            self.preview_canvas.delete("all")
            
            # Open video file
            cap = cv2.VideoCapture(self.source_video_path)
            
            if not cap.isOpened():
                cap.release()
                self.preview_canvas.create_text(
                    self.preview_canvas.winfo_width()//2, 
                    self.preview_canvas.winfo_height()//2,
                    text="æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶", fill="white", font=("Arial", 12)
                )
                return
            
            # Get video dimensions
            self.video_original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            self.video_original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            # Update crop controls max values
            if self.video_original_width and self.video_original_height:
                # Update spinbox max values
                self._update_crop_spinbox_max()
            
            # Read first frame
            ret, frame = cap.read()
            cap.release()
            
            if ret and frame is not None:
                # Convert BGR to RGB
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # Get canvas dimensions
                canvas_width = self.preview_canvas.winfo_width()
                canvas_height = self.preview_canvas.winfo_height()
                
                # If canvas is not ready, use default size
                if canvas_width <= 1 or canvas_height <= 1:
                    canvas_width, canvas_height = 640, 360
                
                # Calculate aspect ratio and resize
                height, width = frame_rgb.shape[:2]
                aspect_ratio = width / height
                
                if canvas_width / canvas_height > aspect_ratio:
                    new_height = canvas_height - 20  # Leave some margin
                    new_width = int(new_height * aspect_ratio)
                else:
                    new_width = canvas_width - 20  # Leave some margin
                    new_height = int(new_width / aspect_ratio)
                
                # Resize frame
                frame_resized = cv2.resize(frame_rgb, (new_width, new_height))
                
                # Convert to PIL Image and then to PhotoImage
                pil_image = Image.fromarray(frame_resized)
                self.first_frame_photo = ImageTk.PhotoImage(pil_image)
                
                # Add to image references to prevent garbage collection
                if not hasattr(self, 'image_references'):
                    self.image_references = []
                self.image_references.append(self.first_frame_photo)
                
                # Display the image in canvas center
                self.preview_canvas.create_image(
                    canvas_width//2, canvas_height//2, 
                    image=self.first_frame_photo, anchor=tk.CENTER
                )
                
                # Add helpful text below the frame
                self.preview_canvas.create_text(
                    canvas_width//2, canvas_height//2 + new_height//2 + 20,
                    text="ç‚¹å‡» 'â–¶ æ’­æ”¾' å¼€å§‹æ’­æ”¾è§†é¢‘", 
                    fill="white", font=("Arial", 12)
                )
                
                self.preview_canvas.create_text(
                    canvas_width//2, canvas_height//2 + new_height//2 + 40,
                    text="ğŸ’¡ è§†é¢‘ç¬¬ä¸€å¸§é¢„è§ˆ", 
                    fill="gray", font=("Arial", 10)
                )
                
            else:
                self.preview_canvas.create_text(
                    self.preview_canvas.winfo_width()//2, 
                    self.preview_canvas.winfo_height()//2,
                    text="æ— æ³•è¯»å–è§†é¢‘ç¬¬ä¸€å¸§", fill="white", font=("Arial", 12)
                )
                
        except Exception as e:
            print(f"âš ï¸ åŠ è½½è§†é¢‘ç¬¬ä¸€å¸§å¤±è´¥: {e}")
            self.preview_canvas.create_text(
                self.preview_canvas.winfo_width()//2, 
                self.preview_canvas.winfo_height()//2,
                text=f"åŠ è½½è§†é¢‘å¤±è´¥: {str(e)}", fill="red", font=("Arial", 10)
            )


    def update_video_frame(self):
        """Update video frame in preview canvas with audio sync"""
        if not self.av_playing or not self.video_cap:
            return
        
        try:
            # Calculate target time based on actual elapsed time (audio sync)
            if self.playback_start_time is not None:
                elapsed_since_start = time.time() - self.playback_start_time
                target_time = self.pause_accumulated_time + elapsed_since_start
            else:
                target_time = self.current_playback_time
            
            fps = self.video_cap.get(cv2.CAP_PROP_FPS) or 30  # Default to 30 if FPS is unknown
            target_frame = int(target_time * fps)
            current_frame = int(self.video_cap.get(cv2.CAP_PROP_POS_FRAMES))
            
            # Calculate frame difference and skip frames if needed
            frame_diff = target_frame - current_frame
            
            if frame_diff > 1:
                # Video is behind audio, skip frames to catch up
                print(f"ğŸ¬ è§†é¢‘åŒæ­¥: è·³è¿‡ {frame_diff - 1} å¸§ (ç›®æ ‡å¸§: {target_frame}, å½“å‰å¸§: {current_frame})")
                self.video_cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)
                ret, frame = self.video_cap.read()
            elif frame_diff < -1:
                # Video is ahead of audio (unlikely but handle it)
                print(f"ğŸ¬ è§†é¢‘åŒæ­¥: è§†é¢‘è¶…å‰éŸ³é¢‘ {abs(frame_diff)} å¸§")
                self.video_cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)
                ret, frame = self.video_cap.read()
            else:
                # Normal playback - read next frame
                ret, frame = self.video_cap.read()
            
            if ret:
                # Update current playback time based on actual video position
                current_frame = self.video_cap.get(cv2.CAP_PROP_POS_FRAMES)
                self.current_playback_time = current_frame / fps if fps > 0 else target_time
                
                # Update play time display
                self.update_play_time_display()
                
                # Check if we're still in the selected time range
                try:
                    end_time = self.end_time_var.get()
                    if self.current_playback_time >= end_time:
                        # Reached end of selected range
                        self.stop_playback()
                        return
                except:
                    pass
                
                # Convert frame to display in canvas
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                height, width = frame.shape[:2]
                
                # Resize frame to fit canvas
                canvas_width = self.preview_canvas.winfo_width()
                canvas_height = self.preview_canvas.winfo_height()
                if canvas_width > 1 and canvas_height > 1:
                    aspect_ratio = width / height
                    if canvas_width / canvas_height > aspect_ratio:
                        new_height = canvas_height
                        new_width = int(canvas_height * aspect_ratio)
                    else:
                        new_width = canvas_width
                        new_height = int(canvas_width / aspect_ratio)
                    
                    frame = cv2.resize(frame, (new_width, new_height))
                    
                    # Convert to PhotoImage and display
                    image = Image.fromarray(frame)
                    photo = ImageTk.PhotoImage(image)
                    
                    # Manage image references to prevent garbage collection
                    self.image_references.append(photo)
                    # Keep only the last few images to avoid memory buildup
                    if len(self.image_references) > 5:
                        self.image_references.pop(0)
                    
                    self.preview_canvas.delete("all")
                    self.preview_canvas.create_image(canvas_width//2, canvas_height//2, 
                                                   image=photo, anchor=tk.CENTER)
                
                # Calculate dynamic delay for next frame update
                # Use shorter delay for better sync accuracy
                next_frame_time = (current_frame + 1) / fps
                time_until_next_frame = next_frame_time - target_time
                
                if time_until_next_frame > 0:
                    delay = max(int(time_until_next_frame * 1000), 16)  # Minimum 16ms (~60 FPS max)
                else:
                    delay = 16  # Immediate update if we're behind
                
                self.video_after_id = self.dialog.after(delay, self.update_video_frame)
            else:
                # End of video
                self.stop_playback()
        except Exception as e:
            print(f"Video playback error: {e}")
            self.stop_playback()


    def display_image_on_canvas(self):
        """åœ¨canvasä¸Šæ˜¾ç¤ºå›¾ç‰‡"""
        try:
            img = Image.open(self.source_image_path)
            
            # æ¸…ç©º canvas
            self.image_canvas.delete("all")
            
            # è·å– canvas çš„å®é™…å¤§å°
            canvas_width = self.image_canvas.winfo_width()
            canvas_height = self.image_canvas.winfo_height()
            
            # å¦‚æœ canvas è¿˜æ²¡æœ‰å‡†å¤‡å¥½ï¼Œä½¿ç”¨é»˜è®¤å¤§å°
            if canvas_width <= 1 or canvas_height <= 1:
                canvas_width, canvas_height = 600, 600
            
            # è®¡ç®—å®½é«˜æ¯”å¹¶è°ƒæ•´å¤§å°ä»¥å¡«æ»¡ canvas
            img_width, img_height = img.size
            aspect_ratio = img_width / img_height
            
            # è®¡ç®—æ–°çš„å°ºå¯¸ï¼Œç•™ä¸€äº›è¾¹è·
            margin = 20
            available_width = canvas_width - margin
            available_height = canvas_height - margin
            
            if available_width / available_height > aspect_ratio:
                # canvas æ›´å®½ï¼Œä»¥é«˜åº¦ä¸ºåŸºå‡†
                new_height = available_height
                new_width = int(new_height * aspect_ratio)
            else:
                # canvas æ›´é«˜ï¼Œä»¥å®½åº¦ä¸ºåŸºå‡†
                new_width = available_width
                new_height = int(new_width / aspect_ratio)
            
            # è°ƒæ•´å›¾ç‰‡å¤§å°
            img_resized = img.resize((new_width, new_height), Image.LANCZOS)
            photo = ImageTk.PhotoImage(img_resized)
            
            # åœ¨ canvas ä¸­å¿ƒæ˜¾ç¤ºå›¾ç‰‡
            x = canvas_width // 2
            y = canvas_height // 2
            self.image_canvas.create_image(x, y, image=photo, anchor=tk.CENTER, tags="image")
            
            # ä¿æŒå¼•ç”¨ä»¥é˜²æ­¢è¢«åƒåœ¾å›æ”¶
            self.image_canvas.image = photo
            
        except Exception as e:
            print(f"æ˜¾ç¤ºå›¾ç‰‡å¤±è´¥: {e}")


    def on_animation_changed(self, *args):
        """å¤„ç†åŠ¨ç”»é€‰é¡¹å˜åŒ–"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡å’ŒéŸ³é¢‘
            if not self.source_image_path or not os.path.exists(self.source_image_path):
                print("âš ï¸ æ²¡æœ‰å›¾ç‰‡ï¼Œæ— æ³•é‡æ–°ç”Ÿæˆè§†é¢‘")
                return
            
            if not self.source_audio_path or not os.path.exists(self.source_audio_path):
                print("âš ï¸ æ²¡æœ‰éŸ³é¢‘ï¼Œæ— æ³•é‡æ–°ç”Ÿæˆè§†é¢‘")
                return
            
            # è·å–æ–°çš„åŠ¨ç”»é€‰æ‹©
            animation_choice = self.animation_var.get()
            self.animation_choice = animation_choice
            
            print(f"ğŸ¬ åŠ¨ç”»é€‰é¡¹å˜åŒ–: {animation_choice} ({'é™æ­¢' if animation_choice == 1 else 'å·¦' if animation_choice == 2 else 'å³' if animation_choice == 3 else 'åŠ¨ç”»'})")
            
            # åœæ­¢å½“å‰æ’­æ”¾
            if self.av_playing:
                self.stop_playback()
            
            # é‡Šæ”¾æ—§çš„è§†é¢‘æ•è·å¯¹è±¡
            if self.video_cap:
                self.video_cap.release()
                self.video_cap = None
                print("ğŸ”„ å·²é‡Šæ”¾æ—§è§†é¢‘èµ„æº")
            
            # é‡æ–°ç”Ÿæˆè§†é¢‘
            print(f"ğŸ”„ æ­£åœ¨é‡æ–°ç”Ÿæˆè§†é¢‘...")
            self.source_video_path = self.workflow.ffmpeg_processor.image_audio_to_video(
                self.source_image_path, 
                self.source_audio_path, 
                animation_choice
            )
            
            if self.source_video_path and os.path.exists(self.source_video_path):
                print(f"âœ“ è§†é¢‘é‡æ–°ç”ŸæˆæˆåŠŸ: {self.source_video_path}")
                
                # é‡ç½®æ’­æ”¾çŠ¶æ€
                self.current_playback_time = 0.0
                self.pause_accumulated_time = 0.0
                self.playback_start_time = None
                
                # åˆ·æ–°è§†é¢‘æ˜¾ç¤º
                self.dialog.after(100, self.load_video_first_frame)
            else:
                print(f"âŒ è§†é¢‘é‡æ–°ç”Ÿæˆå¤±è´¥")
                
        except Exception as e:
            print(f"âŒ å¤„ç†åŠ¨ç”»å˜åŒ–å¤±è´¥: {e}")
    

    def on_video_dnd_drop(self, event):
        """å¤„ç†è§†é¢‘æ‹–æ”¾"""
        file_path = event.data.strip('{}').strip('"')
        if is_video_file(file_path):
            self.handle_new_media(file_path)
            self.update_dialog_title("none")


    def on_image_dnd_drop(self, event):
        """å¤„ç†å›¾ç‰‡æ‹–æ”¾"""
        file_path = event.data.strip('{}').strip('"')
        if is_image_file(file_path):
            self.handle_new_media(file_path)
            self.update_dialog_title("none")

    
    def on_audio_dnd_drop(self, event):
        """å¤„ç†éŸ³é¢‘æ‹–æ”¾"""
        file_path = event.data.strip('{}').strip('"')
        if is_audio_file(file_path):
            self.handle_new_media(file_path)
            self.update_dialog_title("none")


    def handle_new_media(self, av_path):
        if not av_path:
            return

        # åœæ­¢å½“å‰æ’­æ”¾
        if self.av_playing:
            self.stop_playback()
        
        # é‡Šæ”¾æ—§çš„è§†é¢‘æ•è·å¯¹è±¡
        if self.video_cap:
            self.video_cap.release()
            self.video_cap = None

        # æ¸…é™¤è£å‰ªé€‰æ‹©
        self.clear_crop_selection()
        
        # é‡ç½®è§†é¢‘å°ºå¯¸
        self.video_original_width = None
        self.video_original_height = None

        self.start_time = 0.0

        if is_audio_file(av_path):
            self.source_audio_path = self.workflow.ffmpeg_audio_processor.audio_change(av_path)
            self.source_video_path = self.workflow.ffmpeg_processor.add_audio_to_video(self.source_video_path, self.source_audio_path)
        elif is_image_file(av_path):
            self.source_image_path = av_path
            self.source_video_path = self.workflow.ffmpeg_processor.image_audio_to_video(self.source_image_path, self.source_audio_path, self.animation_choice)
        elif is_video_file(av_path):
            if self.workflow.ffmpeg_processor.has_audio_stream(av_path) and self.replace_media_audio=="keep":
                self.source_video_path = self.workflow.ffmpeg_processor.resize_video(av_path, self.workflow.ffmpeg_processor.width)
                self.source_audio_path = self.workflow.ffmpeg_audio_processor.extract_audio_from_video(av_path)
            else:
                self.source_video_path = self.workflow.ffmpeg_processor.add_audio_to_video(av_path, self.source_audio_path, True, True)
                self.source_video_path = self.workflow.ffmpeg_processor.resize_video(self.source_video_path, self.workflow.ffmpeg_processor.width)

        self.audio_duration = self.workflow.ffmpeg_audio_processor.get_duration(self.source_audio_path)
        self.end_time = self.audio_duration

        # é‡ç½®æ’­æ”¾çŠ¶æ€
        self.current_playback_time = 0.0
        self.pause_accumulated_time = 0.0
        self.playback_start_time = None

        self.display_image_on_canvas()

        # åˆ·æ–°è§†é¢‘æ˜¾ç¤ºï¼ˆå»¶è¿ŸåŠ è½½ç¡®ä¿è§†é¢‘èµ„æºå‡†å¤‡å°±ç»ªï¼‰
        self.dialog.after(100, self.load_video_first_frame)
    
    
    def on_canvas_click(self, event):
        """Handle mouse click on preview canvas to start crop selection"""
        if not self.source_video_path:
            return
        
        # Get canvas coordinates
        canvas_x = event.x
        canvas_y = event.y
        
        # Convert canvas coordinates to video coordinates
        video_x, video_y = self.canvas_to_video_coords(canvas_x, canvas_y)
        
        if video_x is not None and video_y is not None:
            self.selecting = True
            self.selection_start_x = canvas_x
            self.selection_start_y = canvas_y
            
            # Clear previous selection
            if self.selection_rect:
                self.preview_canvas.delete(self.selection_rect)
            self.selection_rect = None
    
    
    def on_canvas_drag(self, event):
        """Handle mouse drag on preview canvas to update crop selection"""
        if not self.selecting:
            return
        
        # Get current canvas coordinates
        canvas_x = event.x
        canvas_y = event.y
        
        # Update selection rectangle
        if self.selection_rect:
            self.preview_canvas.delete(self.selection_rect)
        
        # Draw selection rectangle
        x1 = min(self.selection_start_x, canvas_x)
        y1 = min(self.selection_start_y, canvas_y)
        x2 = max(self.selection_start_x, canvas_x)
        y2 = max(self.selection_start_y, canvas_y)
        
        self.selection_rect = self.preview_canvas.create_rectangle(
            x1, y1, x2, y2,
            outline='yellow', width=2, dash=(5, 5)
        )
    
    
    def on_canvas_release(self, event):
        """Handle mouse release on preview canvas to finalize crop selection"""
        if not self.selecting:
            return
        
        self.selecting = False
        
        # Get canvas coordinates
        canvas_x = event.x
        canvas_y = event.y
        
        # Convert to video coordinates
        start_x, start_y = self.canvas_to_video_coords(
            min(self.selection_start_x, canvas_x),
            min(self.selection_start_y, canvas_y)
        )
        end_x, end_y = self.canvas_to_video_coords(
            max(self.selection_start_x, canvas_x),
            max(self.selection_start_y, canvas_y)
        )

        if start_x > end_x:
            start_x, end_x = end_x, start_x
        if start_y > end_y:
            start_y, end_y = end_y, start_y
        
        if start_x is not None and start_y is not None and end_x is not None and end_y is not None:
            # Update crop parameters
            self.crop_start_x = max(0, int(start_x))
            self.crop_start_y = max(0, int(start_y))
            crop_w = max(1, int(end_x - start_x))
            crop_h = max(1, int(end_y - start_y))
            
            # Store crop dimensions
            self.crop_width = crop_w
            self.crop_height = crop_h
            
            # Update UI controls
            self.crop_x_var.set(self.crop_start_x)
            self.crop_y_var.set(self.crop_start_y)
            self.crop_width_var.set(crop_w)
            
            print(f"âœ“ é€‰æ‹©è£å‰ªåŒºåŸŸ: ({self.crop_start_x}, {self.crop_start_y}), å°ºå¯¸: {crop_w}x{crop_h}")
    
    
    def canvas_to_video_coords(self, canvas_x, canvas_y):
        """Convert canvas coordinates to video coordinates"""
        if not self.source_video_path or not self.video_original_width or not self.video_original_height:
            return None, None
        
        try:
            # Get canvas dimensions
            canvas_width = self.preview_canvas.winfo_width()
            canvas_height = self.preview_canvas.winfo_height()
            
            if canvas_width <= 1 or canvas_height <= 1:
                return None, None
            
            # Get displayed image dimensions (from first frame)
            if not hasattr(self, 'first_frame_photo') or not self.first_frame_photo:
                return None, None
            
            # Find the image item on canvas
            items = self.preview_canvas.find_all()
            image_item = None
            for item in items:
                if self.preview_canvas.type(item) == 'image':
                    image_item = item
                    break
            
            if not image_item:
                return None, None
            
            # Get image coordinates and dimensions
            coords = self.preview_canvas.coords(image_item)
            img_x = coords[0]
            img_y = coords[1]
            
            # Get image dimensions from photo
            img_width = self.first_frame_photo.width()
            img_height = self.first_frame_photo.height()
            
            # Calculate image bounds
            img_left = img_x - img_width // 2
            img_right = img_x + img_width // 2
            img_top = img_y - img_height // 2
            img_bottom = img_y + img_height // 2
            
            # Check if click is within image bounds
            if canvas_x < img_left or canvas_x > img_right or canvas_y < img_top or canvas_y > img_bottom:
                return None, None
            
            # Convert to relative coordinates (0.0 to 1.0)
            rel_x = (canvas_x - img_left) / img_width
            rel_y = (canvas_y - img_top) / img_height
            
            # Convert to video coordinates
            video_x = int(rel_x * self.video_original_width)
            video_y = int(rel_y * self.video_original_height)
            
            return video_x, video_y
            
        except Exception as e:
            print(f"âš ï¸ åæ ‡è½¬æ¢å¤±è´¥: {e}")
            return None, None
    
    
    def on_crop_params_changed(self, *args):
        """Handle changes to crop parameter controls"""
        try:
            self.crop_start_x = self.crop_x_var.get()
            self.crop_start_y = self.crop_y_var.get()
            
            width_val = self.crop_width_var.get()
            if width_val == 0:
                self.crop_width = None
            else:
                self.crop_width = width_val
                # Calculate height based on aspect ratio if not set
                if self.crop_width and self.video_original_width and self.video_original_height:
                    aspect_ratio = self.video_original_height / self.video_original_width
                    self.crop_height = int(self.crop_width * aspect_ratio)
            
            # Update selection rectangle display
            self.update_crop_selection_display()
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°è£å‰ªå‚æ•°å¤±è´¥: {e}")
    
    
    def update_crop_selection_display(self):
        """Update the visual selection rectangle on canvas"""
        if not self.source_video_path or self.crop_width is None or self.crop_width == 0:
            # Clear selection if no crop is set
            if self.selection_rect:
                self.preview_canvas.delete(self.selection_rect)
                self.selection_rect = None
            return
        
        try:
            # Clear previous selection
            if self.selection_rect:
                self.preview_canvas.delete(self.selection_rect)
            
            # Calculate crop height if not set
            crop_h = self.crop_height
            if not crop_h and self.video_original_width and self.video_original_height:
                # Calculate height based on aspect ratio
                aspect_ratio = self.video_original_height / self.video_original_width
                crop_h = int(self.crop_width * aspect_ratio)
            elif not crop_h:
                crop_h = self.crop_width  # Fallback to square
            
            # Convert video coordinates to canvas coordinates
            canvas_coords = self.video_to_canvas_coords(
                self.crop_start_x, self.crop_start_y,
                self.crop_start_x + self.crop_width,
                self.crop_start_y + crop_h
            )
            
            if canvas_coords:
                x1, y1, x2, y2 = canvas_coords
                self.selection_rect = self.preview_canvas.create_rectangle(
                    x1, y1, x2, y2,
                    outline='yellow', width=2, dash=(5, 5)
                )
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°é€‰æ‹©æ˜¾ç¤ºå¤±è´¥: {e}")
    
    
    def video_to_canvas_coords(self, video_x1, video_y1, video_x2, video_y2):
        """Convert video coordinates to canvas coordinates"""
        if not self.source_video_path or not self.video_original_width or not self.video_original_height:
            return None
        
        try:
            # Get canvas dimensions
            canvas_width = self.preview_canvas.winfo_width()
            canvas_height = self.preview_canvas.winfo_height()
            
            if canvas_width <= 1 or canvas_height <= 1:
                return None
            
            # Get displayed image dimensions
            if not hasattr(self, 'first_frame_photo') or not self.first_frame_photo:
                return None
            
            # Find the image item on canvas
            items = self.preview_canvas.find_all()
            image_item = None
            for item in items:
                if self.preview_canvas.type(item) == 'image':
                    image_item = item
                    break
            
            if not image_item:
                return None
            
            # Get image coordinates
            coords = self.preview_canvas.coords(image_item)
            img_x = coords[0]
            img_y = coords[1]
            
            # Get image dimensions
            img_width = self.first_frame_photo.width()
            img_height = self.first_frame_photo.height()
            
            # Calculate image bounds
            img_left = img_x - img_width // 2
            img_top = img_y - img_height // 2
            
            # Convert to relative coordinates
            rel_x1 = video_x1 / self.video_original_width
            rel_y1 = video_y1 / self.video_original_height
            rel_x2 = video_x2 / self.video_original_width
            rel_y2 = video_y2 / self.video_original_height
            
            # Convert to canvas coordinates
            canvas_x1 = img_left + rel_x1 * img_width
            canvas_y1 = img_top + rel_y1 * img_height
            canvas_x2 = img_left + rel_x2 * img_width
            canvas_y2 = img_top + rel_y2 * img_height
            
            return (canvas_x1, canvas_y1, canvas_x2, canvas_y2)
            
        except Exception as e:
            print(f"âš ï¸ åæ ‡è½¬æ¢å¤±è´¥: {e}")
            return None
    
    
    def clear_crop_selection(self):
        """Clear the crop selection"""
        self.crop_start_x = 0
        self.crop_start_y = 0
        self.crop_width = None
        self.crop_height = None
        
        if self.selection_rect:
            self.preview_canvas.delete(self.selection_rect)
            self.selection_rect = None
        
        self.crop_x_var.set(0)
        self.crop_y_var.set(0)
        self.crop_width_var.set(0)
        
        print("âœ“ å·²æ¸…é™¤è£å‰ªé€‰æ‹©")
    
    
    def _update_crop_spinbox_max(self):
        """Update max values for crop spinboxes based on video dimensions"""
        if not self.video_original_width or not self.video_original_height:
            return
        
        try:
            # Find and update crop spinboxes
            def update_widget(widget):
                if isinstance(widget, ttk.Spinbox):
                    var = widget.cget('textvariable')
                    if var:
                        var_obj = self.dialog.nametowidget(var) if isinstance(var, str) else var
                        if var_obj == self.crop_x_var:
                            widget.configure(to=self.video_original_width)
                        elif var_obj == self.crop_y_var:
                            widget.configure(to=self.video_original_height)
                        elif hasattr(self, 'crop_width_var') and var_obj == self.crop_width_var:
                            widget.configure(to=self.video_original_width)
                elif isinstance(widget, (ttk.Frame, tk.Frame)):
                    for child in widget.winfo_children():
                        update_widget(child)
            
            # Update all widgets
            for widget in self.dialog.winfo_children():
                update_widget(widget)
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°è£å‰ªæ§ä»¶æœ€å¤§å€¼å¤±è´¥: {e}")
