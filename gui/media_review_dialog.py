import tkinter as tk
from tkinter import ttk, messagebox
import os, time, threading
from PIL import Image, ImageTk
from utility.file_util import get_file_path, safe_remove, safe_file
from utility.audio_transcriber import AudioTranscriber
from project_manager import refresh_scene_media
import config, config_channel
from utility.llm_api import LLMApi
import json
from config import parse_json_from_text
from utility.file_util import is_audio_file, is_video_file, is_image_file
import config_prompt


# å°è¯•å¯¼å…¥æ‹–æ”¾æ”¯æŒ
try:
    from tkinterdnd2 import DND_FILES
    DND_AVAILABLE = True
except ImportError:
    DND_AVAILABLE = False
    print("è­¦å‘Š: tkinterdnd2 ä¸å¯ç”¨ï¼Œæ‹–æ”¾åŠŸèƒ½å°†è¢«ç¦ç”¨")

# Audio recording imports
try:
    import sounddevice as sd
    import soundfile as sf
    import numpy as np
    RECORDING_AVAILABLE = True
except ImportError:
    RECORDING_AVAILABLE = False
    print("è­¦å‘Š: sounddevice æˆ– soundfile ä¸å¯ç”¨ï¼Œå½•éŸ³åŠŸèƒ½å°†è¢«ç¦ç”¨")

# Video playback imports
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    print("è­¦å‘Š: cv2 ä¸å¯ç”¨ï¼Œè§†é¢‘æ’­æ”¾åŠŸèƒ½å°†è¢«ç¦ç”¨")


# Audio playback imports
try:
    import pygame
    PYGAME_AVAILABLE = True
except ImportError:
    PYGAME_AVAILABLE = False
    print("è­¦å‘Š: pygame ä¸å¯ç”¨ï¼ŒéŸ³é¢‘æ’­æ”¾åŠŸèƒ½å°†è¢«ç¦ç”¨")


PROJECT_TYPE_STORY = "story"
PROJECT_TYPE_TALK = "talk"
PROJECT_TYPE_SONG = "song"
PROJECT_TYPE_MUSIC = "music"

PROJECT_TYPE_LIST = [
    PROJECT_TYPE_STORY,
    PROJECT_TYPE_SONG,
    PROJECT_TYPE_MUSIC,
    PROJECT_TYPE_TALK
]



class AVReviewDialog:
    """Dialog for reviewing and configuring audio replacement with drag-and-drop support"""
    
    def __init__(self, parent, video_path, current_scene, previous_scene, next_scene, media_type, replace_media_audio):
        self.parent = parent
        self.current_scene = current_scene
        self.previous_scene = previous_scene
        self.next_scene = next_scene
        self.workflow = parent.workflow
        # Get video dimensions from workflow's ffmpeg_processor
        video_width = self.workflow.ffmpeg_processor.width
        video_height = self.workflow.ffmpeg_processor.height
        self.transcriber = AudioTranscriber(self.workflow.pid, model_size="small", device="cuda")
        self.llm_api = LLMApi()

        self.media_type_names = {
            "clip": "åœºæ™¯åª’ä½“ (clip)",
            "narration": "æ—ç™½è½¨é“ (narration)",
            "zero": "èƒŒæ™¯è½¨é“ (zero)",
            "one": "ç¬¬ä¸€è½¨é“ (one)"
        }

        self.transcribe_way = "single"

        self.media_type = media_type
        self.replace_media_audio = replace_media_audio

        if self.media_type == "clip":
            self.SPEAKER_KEY = "speaker"
            self.SPEAKING_KEY = "speaking"
            self.ACTORS = config_prompt.SPEAKER
        else:
            self.SPEAKER_KEY = "narrator"
            self.SPEAKING_KEY = "voiceover"
            self.ACTORS = config_prompt.NARRATOR
        
        # åª’ä½“å­—æ®µåæ˜ å°„
        if media_type == "clip":
            self.video_field = "clip"
            self.audio_field = "clip_audio"
            self.image_field = "clip_image"
        elif media_type == "narration":
            self.video_field = "narration"
            self.audio_field = "narration_audio"
            self.image_field = "narration_image"
        elif media_type == "zero":
            self.video_field = "zero"
            self.audio_field = "zero_audio"
            self.image_field = "zero_image"
        elif media_type == "one":
            self.video_field = "one"
            self.audio_field = "one_audio"
            self.image_field = "one_image"

        self.source_video_path = get_file_path(self.current_scene, self.video_field)
        self.source_audio_path = get_file_path(self.current_scene, self.audio_field)
        self.source_image_path = get_file_path(self.current_scene, self.image_field)
        
        self.audio_duration = 0.0
        # ä½¿ç”¨ _pending_boundaries ç»Ÿä¸€ç®¡ç†æ—¶é—´è¾¹ç•Œï¼Œç§»é™¤é‡å¤çš„ start_time_var å’Œ end_time_var
        self._pending_boundaries = None
        self._boundaries_initialized = False

        # æ–°å¢æ‹–æ”¾åª’ä½“
        self.animation_choice = 1

        self.current_playback_time = 0.0
        self.av_playing = False
        self.av_paused = False
        self.playback_start_time = None  # Time when playback started
        self.pause_accumulated_time = 0.0  # Total time played before pausing

        self.result = None  # Will store the result when dialog is closed
        
        # Recording state variables
        self.recording = False
        self.recorded_audio = None
        self.recording_thread = None

        # Initialize video-specific states if in video mode
        self.av_playing = False
        self.video_cap = None
        self.video_after_id = None
        # Keep a list of image references to prevent garbage collection
        self.image_references = []
        
        # Crop selection variables
        self.crop_start_x = 0
        self.crop_start_y = 0
        self.crop_width = None  # None means use full width
        self.crop_height = None  # None means use full height
        self.selection_rect = None  # Canvas rectangle ID for selection
        self.selecting = False  # Whether user is currently selecting
        self.selection_start_x = 0
        self.selection_start_y = 0
        self.video_original_width = None
        self.video_original_height = None
        
        self.process_new_media(video_path)

        self.create_dialog()

        self.current_scene["start"] = 0.0
        self.current_scene["end"] = self.audio_duration
        if self.current_scene.get("caption", None):
            self.current_scene[self.SPEAKER_KEY+"_audio"] = self.source_audio_path
        self.audio_json = [ self.current_scene ]
        self._update_fresh_json_text()

        self.dialog.after(100, self.init_load)



    def init_load(self):
        self.draw_waveform_placeholder()
        self.display_image_on_canvas()
        self.load_video_first_frame()
        # åˆå§‹åŒ–æ—¶é—´è½´
        self._draw_progress_bar()
        self._draw_edit_timeline()



    def create_dialog(self):
        """Create the review dialog window"""
        self.dialog = tk.Toplevel(self.parent.root)
        self.dialog.geometry("1800x1000")
        self.dialog.resizable(True, True)
        self.dialog.transient(self.parent.root)
        self.dialog.grab_set()
        self.dialog.title( f"{self.media_type_names.get(self.media_type)} - {self.transcribe_way}" )

        # Main container
        main_frame = ttk.Frame(self.dialog)
        main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
        
        # Media info section
        info_frame = ttk.LabelFrame(main_frame, text="", padding=10)
        info_frame.pack(fill=tk.X, pady=(0, 10))
        
        # Media info row
        info_row = ttk.Frame(info_frame)
        info_row.pack(fill=tk.X, pady=2)
        
        ttk.Label(info_row, text=f"è§†é¢‘æ—¶é•¿: { (self.audio_duration):.2f}ç§’").pack(side=tk.LEFT)
        ttk.Separator(info_row, orient=tk.VERTICAL).pack(side=tk.LEFT, fill=tk.Y, padx=20)
        if self.source_video_path:
            if self.workflow.ffmpeg_processor.has_audio_stream(self.source_video_path):
                audio_status = "æœ‰éŸ³é¢‘"
                audio_color = "green"
            else:
                audio_status = "æ— éŸ³é¢‘"
                audio_color = "red"
            audio_label = ttk.Label(info_row, text=f"éŸ³é¢‘çŠ¶æ€: {audio_status}", foreground=audio_color)
            audio_label.pack(side=tk.LEFT)
            ttk.Separator(info_row, orient=tk.VERTICAL).pack(side=tk.LEFT, fill=tk.Y, padx=20)

        av_path = self.source_video_path if self.source_video_path else self.source_audio_path
        ttk.Label(info_row, text=f"æºåª’ä½“: {av_path}").pack(side=tk.LEFT)
        
        # Media visualization section - ä¸‰æ å¸ƒå±€ï¼šè§†é¢‘ | å›¾ç‰‡ | éŸ³é¢‘
        media_container = ttk.Frame(main_frame)
        media_container.pack(fill=tk.BOTH, expand=True, pady=(0, 10))
        
        # å·¦æ ï¼šè§†é¢‘é¢„è§ˆ + æ‹–æ”¾
        video_frame = ttk.LabelFrame(media_container, text="è§†é¢‘é¢„è§ˆ (å¯æ‹–æ”¾è§†é¢‘æ–‡ä»¶)", padding=10)
        video_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(0, 5))
        
        # Video preview canvas (æ”¯æŒæ‹–æ”¾)
        self.preview_canvas = tk.Canvas(video_frame, bg='black', height=300, highlightthickness=2, highlightbackground='blue')
        self.preview_canvas.pack(fill=tk.BOTH, expand=True)
        if DND_AVAILABLE:
            self.preview_canvas.drop_target_register(DND_FILES)
            self.preview_canvas.dnd_bind('<<Drop>>', self.on_video_dnd_drop)
        
        # Bind mouse events for crop selection
        self.preview_canvas.bind("<Button-1>", self.on_canvas_click)
        self.preview_canvas.bind("<B1-Motion>", self.on_canvas_drag)
        self.preview_canvas.bind("<ButtonRelease-1>", self.on_canvas_release)
        
        # Add crop controls below video preview
        crop_control_frame = ttk.Frame(video_frame)
        crop_control_frame.pack(fill=tk.X, pady=(5, 0))
        
        ttk.Label(crop_control_frame, text="è£å‰ªåŒºåŸŸ:").pack(side=tk.LEFT, padx=(0, 5))
        ttk.Label(crop_control_frame, text="X:").pack(side=tk.LEFT, padx=(0, 2))
        self.crop_x_var = tk.IntVar(value=0)
        ttk.Spinbox(crop_control_frame, from_=0, to=9999, textvariable=self.crop_x_var, width=8, command=self.on_crop_params_changed).pack(side=tk.LEFT, padx=(0, 5))
        
        ttk.Label(crop_control_frame, text="Y:").pack(side=tk.LEFT, padx=(0, 2))
        self.crop_y_var = tk.IntVar(value=0)
        ttk.Spinbox(crop_control_frame, from_=0, to=9999, textvariable=self.crop_y_var, width=8, command=self.on_crop_params_changed).pack(side=tk.LEFT, padx=(0, 5))
        
        ttk.Label(crop_control_frame, text="å®½åº¦:").pack(side=tk.LEFT, padx=(0, 2))
        self.crop_width_var = tk.IntVar(value=0)  # 0 means auto
        crop_width_spinbox = ttk.Spinbox(crop_control_frame, from_=0, to=9999, textvariable=self.crop_width_var, width=8, command=self.on_crop_params_changed)
        crop_width_spinbox.pack(side=tk.LEFT, padx=(0, 5))
        ttk.Label(crop_control_frame, text="(0=è‡ªåŠ¨)").pack(side=tk.LEFT, padx=(0, 5))
        
        ttk.Button(crop_control_frame, text="æ¸…é™¤é€‰æ‹©", command=self.clear_crop_selection).pack(side=tk.LEFT, padx=(5, 0))
        
        # ä¸­æ ï¼šå›¾ç‰‡æ˜¾ç¤º + æ‹–æ”¾
        image_frame = ttk.LabelFrame(media_container, text="å›¾ç‰‡ (å¯æ‹–æ”¾å›¾ç‰‡æ–‡ä»¶)", padding=10)
        image_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5)
        
        # Image canvas (æ”¯æŒæ‹–æ”¾)
        self.image_canvas = tk.Canvas(image_frame, bg='gray20', height=300, highlightthickness=2, highlightbackground='green')
        self.image_canvas.pack(fill=tk.BOTH, expand=True)
        if DND_AVAILABLE:
            self.image_canvas.drop_target_register(DND_FILES)
            self.image_canvas.dnd_bind('<<Drop>>', self.on_image_dnd_drop)
        
        # åŠ¨ç”»é€‰æ‹©
        anim_frame = ttk.Frame(image_frame)
        anim_frame.pack(fill=tk.X, pady=2)
        ttk.Label(anim_frame, text="åŠ¨ç”»:").pack(side=tk.LEFT, padx=2)
        
        self.animation_var = tk.IntVar(value=4)
        for value, text in [(1, "é™æ­¢"), (2, "å·¦"), (3, "å³"), (4, "åŠ¨ç”»")]:
            ttk.Radiobutton(anim_frame, text=text, variable=self.animation_var, value=value).pack(side=tk.LEFT, padx=2)

        # ç»‘å®š animation_var å˜åŒ–äº‹ä»¶
        self.animation_var.trace('w', self.on_animation_changed)
        
        # å³æ ï¼šéŸ³é¢‘æ³¢å½¢ + æ‹–æ”¾
        waveform_frame = ttk.LabelFrame(media_container, text="éŸ³é¢‘æ³¢å½¢ (å¯æ‹–æ”¾éŸ³é¢‘æ–‡ä»¶)", padding=10)
        waveform_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(5, 0))
        
        # Waveform canvas (æ”¯æŒæ‹–æ”¾)
        self.waveform_canvas = tk.Canvas(waveform_frame, bg='black', height=300, highlightthickness=2, highlightbackground='orange')
        self.waveform_canvas.pack(fill=tk.BOTH, expand=True)
        if DND_AVAILABLE:
            self.waveform_canvas.drop_target_register(DND_FILES)
            self.waveform_canvas.dnd_bind('<<Drop>>', self.on_audio_dnd_drop)
        
        # ========== æ—¶é—´è½´åŒºåŸŸ (Timeline Section) ==========
        timeline_container = ttk.LabelFrame(main_frame, text="æ—¶é—´è½´æ§åˆ¶", padding=5)
        timeline_container.pack(fill=tk.X, pady=(5, 5))
        
        # --- Progress Bar (æ’­æ”¾è¿›åº¦æ¡) ---
        progress_frame = ttk.Frame(timeline_container)
        progress_frame.pack(fill=tk.X, pady=(0, 5))
        
        ttk.Label(progress_frame, text="æ’­æ”¾ä½ç½®:", width=10).pack(side=tk.LEFT, padx=(0, 5))
        
        # Progress bar canvas for playback position
        self.progress_canvas = tk.Canvas(progress_frame, height=30, bg='#2d2d2d', highlightthickness=1, highlightbackground='#555')
        self.progress_canvas.pack(side=tk.LEFT, fill=tk.X, expand=True, padx=(0, 10))
        
        # Progress bar å†…éƒ¨å…ƒç´ ï¼ˆç¨ååˆå§‹åŒ–ï¼‰
        self.progress_bar_bg = None
        self.progress_playhead = None
        self.progress_dragging = False
        
        # ç»‘å®š Progress Bar äº‹ä»¶
        self.progress_canvas.bind('<Configure>', self._on_progress_canvas_configure)
        self.progress_canvas.bind('<Button-1>', self._on_progress_click)
        self.progress_canvas.bind('<B1-Motion>', self._on_progress_drag)
        self.progress_canvas.bind('<ButtonRelease-1>', self._on_progress_release)
        
        # å½“å‰æ’­æ”¾æ—¶é—´æ˜¾ç¤º
        self.progress_time_label = ttk.Label(progress_frame, text="0.00", width=8)
        self.progress_time_label.pack(side=tk.LEFT)
        
        # --- Edit Timeline (å‰ªè¾‘æ—¶é—´è½´) ---
        edit_timeline_frame = ttk.Frame(timeline_container)
        edit_timeline_frame.pack(fill=tk.X, pady=(5, 0))
        
        ttk.Label(edit_timeline_frame, text="å‰ªè¾‘åŒºé—´:", width=10).pack(side=tk.LEFT, padx=(0, 5))
        
        # Edit timeline canvas for scene boundaries
        self.edit_timeline_canvas = tk.Canvas(edit_timeline_frame, height=40, bg='#1a1a2e', highlightthickness=1, highlightbackground='#555')
        self.edit_timeline_canvas.pack(side=tk.LEFT, fill=tk.X, expand=True, padx=(0, 10))
        
        # Edit Timeline å†…éƒ¨å…ƒç´ 
        self.edit_timeline_bg = None
        self.edit_handles = []  # å­˜å‚¨æ‰€æœ‰å¯æ‹–åŠ¨èŠ‚ç‚¹
        self.edit_handle_dragging = None  # å½“å‰æ­£åœ¨æ‹–åŠ¨çš„èŠ‚ç‚¹ç´¢å¼•
        self.edit_regions = []  # å­˜å‚¨åœºæ™¯åŒºåŸŸæ˜¾ç¤º
        # _pending_boundaries å’Œ _boundaries_initialized å·²åœ¨ __init__ ä¸­åˆå§‹åŒ–
        
        # ç»‘å®š Edit Timeline äº‹ä»¶
        self.edit_timeline_canvas.bind('<Configure>', self._on_edit_timeline_configure)
        self.edit_timeline_canvas.bind('<Button-1>', self._on_edit_timeline_click)
        self.edit_timeline_canvas.bind('<B1-Motion>', self._on_edit_timeline_drag)
        self.edit_timeline_canvas.bind('<ButtonRelease-1>', self._on_edit_timeline_release)
        
        # åœºæ™¯æ•°é‡æ˜¾ç¤º
        self.scene_count_label = ttk.Label(edit_timeline_frame, text="1 åœºæ™¯", width=10)
        self.scene_count_label.pack(side=tk.LEFT)
        
        # Media controls (placed below the media visualization)
        control_container = ttk.Frame(main_frame)
        control_container.pack(fill=tk.X, pady=(0, 10))
        
        # Media controls
        control_frame = ttk.Frame(control_container)
        control_frame.pack(fill=tk.X, pady=5)
        
        self.play_button = ttk.Button(control_frame, text="â–¶ æ’­æ”¾", command=self.toggle_playback)
        self.play_button.pack(side=tk.LEFT, padx=15)
        
        self.play_time_label = ttk.Label(control_frame, text="0.00 / 0.00", foreground="blue")
        self.play_time_label.pack(side=tk.LEFT, padx=15)

        max_duration = self.workflow.ffmpeg_processor.get_duration(self.source_video_path) if self.source_video_path else self.workflow.ffmpeg_audio_processor.get_duration(self.source_audio_path)

        separator = ttk.Separator(control_frame, orient='vertical')
        separator.pack(side=tk.LEFT, fill=tk.Y, padx=15)

        ttk.Button(control_frame, text="æ¢å¤å‰ªè¾‘å˜åŠ¨", command=self.restore_edit_timeline_change).pack(side=tk.LEFT, padx=15)
        ttk.Button(control_frame, text="ç¡®è®¤å‰ªè¾‘å˜åŠ¨", command=self.confirm_edit_timeline_change).pack(side=tk.LEFT, padx=15)
        
        # åˆ†éš”ç¬¦
        ttk.Separator(control_frame, orient=tk.VERTICAL).pack(side=tk.LEFT, fill=tk.Y, padx=20)
        
        
        # add a button to let user record the audio from microphone, then put it as source_audio_path, then transcribe it, set self.regenerate_audio to True
        ttk.Button(control_frame, text="å½•éŸ³", command=self.record_audio).pack(side=tk.LEFT, padx=(0, 10))

        # Initialize play time display
        self.update_play_time_display()
        
        # ä¸å†éœ€è¦ trace å›è°ƒï¼Œå› ä¸ºå·²ç§»é™¤ start_time_var å’Œ end_time_var
        self.update_duration_display()
        
        # Text editors section for JSON data
        editors_frame = ttk.LabelFrame(main_frame, text="JSONç¼–è¾‘å™¨", padding=10)
        editors_frame.pack(fill=tk.BOTH, expand=True, pady=(0, 10))
        
        # Create horizontal frame for side-by-side editors
        editors_container = ttk.Frame(editors_frame)
        editors_container.pack(fill=tk.BOTH, expand=True)
        
        # Editor 1: Fresh JSON (left side)
        fresh_frame = ttk.Frame(editors_container)
        fresh_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(0, 5))
        
        self.fresh_label = ttk.Label(fresh_frame, text="Converation Script")
        self.fresh_label.pack(anchor="w", pady=(0, 5))
        
        # Fresh JSON text editor with scrollbar
        fresh_text_frame = ttk.Frame(fresh_frame)
        fresh_text_frame.pack(fill=tk.BOTH, expand=True)
        
        self.fresh_json_text = tk.Text(fresh_text_frame, wrap=tk.WORD, width=40, height=15)
        fresh_scrollbar = ttk.Scrollbar(fresh_text_frame, orient="vertical", command=self.fresh_json_text.yview)
        self.fresh_json_text.configure(yscrollcommand=fresh_scrollbar.set)
        
        self.fresh_json_text.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        fresh_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        
        # æ ‡å¿—ä½ï¼Œé˜²æ­¢å¾ªç¯æ›´æ–°
        self._updating_from_json = False
        
        # Buttons for fresh JSON editor
        fresh_buttons_frame = ttk.Frame(fresh_frame)
        fresh_buttons_frame.pack(fill=tk.X, pady=(5, 0))
        
        # æ—ç™½è¯­éŸ³ç»„
        ttk.Label(fresh_buttons_frame, text="è§’è‰²").pack(side=tk.LEFT, padx=(0, 5))
        self.speaker = ttk.Combobox(fresh_buttons_frame, values=self.ACTORS, state="normal", width=30)
        self.speaker.pack(side=tk.LEFT, padx=(0, 10))
        self.speaker.current(0)
        self.speaker.bind("<<ComboboxSelected>>", lambda e: self.on_speaker_changed(e))
        self.speaker.bind("<FocusOut>", lambda e: self.on_speaker_changed(e))

        # transcribe exsiting conversation (if >>30 sec), then remix single conversation
        ttk.Button(fresh_buttons_frame, text="å•é‡å»º", command=lambda: self.remix_conversation("single", False)).pack(side=tk.LEFT)
        ttk.Button(fresh_buttons_frame, text="å¤šé‡å»º", command=lambda: self.remix_conversation("multiple", False)).pack(side=tk.LEFT)
        
        ttk.Button(fresh_buttons_frame, text="å‰æ¥é‡", command=lambda: self.remix_conversation("connect_prev", False)).pack(side=tk.LEFT)
        ttk.Button(fresh_buttons_frame, text="åæ¥é‡", command=lambda: self.remix_conversation("connect_next", False)).pack(side=tk.LEFT)
        # transcribe exsiting conversation (if >>30 sec), then remix multiple conversation

        ttk.Button(fresh_buttons_frame, text="å•è½¬å½•", command=lambda: self.transcribe_audio("single")).pack(side=tk.LEFT)
        ttk.Button(fresh_buttons_frame, text="å¤šè½¬å½•", command=lambda: self.transcribe_audio("multiple")).pack(side=tk.LEFT)
        ttk.Button(fresh_buttons_frame, text="ç”ŸéŸ³é¢‘", command=self.regenerate_audio).pack(side=tk.LEFT)
        
        # ttk.Button(fresh_buttons_frame, text="å‰ªéŸ³è§†", command=lambda: self.trim_video(False)).pack(side=tk.LEFT)
        # ttk.Button(fresh_buttons_frame, text="å‰ªè§†é¢‘", command=lambda: self.trim_video(True)).pack(side=tk.LEFT)
        
        ttk.Button(fresh_buttons_frame, text="æ›¿æ¢", command=self.confirm_replacement).pack(side=tk.RIGHT)
        ttk.Button(fresh_buttons_frame, text="å–æ¶ˆ", command=self.cancel).pack(side=tk.RIGHT)
        
        if not pygame.mixer.get_init():
            pygame.mixer.init(frequency=44100, size=-16, channels=2, buffer=2048)
        

    def draw_waveform_placeholder(self):
        """Draw a simple waveform placeholder"""
        if not self.source_audio_path:
            return

        width = 750
        height = 180
        center_y = height // 2
        
        # Clear canvas
        self.waveform_canvas.delete("all")
        
        # Draw simple waveform simulation
        import math
        for x in range(0, width, 2):
            # Create random-looking waveform
            amplitude = 50 * math.sin(x * 0.05) * (0.5 + 0.5 * math.sin(x * 0.01))
            y1 = center_y - amplitude
            y2 = center_y + amplitude
            self.waveform_canvas.create_line(x, y1, x, y2, fill="green", width=1)
        
        # Draw time markers  
        display_duration = self.workflow.ffmpeg_audio_processor.get_duration(self.source_audio_path)
        if display_duration > 0:
            for i in range(0, int(display_duration) + 1, max(1, int(display_duration) // 10)):
                x = (i / display_duration) * width
                self.waveform_canvas.create_line(x, 0, x, height, fill="gray", width=1)
                self.waveform_canvas.create_text(x, height - 10, text=f"{i}s", fill="white", anchor="n")
    

    def update_duration_display(self, *args):
        """Update the selected duration display"""
        if not self.source_audio_path:
            return
        try:
            start = self._get_start_time()
            end = self._get_end_time()
            if end > start:
                # Update waveform selection visualization
                self.waveform_canvas.delete("selection")
                # Draw selection overlay
                width = self.waveform_canvas.winfo_width()
                height = self.waveform_canvas.winfo_height()
                
                display_duration = self.workflow.ffmpeg_processor.get_duration(self.source_video_path) if self.source_video_path else self.workflow.ffmpeg_audio_processor.get_duration(self.source_audio_path)
                if display_duration > 0:
                    start_x = (start / display_duration) * width
                    end_x = (end / display_duration) * width
                else:
                    start_x = end_x = 0
                # Draw selection rectangle
                self.waveform_canvas.create_rectangle(start_x, 0, end_x, height, 
                                                    fill="yellow", stipple="gray50", tags="selection")
        except:
            pass
    

    def update_play_time_display(self):
        """Update the play time display"""
        try:
            current_time = self.current_playback_time
            
            # Get total duration from video or audio
            if self.source_video_path:
                total_duration = self.workflow.ffmpeg_processor.get_duration(self.source_video_path)
            elif self.source_audio_path:
                total_duration = self.workflow.ffmpeg_audio_processor.get_duration(self.source_audio_path)
            else:
                total_duration = 0.0
            
            # Ensure we have valid values
            if total_duration is None or total_duration <= 0:
                total_duration = 0.0
            if current_time is None or current_time < 0:
                current_time = 0.0
                
            current_str = f"{current_time:.2f}"
            total_str = f"{total_duration:.2f}"
            
            self.play_time_label.config(text=f"{current_str} / {total_str}")
            
            # åŒæ—¶æ›´æ–° Progress Bar æ’­æ”¾å¤´ä½ç½®
            self._update_progress_playhead()
            
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°æ—¶é—´æ˜¾ç¤ºå¤±è´¥: {e}")
            self.play_time_label.config(text="0.00 / 0.00")
    

    # ========== Progress Bar (æ’­æ”¾è¿›åº¦æ¡) æ–¹æ³• ==========
    
    def _on_progress_canvas_configure(self, event=None):
        """Progress Bar canvas å¤§å°å˜åŒ–æ—¶é‡ç»˜"""
        self._draw_progress_bar()
    
    def _draw_progress_bar(self):
        """ç»˜åˆ¶ Progress Bar èƒŒæ™¯å’Œæ’­æ”¾å¤´"""
        canvas = self.progress_canvas
        canvas.delete('all')
        
        width = canvas.winfo_width()
        height = canvas.winfo_height()
        if width <= 1 or height <= 1:
            return
        
        padding = 10
        bar_height = 8
        bar_y = (height - bar_height) // 2
        
        # ç»˜åˆ¶è¿›åº¦æ¡èƒŒæ™¯
        canvas.create_rectangle(padding, bar_y, width - padding, bar_y + bar_height,
                               fill='#404040', outline='#606060', tags='progress_bg')
        
        # ç»˜åˆ¶å·²æ’­æ”¾éƒ¨åˆ†
        if self.audio_duration > 0:
            progress_ratio = min(1.0, self.current_playback_time / self.audio_duration)
            progress_x = padding + (width - 2 * padding) * progress_ratio
            canvas.create_rectangle(padding, bar_y, progress_x, bar_y + bar_height,
                                   fill='#4CAF50', outline='', tags='progress_fill')
        
        # ç»˜åˆ¶æ’­æ”¾å¤´ï¼ˆå¯æ‹–åŠ¨çš„åœ†å½¢æŒ‰é’®ï¼‰
        self._draw_playhead()
    
    def _draw_playhead(self):
        """ç»˜åˆ¶æ’­æ”¾å¤´"""
        canvas = self.progress_canvas
        canvas.delete('playhead')
        
        width = canvas.winfo_width()
        height = canvas.winfo_height()
        if width <= 1 or self.audio_duration <= 0:
            return
        
        padding = 10
        progress_ratio = min(1.0, max(0, self.current_playback_time / self.audio_duration))
        playhead_x = padding + (width - 2 * padding) * progress_ratio
        playhead_y = height // 2
        playhead_radius = 8
        
        # ç»˜åˆ¶æ’­æ”¾å¤´åœ†å½¢
        canvas.create_oval(playhead_x - playhead_radius, playhead_y - playhead_radius,
                          playhead_x + playhead_radius, playhead_y + playhead_radius,
                          fill='#FF5722', outline='white', width=2, tags='playhead')
        
        # æ›´æ–°æ—¶é—´æ˜¾ç¤º
        self.progress_time_label.config(text=f"{self.current_playback_time:.2f}")
    
    def _update_progress_playhead(self):
        """æ›´æ–°æ’­æ”¾å¤´ä½ç½®ï¼ˆæ’­æ”¾æ—¶è°ƒç”¨ï¼‰"""
        self._draw_progress_bar()
    
    def _on_progress_click(self, event):
        """Progress Bar ç‚¹å‡»äº‹ä»¶"""
        self.progress_dragging = True
        self._seek_to_position(event.x)
    
    def _on_progress_drag(self, event):
        """Progress Bar æ‹–åŠ¨äº‹ä»¶"""
        if self.progress_dragging:
            self._seek_to_position(event.x)
    
    def _on_progress_release(self, event):
        """Progress Bar é‡Šæ”¾äº‹ä»¶"""
        if self.progress_dragging:
            self._seek_to_position(event.x)
            self.progress_dragging = False
    
    def _seek_to_position(self, x):
        """è·³è½¬åˆ°æŒ‡å®šä½ç½®"""
        canvas = self.progress_canvas
        width = canvas.winfo_width()
        padding = 10
        
        # è®¡ç®—æ—¶é—´
        bar_width = width - 2 * padding
        if bar_width <= 0:
            return
        
        ratio = max(0, min(1, (x - padding) / bar_width))
        new_time = ratio * self.audio_duration
        
        # æ›´æ–°æ’­æ”¾ä½ç½®
        self.current_playback_time = new_time
        self.pause_accumulated_time = new_time
        
        # æ‹–åŠ¨è¿›åº¦æ¡ä¼šå¯¼è‡´éœ€è¦é‡æ–°åŠ è½½éŸ³é¢‘ï¼Œæ‰€ä»¥æ¸…é™¤æš‚åœçŠ¶æ€
        was_playing = self.av_playing
        self.av_paused = False
        
        # å¦‚æœæ­£åœ¨æ’­æ”¾ï¼Œéœ€è¦é‡æ–°è®¾ç½®æ’­æ”¾ä½ç½®
        if was_playing:
            self.playback_start_time = time.time()
            
            # æ›´æ–°è§†é¢‘ä½ç½®
            if self.source_video_path and self.video_cap:
                fps = self.video_cap.get(cv2.CAP_PROP_FPS) or 30
                frame_num = int(new_time * fps)
                self.video_cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)
            
            # æ›´æ–°éŸ³é¢‘ä½ç½®ï¼ˆéœ€è¦é‡æ–°åŠ è½½ï¼‰
            if self.source_audio_path:
                pygame.mixer.music.stop()
                pygame.mixer.music.load(self.source_audio_path)
                pygame.mixer.music.play(start=new_time)
                print(f"ğŸ” è¿›åº¦æ¡æ‹–åŠ¨: è·³è½¬åˆ° {new_time:.2f}s")
        
        # æ›´æ–°æ˜¾ç¤º
        self.update_play_time_display()
        self._draw_progress_bar()
    
    # ========== Edit Timeline (å‰ªè¾‘æ—¶é—´è½´) æ–¹æ³• ==========
    
    def _on_edit_timeline_configure(self, event=None):
        """Edit Timeline canvas å¤§å°å˜åŒ–æ—¶é‡ç»˜"""
        self._draw_edit_timeline()
    
    def _draw_edit_timeline(self):
        """ç»˜åˆ¶å‰ªè¾‘æ—¶é—´è½´"""
        canvas = self.edit_timeline_canvas
        canvas.delete('all')
        
        width = canvas.winfo_width()
        height = canvas.winfo_height()
        if width <= 1 or height <= 1:
            return
        
        padding = 10
        bar_y = height // 2
        bar_height = 16
        
        # è·å–åœºæ™¯è¾¹ç•Œæ—¶é—´ç‚¹
        boundaries = self._get_scene_boundaries()
        num_scenes = len(boundaries) - 1 if len(boundaries) > 1 else 1
        
        # è°ƒè¯•ï¼šæ‰“å°è¾¹ç•Œä¿¡æ¯
        if len(boundaries) >= 2:
            print(f"ğŸ¨ ç»˜åˆ¶æ—¶é—´è½´ - è¾¹ç•Œ: start={boundaries[0]:.2f}, end={boundaries[-1]:.2f}, åœºæ™¯æ•°={num_scenes}")
        
        # æ›´æ–°åœºæ™¯æ•°é‡æ˜¾ç¤º
        self.scene_count_label.config(text=f"{num_scenes} åœºæ™¯")
        
        # å®šä¹‰é¢œè‰²åˆ—è¡¨ï¼ˆç”¨äºåŒºåˆ†ä¸åŒåœºæ™¯ï¼‰
        colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c', '#e67e22', '#34495e']
        
        # ç»˜åˆ¶æ—¶é—´è½´èƒŒæ™¯
        canvas.create_rectangle(padding, bar_y - bar_height // 2, width - padding, bar_y + bar_height // 2,
                               fill='#2c3e50', outline='#7f8c8d', tags='timeline_bg')
        
        # ç»˜åˆ¶åœºæ™¯åŒºåŸŸ
        self.edit_regions = []
        for i in range(len(boundaries) - 1):
            start_time = boundaries[i]
            end_time = boundaries[i + 1]
            
            if self.audio_duration > 0:
                start_x = padding + (width - 2 * padding) * (start_time / self.audio_duration)
                end_x = padding + (width - 2 * padding) * (end_time / self.audio_duration)
            else:
                start_x = padding
                end_x = width - padding
            
            color = colors[i % len(colors)]
            region_id = canvas.create_rectangle(start_x, bar_y - bar_height // 2 + 2,
                                               end_x, bar_y + bar_height // 2 - 2,
                                               fill=color, outline='', tags=f'region_{i}')
            self.edit_regions.append(region_id)
            
            # åœ¨åŒºåŸŸä¸­å¤®æ˜¾ç¤ºåœºæ™¯ç¼–å·
            center_x = (start_x + end_x) / 2
            if end_x - start_x > 20:  # åªæœ‰ç©ºé—´è¶³å¤Ÿæ—¶æ‰æ˜¾ç¤ºæ–‡å­—
                canvas.create_text(center_x, bar_y, text=str(i + 1), fill='white',
                                  font=('Arial', 8, 'bold'), tags=f'region_text_{i}')
        
        # ç»˜åˆ¶å¯æ‹–åŠ¨çš„è¾¹ç•ŒèŠ‚ç‚¹
        self._draw_edit_handles(boundaries)
    
    def _draw_edit_handles(self, boundaries):
        """ç»˜åˆ¶å¯æ‹–åŠ¨çš„è¾¹ç•ŒèŠ‚ç‚¹"""
        canvas = self.edit_timeline_canvas
        canvas.delete('handle')
        
        width = canvas.winfo_width()
        height = canvas.winfo_height()
        padding = 10
        handle_radius = 6
        bar_y = height // 2
        
        self.edit_handles = []
        
        for i, boundary_time in enumerate(boundaries):
            if self.audio_duration > 0:
                x = padding + (width - 2 * padding) * (boundary_time / self.audio_duration)
            else:
                x = padding if i == 0 else width - padding
            
            # ç»˜åˆ¶èŠ‚ç‚¹ï¼ˆä¸‰è§’å½¢ + çº¿æ¡ï¼‰
            # å‚ç›´çº¿
            canvas.create_line(x, bar_y - 12, x, bar_y + 12, fill='white', width=2, tags=('handle', f'handle_line_{i}'))
            
            # é¡¶éƒ¨ä¸‰è§’å½¢
            canvas.create_polygon(x - 5, bar_y - 12, x + 5, bar_y - 12, x, bar_y - 6,
                                 fill='#FFD700', outline='white', tags=('handle', f'handle_top_{i}'))
            
            # åº•éƒ¨ä¸‰è§’å½¢
            canvas.create_polygon(x - 5, bar_y + 12, x + 5, bar_y + 12, x, bar_y + 6,
                                 fill='#FFD700', outline='white', tags=('handle', f'handle_bottom_{i}'))
            
            self.edit_handles.append({
                'index': i,
                'time': boundary_time,
                'x': x
            })
    
    def _ensure_boundaries_initialized(self):
        """ç¡®ä¿è¾¹ç•Œå·²åˆå§‹åŒ–"""
        if not self._boundaries_initialized or self._pending_boundaries is None:
            self._pending_boundaries = self._get_boundaries_from_audio_json()
            if not self._pending_boundaries or len(self._pending_boundaries) < 2:
                self._pending_boundaries = [0.0, self.audio_duration]
            self._boundaries_initialized = True
    
    def _get_start_time(self):
        """è·å–å¼€å§‹æ—¶é—´ï¼šä» _pending_boundaries[0] è·å–"""
        self._ensure_boundaries_initialized()
        if self._pending_boundaries and len(self._pending_boundaries) > 0:
            return self._pending_boundaries[0]
        return 0.0
    
    def _get_end_time(self):
        """è·å–ç»“æŸæ—¶é—´ï¼šä» _pending_boundaries[-1] è·å–"""
        self._ensure_boundaries_initialized()
        if self._pending_boundaries and len(self._pending_boundaries) > 1:
            return self._pending_boundaries[-1]
        return self.audio_duration
    
    def _set_start_time(self, value):
        """è®¾ç½®å¼€å§‹æ—¶é—´ï¼šæ›´æ–° _pending_boundaries[0]"""
        self._ensure_boundaries_initialized()
        if self._pending_boundaries:
            self._pending_boundaries[0] = value
            self._boundaries_initialized = True
        self.update_duration_display()
    
    def _set_end_time(self, value):
        """è®¾ç½®ç»“æŸæ—¶é—´ï¼šæ›´æ–° _pending_boundaries[-1]"""
        self._ensure_boundaries_initialized()
        if self._pending_boundaries:
            self._pending_boundaries[-1] = value
            self._boundaries_initialized = True
        self.update_duration_display()
    
    def _get_scene_boundaries(self):
        """è·å–åœºæ™¯è¾¹ç•Œæ—¶é—´ç‚¹åˆ—è¡¨"""
        # ç¡®ä¿è¾¹ç•Œå·²åˆå§‹åŒ–
        self._ensure_boundaries_initialized()
        
        # è¿”å›å½“å‰è¾¹ç•Œçš„å‰¯æœ¬
        if self._pending_boundaries and len(self._pending_boundaries) >= 2:
            return self._pending_boundaries.copy()
        
        # å¦‚æœä»ç„¶æ²¡æœ‰æœ‰æ•ˆè¾¹ç•Œï¼Œè¿”å›é»˜è®¤å€¼
        return [0.0, self.audio_duration]
    
    def _on_edit_timeline_click(self, event):
        """Edit Timeline ç‚¹å‡»äº‹ä»¶"""
        # æ£€æŸ¥æ˜¯å¦ç‚¹å‡»äº†æŸä¸ªèŠ‚ç‚¹
        handle_index = self._find_handle_at(event.x, event.y)
        if handle_index is not None:
            self.edit_handle_dragging = handle_index
    
    def _on_edit_timeline_drag(self, event):
        """Edit Timeline æ‹–åŠ¨äº‹ä»¶"""
        if self.edit_handle_dragging is not None:
            self._drag_handle_to(self.edit_handle_dragging, event.x)
    
    def _on_edit_timeline_release(self, event):
        """Edit Timeline é‡Šæ”¾äº‹ä»¶"""
        if self.edit_handle_dragging is not None:
            self._drag_handle_to(self.edit_handle_dragging, event.x)
            self.edit_handle_dragging = None
            # æ³¨æ„ï¼šä¸å†ç«‹å³æ›´æ–° audio_jsonï¼Œä¿æŒ pending çŠ¶æ€
            # ç”¨æˆ·éœ€è¦ç‚¹å‡»"ç¡®è®¤å‰ªè¾‘åŒºé—´å˜åŠ¨"æ¥åº”ç”¨æ›´æ”¹
    
    def _find_handle_at(self, x, y):
        """æŸ¥æ‰¾æŒ‡å®šä½ç½®çš„èŠ‚ç‚¹ç´¢å¼•"""
        for handle in self.edit_handles:
            if abs(x - handle['x']) < 15:  # 15 åƒç´ å®¹å·®
                return handle['index']
        return None
    
    def _drag_handle_to(self, handle_index, x):
        """æ‹–åŠ¨èŠ‚ç‚¹åˆ°æŒ‡å®šä½ç½®"""
        canvas = self.edit_timeline_canvas
        width = canvas.winfo_width()
        padding = 10
        
        # è®¡ç®—æ–°æ—¶é—´
        bar_width = width - 2 * padding
        if bar_width <= 0:
            return
        
        ratio = max(0, min(1, (x - padding) / bar_width))
        new_time = ratio * self.audio_duration
        
        # ç¡®ä¿è¾¹ç•Œå·²åˆå§‹åŒ–
        self._ensure_boundaries_initialized()
        
        # æ‹–åŠ¨æ—¶åªæ›´æ–° _pending_boundariesï¼Œaudio_json çš„ start/end åœ¨ç¡®è®¤æ—¶æ‰æ›´æ–°
        boundaries = self._pending_boundaries
        
        # ç¡®ä¿ä¸è¶Šç•Œï¼ˆä¿æŒé¡ºåºï¼‰
        # ç¬¬ä¸€ä¸ª handle å¯ä»¥ç§»åŠ¨ï¼Œä½†ä¸èƒ½è¶…è¿‡ç¬¬äºŒä¸ª
        # æœ€åä¸€ä¸ª handle å¯ä»¥ç§»åŠ¨ï¼Œä½†ä¸èƒ½å°äºå€’æ•°ç¬¬äºŒä¸ª
        if handle_index == 0:
            min_time = 0.0
            max_time = boundaries[1] - 0.1 if len(boundaries) > 1 else self.audio_duration
        elif handle_index == len(boundaries) - 1:
            min_time = boundaries[handle_index - 1] + 0.1
            max_time = self.audio_duration
        else:
            min_time = boundaries[handle_index - 1] + 0.1
            max_time = boundaries[handle_index + 1] - 0.1
        
        new_time = max(min_time, min(max_time, new_time))
        
        # æ›´æ–°è¾¹ç•Œ
        boundaries[handle_index] = new_time
        self._pending_boundaries = boundaries
        
        # å½“å¼€å§‹æˆ–ç»“æŸæ—¶é—´æ”¹å˜æ—¶ï¼Œè§¦å‘æ›´æ–°æ˜¾ç¤º
        if handle_index == 0 or handle_index == len(boundaries) - 1:
            self.update_duration_display()
        
        # é‡ç»˜æ—¶é—´è½´
        self._draw_edit_timeline()
    

    def _get_boundaries_from_audio_json(self):
        """ä» audio_json çš„ start/end å­—æ®µç”Ÿæˆè¾¹ç•Œåˆ—è¡¨"""
        if not hasattr(self, 'audio_json') or not self.audio_json:
            return [0.0, self.audio_duration]
        
        boundaries = []
        current_time = 0.0
        
        for i, scene in enumerate(self.audio_json):
            # ç¡®ä¿æ¯ä¸ªåœºæ™¯éƒ½æœ‰ start/end å­—æ®µ
            if 'start' not in scene or 'end' not in scene:
                # å¦‚æœæ²¡æœ‰ï¼Œä» duration è®¡ç®—å¹¶æ·»åŠ 
                duration = scene.get('duration', 0)
                scene['start'] = current_time
                scene['end'] = current_time + duration
            
            # æ·»åŠ èµ·å§‹è¾¹ç•Œï¼ˆåªåœ¨ç¬¬ä¸€ä¸ªåœºæ™¯æ—¶ï¼‰
            if i == 0:
                boundaries.append(scene['start'])
            
            # æ·»åŠ ç»“æŸè¾¹ç•Œ
            boundaries.append(scene['end'])
            current_time = scene['end']
        
        return boundaries if len(boundaries) >= 2 else [0.0, self.audio_duration]
    
    def _update_scene_durations_from_boundaries(self, boundaries):
        """æ ¹æ®è¾¹ç•Œæ—¶é—´æ›´æ–° audio_json ä¸­çš„ start/end/duration å­—æ®µ"""
        if not hasattr(self, 'audio_json') or not self.audio_json:
            return
        
        for i in range(len(self.audio_json)):
            if i < len(boundaries) - 1:
                start_time = boundaries[i]
                end_time = boundaries[i + 1]
                self.audio_json[i]['start'] = start_time
                self.audio_json[i]['end'] = end_time
                self.audio_json[i]['duration'] = end_time - start_time
    
    def _update_audio_json_from_timeline(self):
        """ä»æ—¶é—´è½´æ›´æ–° audio_json"""
        # duration å·²ç»åœ¨æ‹–åŠ¨è¿‡ç¨‹ä¸­æ›´æ–°äº†
        pass
    
    def refresh_edit_timeline(self):
        """åˆ·æ–°å‰ªè¾‘æ—¶é—´è½´ï¼ˆå½“ audio_json å˜åŒ–æ—¶è°ƒç”¨ï¼‰"""
        # æ¸…é™¤å¾…ç¡®è®¤çŠ¶æ€å’Œè¾¹ç•Œï¼Œå¼ºåˆ¶é‡æ–°è®¡ç®—
        self._pending_boundaries = None
        self._boundaries_initialized = False
        
        # é‡æ–°åˆå§‹åŒ–è¾¹ç•Œï¼ˆä» audio_json çš„ start/end å­—æ®µè¯»å–ï¼‰
        self._ensure_boundaries_initialized()
        
        self._draw_edit_timeline()
        self.update_duration_display()


    def restore_edit_timeline_change(self):
        if self.media_type != "clip":
            return
        
        """æ¢å¤å‰ªè¾‘åŒºé—´å˜åŠ¨ - ä» audio_json çš„ start/end å­—æ®µé‡æ–°è¯»å–è¾¹ç•Œ"""
        self.stop_playback()
        
        # æ¸…é™¤å¾…ç¡®è®¤çš„è¾¹ç•Œå˜åŒ–ï¼Œå¼ºåˆ¶ä» audio_json é‡æ–°è¯»å–
        self._pending_boundaries = None
        self._boundaries_initialized = False
        
        # ä» audio_json çš„ start/end å­—æ®µé‡æ–°åˆå§‹åŒ–è¾¹ç•Œ
        self._ensure_boundaries_initialized()
        
        print("âœ“ å·²æ¢å¤å‰ªè¾‘åŒºé—´åˆ°åŸå§‹çŠ¶æ€ï¼ˆä» audio_json è¯»å–ï¼‰")
        
        # é‡ç»˜æ—¶é—´è½´
        self._draw_edit_timeline()
        self.update_duration_display()


    def confirm_edit_timeline_change(self):
        if self.media_type != "clip":
            return
        
        """ç¡®è®¤å‰ªè¾‘åŒºé—´å˜åŠ¨ - å°†æ—¶é—´è½´çš„å€¼ä¿å­˜åˆ° audio_json"""
        self.stop_playback()
        
        # è·å–è¾¹ç•Œï¼šä¼˜å…ˆä½¿ç”¨ _pending_boundariesï¼Œå¦åˆ™ä» audio_json è®¡ç®—
        if self._pending_boundaries is not None:
            boundaries = self._pending_boundaries
            print(f"ğŸ“ ä½¿ç”¨ pending_boundaries: {len(boundaries)} ä¸ªè¾¹ç•Œç‚¹")
        else:
            # æ²¡æœ‰ pending çŠ¶æ€æ—¶ï¼Œä»å½“å‰ audio_json è®¡ç®—è¾¹ç•Œ
            boundaries = self._get_boundaries_from_audio_json()
            print(f"ğŸ“ ä» audio_json è®¡ç®—è¾¹ç•Œ: {len(boundaries)} ä¸ªè¾¹ç•Œç‚¹")
        
        # æ›´æ–° audio_json ä¸­æ¯ä¸ªåœºæ™¯çš„ start/end/durationï¼ˆä»…å½“æœ‰ pending å˜æ›´æ—¶ï¼‰
        if self._pending_boundaries is not None and self.audio_json:
            for i in range(len(self.audio_json)):
                if i < len(boundaries) - 1:
                    start_time = boundaries[i]
                    end_time = boundaries[i + 1]
                    self.audio_json[i]['start'] = start_time
                    self.audio_json[i]['end'] = end_time
                    self.audio_json[i]['duration'] = end_time - start_time
            
            self._update_fresh_json_text()
        
        # ä» audio_json é‡æ–°è®¡ç®—è¾¹ç•Œï¼ˆstart/end å­—æ®µå·²åœ¨ä¸Šé¢æ›´æ–°ï¼‰
        self._pending_boundaries = None
        self._boundaries_initialized = False
        self._ensure_boundaries_initialized()
        
        # é‡ç»˜æ—¶é—´è½´
        self._draw_edit_timeline()
        self.update_duration_display()



    def trim_video(self, audio_unchange):
        video_path = safe_file(self.source_video_path)
        audio_path = safe_file(self.source_audio_path)
        if not audio_path or not video_path:
            return

        start_time = float(self._get_start_time())
        end_time = float(self._get_end_time())
        duration = self.workflow.ffmpeg_processor.get_duration(video_path)
        
        if end_time <= start_time:
            messagebox.showerror("é”™è¯¯", "ç»“æŸæ—¶é—´å¿…é¡»å¤§äºå¼€å§‹æ—¶é—´")
            return
        
        if end_time > duration:
            end_time = duration
        if start_time < 0:
            start_time = 0.0

        if start_time > 0.05 or abs(duration-end_time) > 0.05:
            video_path = self.workflow.ffmpeg_processor.trim_video( video_path, start_time, end_time, volume=1.0 )
            if audio_unchange:
                video_path = self.workflow.ffmpeg_processor.add_audio_to_video(video_path, audio_path, True)
            else:    
                self.source_audio_path = self.workflow.ffmpeg_audio_processor.audio_cut_fade( audio_path, start_time, end_time-start_time )

        if self.crop_start_x and self.crop_start_y:
            crop_x =self.crop_start_x
            crop_y = self.crop_start_y
            
            crop_width = self.workflow.ffmpeg_processor.width - crop_x
            crop_height = self.workflow.ffmpeg_processor.height - crop_y
            if self.workflow.ffmpeg_processor.width > self.workflow.ffmpeg_processor.height: # 16:9
                if crop_width/crop_height <= self.workflow.ffmpeg_processor.width/self.workflow.ffmpeg_processor.height:
                    crop_height = int(crop_width/16.0*9.0)
                else:
                    crop_width = int(crop_height/9.0*16.0)
            else:  # 9:16
                if crop_width/crop_height <= self.workflow.ffmpeg_processor.width/self.workflow.ffmpeg_processor.height:
                    crop_height = int(crop_width/9.0*16.0)
                else:
                    crop_width = int(crop_height/16.0*9.0)
            video_path = self.workflow.ffmpeg_processor.resize_video(video_path, width=crop_width, height=crop_height, startx=crop_x, starty=crop_y )
            video_path = self.workflow.ffmpeg_processor.resize_video(video_path, width=self.workflow.ffmpeg_processor.width, height=self.workflow.ffmpeg_processor.height)

        self.source_video_path = video_path



    def _transcribe_recorded_audio(self):
        """è½¬å½•å½•åˆ¶çš„éŸ³é¢‘"""
        if self.audio_duration <= 30:
            return False

        print("ğŸ”„ å¼€å§‹è½¬å½•å½•éŸ³...")
        
        # ä½¿ç”¨éŸ³é¢‘è½¬å½•å™¨è½¬å½•
        audio_json = self.transcriber.transcribe_with_whisper(
            self.source_audio_path, 
            self.workflow.language, 
            3,
            15
        )

        if not audio_json or len(audio_json) == 0:
            print("âš ï¸ å½•éŸ³è½¬å½•å¤±è´¥")
            return False

        duration = self.workflow.ffmpeg_audio_processor.get_duration(self.source_audio_path)
        audio_json[-1]["end"] = duration

        if self.transcribe_way == "single" or len(audio_json) == 1:
            all_captions = ". ".join([json_item["caption"] for json_item in audio_json])
            self.current_scene["caption"] = all_captions
            self.current_scene[self.SPEAKING_KEY] = all_captions
            self.audio_json = [self.current_scene]
        else: # multiple ~ only for self.media_type == "clip"
            raw_id = int((self.current_scene["id"]/100)*100)
            for item in audio_json:
                raw_id += 100
                item["id"] = raw_id
                item[self.SPEAKING_KEY] = item["caption"]
            self.audio_json = self.align_json_to_current_scene(audio_json)

        return True



    def toggle_playback(self):
        """Toggle media playback (audio or video+audio)"""
        if not self.av_playing:
            self.start_playback()
        else:
            self.pause_playback()


    def start_playback(self):
        self.av_playing = True
        was_paused = self.av_paused  # è®°å½•æ˜¯å¦æ˜¯ä»æš‚åœçŠ¶æ€æ¢å¤
        self.av_paused = False
        self.play_button.config(text="â¸ æš‚åœ")
        
        # è®¾ç½®ç´¯è®¡æ—¶é—´ä¸ºå½“å‰æ’­æ”¾ä½ç½®ï¼Œè¿™æ ·çº¿ç¨‹ä¸­çš„è®¡ç®—æ‰æ­£ç¡®
        # current_playback_time = pause_accumulated_time + elapsed_since_start
        self.pause_accumulated_time = self.current_playback_time
        
        # *** å…³é”®ä¿®å¤ï¼šåœ¨å¯åŠ¨è§†é¢‘å’Œçº¿ç¨‹ä¹‹å‰å…ˆè®¾ç½®è®¡æ—¶åŸºå‡† ***
        # è¿™æ · update_video_frame å’Œæ›´æ–°çº¿ç¨‹ä»ä¸€å¼€å§‹å°±ä½¿ç”¨æ­£ç¡®çš„æ—¶é—´
        self.playback_start_time = time.time()
        print(f"â–¶ å¼€å§‹æ’­æ”¾: current_time={self.current_playback_time:.2f}s, pause_accumulated={self.pause_accumulated_time:.2f}s, was_paused={was_paused}, playback_start_time={self.playback_start_time:.3f}")
        
        if self.source_video_path:
            # å¦‚æœè§†é¢‘æ•è·å¯¹è±¡ä¸å­˜åœ¨ï¼Œæˆ–è€…ä¸æ˜¯ä»æš‚åœæ¢å¤ï¼Œå°±é‡æ–°åˆ›å»º
            if self.video_cap is None or (not was_paused and self.current_playback_time < 0.1):
                # é‡Šæ”¾æ—§çš„è§†é¢‘æ•è·å¯¹è±¡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if self.video_cap:
                    self.video_cap.release()
                # åˆ›å»ºæ–°çš„è§†é¢‘æ•è·å¯¹è±¡
                self.video_cap = cv2.VideoCapture(self.source_video_path)
                if not self.video_cap.isOpened():
                    print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {self.source_video_path}")
                    self.av_playing = False
                    self.play_button.config(text="â–¶ æ’­æ”¾")
                    return
                print(f"âœ“ å·²åŠ è½½è§†é¢‘: {os.path.basename(self.source_video_path)}")
            
            # å¦‚æœéœ€è¦ä»æŒ‡å®šä½ç½®å¼€å§‹æ’­æ”¾ï¼ˆéæš‚åœæ¢å¤ï¼‰ï¼Œè®¾ç½®è§†é¢‘ä½ç½®
            if not was_paused and self.current_playback_time > 0:
                fps = self.video_cap.get(cv2.CAP_PROP_FPS) or 30
                start_frame = int(self.current_playback_time * fps)
                self.video_cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
            elif was_paused:
                # ä»æš‚åœæ¢å¤ï¼Œç¡®ä¿è§†é¢‘ä½ç½®æ­£ç¡®
                fps = self.video_cap.get(cv2.CAP_PROP_FPS) or 30
                current_frame = int(self.current_playback_time * fps)
                self.video_cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame)
                print(f"ğŸ¬ è§†é¢‘ä»æš‚åœä½ç½®æ¢å¤: {self.current_playback_time:.2f}s, å¸§: {current_frame}")
            
            # Start video frame updates
            self.update_video_frame()
            
        if self.source_audio_path:
            # åªæœ‰åœ¨çœŸæ­£è¶…è¿‡ç»“æŸä½ç½®æ—¶æ‰é‡ç½®ï¼ˆä¸æ˜¯æ¥è¿‘ï¼‰
            if self.current_playback_time >= self.audio_duration:
                print(f"âš ï¸ æ’­æ”¾ä½ç½® {self.current_playback_time:.2f}s å·²è¶…è¿‡éŸ³é¢‘æ—¶é•¿ {self.audio_duration:.2f}sï¼Œé‡ç½®åˆ°å¼€å§‹")
                self.current_playback_time = 0.0
                self.pause_accumulated_time = 0.0
                self.playback_start_time = time.time()  # é‡ç½®åæ›´æ–°æ—¶é—´åŸºå‡†
                was_paused = False
            
            if was_paused and self.current_playback_time > 0:
                # ä»æš‚åœçŠ¶æ€æ¢å¤ï¼Œä½¿ç”¨ unpause
                pygame.mixer.music.unpause()
                print(f"ğŸµ éŸ³é¢‘ä»æš‚åœä½ç½®æ¢å¤æ’­æ”¾: {self.current_playback_time:.2f}s")
            else:
                # é¦–æ¬¡æ’­æ”¾æˆ–ä»æŒ‡å®šä½ç½®å¼€å§‹
                pygame.mixer.music.stop()
                pygame.mixer.music.load(self.source_audio_path)
                pygame.mixer.music.play(start=self.current_playback_time)
                print(f"ğŸµ éŸ³é¢‘ä» {self.current_playback_time:.2f}s å¼€å§‹æ’­æ”¾")
    
        self.start_time_update_thread()


    def pause_playback(self):
        """Pause audio-only playback"""
        # ç«‹å³è®¾ç½®çŠ¶æ€ï¼Œé˜²æ­¢æ›´æ–°çº¿ç¨‹ç»§ç»­è¿è¡Œ
        self.av_playing = False
        self.av_paused = True
        
        # å…ˆè®¡ç®—å½“å‰æ’­æ”¾ä½ç½®
        if self.playback_start_time is not None:
            elapsed_since_start = time.time() - self.playback_start_time
            self.current_playback_time = self.pause_accumulated_time + elapsed_since_start
            print(f"â¸ æš‚åœæ’­æ”¾: current_time={self.current_playback_time:.2f}s, pause_accumulated={self.pause_accumulated_time:.2f}s, elapsed={elapsed_since_start:.2f}s")
        else:
            print(f"âš ï¸ æš‚åœæ—¶ playback_start_time ä¸º None")
        
        # æ›´æ–°æŒ‰é’®
        self.play_button.config(text="â–¶ æ’­æ”¾")
        
        # Cancel video frame updates
        if self.video_after_id:
            self.dialog.after_cancel(self.video_after_id)
            
        # Pause audio (ä½¿ç”¨ pause è€Œä¸æ˜¯ stopï¼Œè¿™æ ·å¯ä»¥ä¿ç•™æ’­æ”¾ä½ç½®)
        if self.source_audio_path:
            print(f"â¸ æ­£åœ¨æš‚åœéŸ³é¢‘æ’­æ”¾ï¼Œä½ç½®: {self.current_playback_time:.2f}s")
            pygame.mixer.music.pause()
        
        print(f"â¸ æš‚åœå®Œæˆï¼Œæœ€ç»ˆä½ç½®: {self.current_playback_time:.2f}s")
        # æ›´æ–°çº¿ç¨‹ä¼šåœ¨ä¸‹ä¸€æ¬¡è¿­ä»£ï¼ˆ100mså†…ï¼‰æ£€æµ‹åˆ°çŠ¶æ€å˜åŒ–å¹¶é€€å‡º


    def stop_playback(self):
        """Stop media playback (audio or video+audio)"""
        print(f"â¹ stop_playback è¢«è°ƒç”¨: av_playing={self.av_playing}, av_paused={self.av_paused}, current_time={self.current_playback_time:.2f}s")
        
        # å¦‚æœå·²ç»æš‚åœï¼Œè¿™æ˜¯ä¸€ä¸ªé”™è¯¯è°ƒç”¨ï¼ˆå¯èƒ½æ˜¯ç«æ€æ¡ä»¶ï¼‰ï¼Œç›´æ¥è¿”å›
        if self.av_paused:
            print(f"âš ï¸ stop_playback åœ¨æš‚åœçŠ¶æ€ä¸‹è¢«è°ƒç”¨ï¼Œå¿½ç•¥ï¼ˆç«æ€æ¡ä»¶ï¼‰")
            return
        
        self.av_playing = False
        self.av_paused = False
        self.play_button.config(text="â–¶ æ’­æ”¾")
        self.current_playback_time = 0.0
        self.pause_accumulated_time = 0.0
        
        # Cancel video frame updates
        if self.source_video_path:
            if self.video_after_id:
                self.dialog.after_cancel(self.video_after_id)
            # Reset video to beginning
            if self.video_cap:
                self.video_cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
        
        if self.source_audio_path:
            pygame.mixer.music.stop()
            print(f"â¹ éŸ³é¢‘æ’­æ”¾åœæ­¢")
        
        # Reset playback time tracking
        self.pause_accumulated_time = 0.0
        self.playback_start_time = None
        
        # Update play time display to show 0.00 / total_duration
        self.update_play_time_display()



    def transcribe_audio(self, transcribe_way):
        self.transcribe_way = transcribe_way
        if self.media_type != 'clip' and self.media_type != 'narration':
            return
        self._transcribe_recorded_audio()
        self._update_fresh_json_text()


    def remix_conversation(self, mode, transcribe):
        if mode == "connect_prev" or mode == "connect_next" :
            self.transcribe_way = "single"
        else:
            self.transcribe_way = mode
        self.dialog.title( f"{self.media_type_names.get(self.media_type)} - {self.transcribe_way}" )
        
        if self.media_type != 'clip' and self.media_type != 'narration':
            return

        if transcribe:
            self._transcribe_recorded_audio()
            self._update_fresh_json_text()

        #topic = config_channel.CHANNEL_CONFIG[project_manager.PROJECT_CONFIG.get('channel', 'default')]["topic"]
        refresh_conversation = self.fresh_json_text.get(1.0, tk.END).strip()
        if mode == "connect_next":
            selected_prompt = config_channel.CHANNEL_CONFIG[self.workflow.channel]["channel_prompt"]["connection"]
            selected_prompt_example_file = self.workflow.channel + "_connection.json"
            refresh_json = [
                {
                    "name": "connection_addon_content",
                    "speaking": refresh_conversation
                },
                {
                    "name":"previous_scene",
                    "explicit": self.current_scene.get("explicit", "explicit"),
                    "implicit": self.current_scene.get("implicit", "implicit"),
                    "speaking": self.current_scene.get("speaking", ""),
                    "speaker": self.current_scene.get("speaker", ""),
                    "voiceover": self.current_scene.get("voiceover", "")
                },
                {
                    "name":"next_scene",
                    "explicit": self.next_scene.get("explicit", "explicit"),
                    "implicit": self.next_scene.get("implicit", "implicit"),
                    "speaking": self.next_scene.get("speaking", ""),
                    "speaker": self.next_scene.get("speaker", ""),
                    "voiceover": self.next_scene.get("voiceover", "")
                }
            ]
            refresh_conversation = json.dumps(refresh_json, indent=2, ensure_ascii=False)
        elif mode == "connect_prev":
            selected_prompt = config_channel.CHANNEL_CONFIG[self.workflow.channel]["channel_prompt"]["connection"]
            selected_prompt_example_file = self.workflow.channel + "_connection.json"
            refresh_json = [
                {
                    "name": "connection_addon_content",
                    "speaking": refresh_conversation
                },
                {
                    "name":"previous_scene",
                    "explicit": self.previous_scene.get("explicit", "explicit"),
                    "implicit": self.previous_scene.get("implicit", "implicit"),
                    "speaking": self.previous_scene.get("speaking", ""),
                    "speaker": self.previous_scene.get("speaker", ""),
                    "voiceover": self.previous_scene.get("voiceover", "")
                },
                {
                    "name":"next_scene",
                    "explicit": self.current_scene.get("explicit", "explicit"),
                    "implicit": self.current_scene.get("implicit", "implicit"),
                    "speaking": self.current_scene.get("speaking", ""),
                    "speaker": self.current_scene.get("speaker", ""),
                    "voiceover": self.current_scene.get("voiceover", "")
                }
            ]
            refresh_conversation = json.dumps(refresh_json, indent=2, ensure_ascii=False)
        else:
            refresh_json = None
            if refresh_conversation:
                try:
                    refresh_json = json.loads(refresh_conversation)
                    refresh_json_copy = []
                    for item in refresh_json:
                        new_item = {
                            "explicit": self.current_scene.get("explicit", "explicit"),
                            "implicit": self.current_scene.get("implicit", "implicit"),
                            "story_details": self.current_scene.get("story_details", ""),
                            "speaking": item.get("speaking", ""),
                            "speaker": item.get("speaker", ""),
                            "voiceover": item.get("voiceover", "")
                        }
                        refresh_json_copy.append(new_item)
                    refresh_conversation = json.dumps(refresh_json_copy, indent=2, ensure_ascii=False)
                except:
                    refresh_json = None

            if not refresh_json:
                refresh_json = [
                    {
                        "name": self.current_scene.get("name", "story"),
                        "explicit": self.current_scene.get("explicit", "explicit"),
                        "implicit": self.current_scene.get("implicit", "implicit"),
                        "story_details": self.current_scene.get("story_details", ""),
                        "speaking": refresh_conversation if refresh_conversation else self.current_scene.get("speaking", ""),
                        "speaker": self.current_scene.get("speaker", ""),
                        "voiceover": refresh_conversation if refresh_conversation else self.current_scene.get("voiceover", "")
                    }
                ]
                refresh_conversation = json.dumps(refresh_json, indent=2, ensure_ascii=False)

            selected_prompt = config_channel.CHANNEL_CONFIG[self.workflow.channel]["channel_prompt"][self.current_scene["name"]]
            selected_prompt_example_file = self.workflow.channel + "_" + self.current_scene["name"] + ".json"

        # read file from media folder
        example_file = os.path.join(os.path.dirname(__file__), "../media", selected_prompt_example_file)
        with open(example_file, "r", encoding="utf-8") as f:
            selected_prompt_example_text = f.read()
        selected_prompt_example = parse_json_from_text(selected_prompt_example_text)
        if selected_prompt_example is None:
            messagebox.showerror("é”™è¯¯", f"æ— æ³•è§£æç¤ºä¾‹æ–‡ä»¶: {example_file}")
            return

        if self.transcribe_way == "multiple":
            # Convert array back to JSON string for formatting
            example_json_str = json.dumps(selected_prompt_example, indent=2, ensure_ascii=False)
            if len(self.audio_json) > 1:
                selected_prompt = selected_prompt.format(json=f"json array holding {len(self.audio_json)} scenes", example=example_json_str)
            else:
                selected_prompt = selected_prompt.format(json="json array holding scenes", example=example_json_str)
        else:
            # Use first element of the array
            selected_prompt_example_item = selected_prompt_example[0] if selected_prompt_example else {}
            example_json_str = json.dumps(selected_prompt_example_item, indent=2, ensure_ascii=False)
            selected_prompt = selected_prompt.format(json="a single json item describing a scene", example=example_json_str)
        #format_args = selected_prompt.get("format_args", {}).copy()  # å¤åˆ¶é¢„è®¾å‚æ•°

        new_scenes = self.llm_api.generate_json(
            system_prompt=selected_prompt,
            user_prompt=refresh_conversation,
            expect_list=True
        )
        if not new_scenes or len(new_scenes) == 0:
            return

        if self.transcribe_way == "single" or (len(self.audio_json) == 1 and len(new_scenes) == 1):
            self.current_scene[self.SPEAKER_KEY] = new_scenes[0].get(self.SPEAKER_KEY, "")
            self.current_scene[self.SPEAKER_KEY+"_audio"] = self.source_audio_path
            self.current_scene["actions"] = new_scenes[0].get("actions", "")
            self.current_scene["visual"] = new_scenes[0].get("visual", "")
            self.current_scene["start"] = 0.0
            self.current_scene["extend"] = 0.0
            self.current_scene["end"] = self.audio_duration
            self.current_scene["duration"] = self.audio_duration
            self.current_scene["caption"] = ". ".join([item.get("caption", "") for item in new_scenes])
            self.current_scene["speaking"] = ". ".join([item.get("speaking", "") for item in new_scenes])
            self.current_scene["voiceover"] = ". ".join([item.get("voiceover", "") for item in new_scenes])
            self.audio_json = [self.current_scene]

        else:
            start_id = self.current_scene.get("id", 0)
            start_time = 0.0
            for i, scene in enumerate(new_scenes):
                fresh_scene = self.audio_json[i] if i < len(self.audio_json) else self.audio_json[-1]
                scene["caption"] = fresh_scene["caption"]
                duration = fresh_scene.get("duration", self.audio_duration)
                scene["duration"] = duration if len(self.audio_json) == len(new_scenes) else self.audio_duration / len(new_scenes)
                scene["start"] = start_time
                scene["extend"] = 0.0
                start_time = start_time + scene["duration"]
                scene["end"] = start_time
                scene["id"] = start_id + 1
                start_id = scene["id"]

                scene[self.SPEAKER_KEY+"_audio"] = None

            self.audio_json = self.align_json_to_current_scene(new_scenes)

        self._update_fresh_json_text()


    def align_json_to_current_scene(self, json_array):
        new_json_array = []
        for item in json_array:
            new_item = self.current_scene.copy()
            new_item["name"] = self.current_scene.get("name", "story")
            new_item["explicit"] = self.current_scene.get("explicit", "explicit")
            new_item["implicit"] = self.current_scene.get("implicit", "implicit")

            if "id" in item:
                new_item["id"] = item["id"]
            if self.SPEAKER_KEY in item:
                new_item[self.SPEAKER_KEY] = item[self.SPEAKER_KEY]
            if self.SPEAKER_KEY+"_audio" in item:
                new_item[self.SPEAKER_KEY+"_audio"] = item[self.SPEAKER_KEY+"_audio"]
            if "duration" in item:
                new_item["duration"] = item["duration"]
            if "visual" in item:
                new_item["visual"] = item["visual"]
            if "actions" in item:
                new_item["actions"] = item["actions"]
            if "start" in item:
                new_item["start"] = item["start"]
            if "end" in item:
                new_item["end"] = item["end"]
            if "caption" in item:
                new_item["caption"] = item["caption"]
            if "speaking" in item:
                new_item["speaking"] = item["speaking"]
            if "voiceover" in item:
                new_item["voiceover"] = item["voiceover"]
            new_json_array.append(new_item)
        return new_json_array


    def regenerate_audio(self):
        fresh_text = self.fresh_json_text.get(1.0, tk.END).strip()
        try:
            fresh_json = json.loads(fresh_text)
            if self.transcribe_way == "single":
                self.current_scene[self.SPEAKER_KEY] = fresh_json[0].get(self.SPEAKER_KEY, "")
                self.current_scene["actions"] = fresh_json[0].get("actions", "")
                self.current_scene["visual"] = fresh_json[0].get("visual", "")
                self.current_scene["caption"] = ". ".join([item.get("caption", "") for item in fresh_json])
                self.current_scene["speaking"] = ". ".join([item.get("speaking", "") for item in fresh_json])
                self.current_scene["voiceover"] = ". ".join([item.get("voiceover", "") for item in fresh_json])
                self.audio_json = [self.current_scene]
            else:
                self.audio_json = self.align_json_to_current_scene(fresh_json)
        except:
            messagebox.showerror("é”™è¯¯", "Fresh JSONæ ¼å¼ä¸æ­£ç¡®")
            return

        #self.audio_json, self.source_audio_path = self.parent.workflow.regenerate_audio(self.audio_json, self.workflow.language)
        lang = "chinese" if self.workflow.language == "zh" or self.workflow.language == "tw" else "english"
        start_time = 0.0
        for json_item in self.audio_json:
            speaker = json_item[self.SPEAKER_KEY]
            content = json_item[self.SPEAKING_KEY]
            actions = json_item["actions"]
            tts_wav = self.workflow.regenerate_audio_item(speaker, content, actions, lang)
            if tts_wav:
                olda, a = refresh_scene_media(json_item, self.SPEAKER_KEY+"_audio", ".wav", tts_wav, True)
                refresh_scene_media(json_item, self.media_type+"_audio", ".wav", a, True)
                v = self.workflow.ffmpeg_processor.add_audio_to_video(self.source_video_path, json_item[self.media_type+"_audio"])
                refresh_scene_media(json_item, self.media_type, ".mp4", v)
                if self.media_type == "clip":
                    json_item["duration"] = self.workflow.ffmpeg_audio_processor.get_duration(json_item[self.media_type+"_audio"])
                    json_item["extend"] = 1.0
                    json_item["start"] = start_time
                    start_time = start_time + json_item["duration"]
                    json_item["end"] = start_time
                    json_item["caption"] = json_item["speaking"]
            else:
                json_item[self.media_type+"_audio"] = None
                json_item[self.SPEAKER_KEY+"_audio"] = None
                json_item[self.media_type] = None
                if self.media_type == "clip":
                    json_item["duration"] = 0.0
                    json_item["start"] = 0.0
                    json_item["end"] = 0.0
                    json_item["caption"] = None

        # filter out json_item with clip_audio is None
        temp_json = [json_item for json_item in self.audio_json if json_item[self.SPEAKER_KEY+"_audio"] is not None]
        self.source_audio_path = self.workflow.ffmpeg_audio_processor.concat_audios([json_item[self.SPEAKER_KEY+"_audio"] for json_item in temp_json])
        self.audio_duration = self.workflow.ffmpeg_audio_processor.get_duration(self.source_audio_path)
        self._set_start_time(0.0)
        self._set_end_time(self.audio_duration)

        self.source_video_path = self.workflow.ffmpeg_processor.adjust_video_to_duration( self.source_video_path, self.audio_duration )

        self._update_fresh_json_text()

        self.refresh_edit_timeline()
        self.draw_waveform_placeholder()
        # pop up a messagebox to confirm the audio is regenerated
        messagebox.showinfo("æˆåŠŸ", "éŸ³é¢‘å·²é‡æ–°ç”Ÿæˆ")


    def record_audio(self):
        """å½•éŸ³åŠŸèƒ½ï¼šä»éº¦å…‹é£å½•éŸ³å¹¶è®¾ç½®ä¸ºæºéŸ³é¢‘"""
        if not RECORDING_AVAILABLE:
            messagebox.showerror("é”™è¯¯", "å½•éŸ³åŠŸèƒ½ä¸å¯ç”¨ã€‚è¯·å®‰è£… sounddevice å’Œ soundfile åº“ã€‚")
            return
        
        if self.recording:
            self.stop_recording()
        else:
            self.start_recording()


    def start_recording(self):
        """å¼€å§‹å½•éŸ³"""
        try:
            # åˆ›å»ºå½•éŸ³å¯¹è¯æ¡†
            self.recording_dialog = tk.Toplevel(self.dialog)
            self.recording_dialog.title("å½•éŸ³ä¸­...")
            self.recording_dialog.geometry("400x200")
            self.recording_dialog.resizable(False, False)
            self.recording_dialog.transient(self.dialog)
            self.recording_dialog.grab_set()
            
            # å½•éŸ³å¯¹è¯æ¡†å¸ƒå±€
            main_frame = ttk.Frame(self.recording_dialog, padding=20)
            main_frame.pack(fill=tk.BOTH, expand=True)
            
            # å½•éŸ³çŠ¶æ€æ˜¾ç¤º
            self.recording_status_label = ttk.Label(main_frame, text="æ­£åœ¨å½•éŸ³...", 
                                                  font=("Arial", 14), foreground="red")
            self.recording_status_label.pack(pady=10)
            
            # å½•éŸ³æ—¶é•¿æ˜¾ç¤º
            self.recording_time_label = ttk.Label(main_frame, text="00:00", 
                                                font=("Arial", 12))
            self.recording_time_label.pack(pady=5)
            
            # åœæ­¢å½•éŸ³æŒ‰é’®
            ttk.Button(main_frame, text="åœæ­¢å½•éŸ³", command=self.stop_recording).pack(pady=20)
            
            # å½•éŸ³å‚æ•°
            self.sample_rate = 44100  # é‡‡æ ·ç‡
            self.channels = 1  # å•å£°é“
            self.recording = True
            self.recorded_audio = []
            self.recording_start_time = time.time()
            
            # å¼€å§‹å½•éŸ³çº¿ç¨‹
            self.recording_thread = threading.Thread(target=self._recording_worker, daemon=True)
            self.recording_thread.start()
            
            # å¼€å§‹æ—¶é—´æ›´æ–°çº¿ç¨‹
            self._update_recording_time()
            
            print("ğŸ¤ å¼€å§‹å½•éŸ³...")
            
        except Exception as e:
            messagebox.showerror("é”™è¯¯", f"å¯åŠ¨å½•éŸ³å¤±è´¥: {str(e)}")
            self.recording = False



    def _recording_worker(self):
        """å½•éŸ³å·¥ä½œçº¿ç¨‹"""
        try:
            # å›è°ƒå‡½æ•°ç”¨äºæ¥æ”¶éŸ³é¢‘æ•°æ®
            def audio_callback(indata, frames, time, status):
                if status:
                    print(f"å½•éŸ³çŠ¶æ€: {status}")
                if self.recording:
                    self.recorded_audio.append(indata.copy())
            # å¼€å§‹éŸ³é¢‘æµ
            with sd.InputStream(samplerate=self.sample_rate, 
                              channels=self.channels, 
                              callback=audio_callback,
                              dtype='float32'):
                while self.recording:
                    time.sleep(0.1)
                    
        except Exception as e:
            print(f"å½•éŸ³çº¿ç¨‹é”™è¯¯: {e}")
            self.recording = False
            # åœ¨ä¸»çº¿ç¨‹ä¸­æ˜¾ç¤ºé”™è¯¯
            self.dialog.after(0, lambda: messagebox.showerror("é”™è¯¯", f"å½•éŸ³å¤±è´¥: {str(e)}"))



    def _update_recording_time(self):
        """æ›´æ–°å½•éŸ³æ—¶é—´æ˜¾ç¤º"""
        if self.recording and hasattr(self, 'recording_dialog') and self.recording_dialog.winfo_exists():
            elapsed = time.time() - self.recording_start_time
            minutes = int(elapsed // 60)
            sec = int(elapsed % 60)
            time_str = f"{minutes:02d}:{sec:02d}"
            self.recording_time_label.config(text=time_str)
            # æ¯100msæ›´æ–°ä¸€æ¬¡
            self.recording_dialog.after(100, self._update_recording_time)



    def stop_recording(self):
        """åœæ­¢å½•éŸ³å¹¶ä¿å­˜æ–‡ä»¶"""
        if not self.recording:
            return
            
        self.recording = False
        
        try:
            # å…³é—­å½•éŸ³å¯¹è¯æ¡†
            if hasattr(self, 'recording_dialog') and self.recording_dialog.winfo_exists():
                self.recording_dialog.destroy()
            
            if not self.recorded_audio:
                messagebox.showwarning("è­¦å‘Š", "æ²¡æœ‰å½•åˆ¶åˆ°éŸ³é¢‘æ•°æ®")
                return
            
            # åˆå¹¶å½•éŸ³æ•°æ®
            audio_data = np.concatenate(self.recorded_audio, axis=0)
            
            # ä¿å­˜å½•éŸ³æ–‡ä»¶
            recorded_file_path = config.get_temp_file(self.parent.workflow.pid, "wav")
            sf.write(recorded_file_path, audio_data, self.sample_rate)
            
            print(f"âœ“ å½•éŸ³ä¿å­˜åˆ°: {recorded_file_path}")
            print(f"âœ“ å½•éŸ³æ—¶é•¿: {len(audio_data) / self.sample_rate:.2f} ç§’")
            
            # è®¾ç½®ä¸ºæºéŸ³é¢‘è·¯å¾„
            if self.source_audio_path:
                safe_remove(self.source_audio_path)  # æ¸…ç†ä¹‹å‰çš„éŸ³é¢‘æ–‡ä»¶
            
            self.source_audio_path = recorded_file_path
            self.source_video_path = self.workflow.ffmpeg_processor.add_audio_to_video(self.source_video_path, self.source_audio_path, True, True)
            
            # æ›´æ–°éŸ³é¢‘æ—¶é•¿å’Œæ—¶é—´é€‰æ‹©å™¨
            self.audio_duration = self.workflow.ffmpeg_audio_processor.get_duration(self.source_audio_path)
            self._set_start_time(0.0)
            self._set_end_time(self.audio_duration)
            
            # æ›´æ–°æ—¶é—´é€‰æ‹©å™¨çš„æœ€å¤§å€¼
            for widget in self.dialog.winfo_children():
                if isinstance(widget, ttk.Frame):
                    for child in widget.winfo_children():
                        if isinstance(child, ttk.Spinbox):
                            try:
                                child.configure(to=self.audio_duration)
                            except:
                                pass
            # é‡æ–°ç»˜åˆ¶æ³¢å½¢
            self.draw_waveform_placeholder()
            
            messagebox.showinfo("æˆåŠŸ", f"å½•éŸ³å®Œæˆï¼\næ–‡ä»¶ä¿å­˜åˆ°: {os.path.basename(recorded_file_path)}\næ—¶é•¿: {self.audio_duration:.2f} ç§’")
            
        except Exception as e:
            messagebox.showerror("é”™è¯¯", f"ä¿å­˜å½•éŸ³å¤±è´¥: {str(e)}")
            print(f"ä¿å­˜å½•éŸ³é”™è¯¯: {e}")



    def confirm_replacement(self):
        self.confirm_edit_timeline_change()
        video_path = self.source_video_path
        
        v = self.current_scene.get(self.video_field, None)
        a = self.current_scene.get(self.audio_field, None)
        i = self.current_scene.get(self.image_field, None)
        if i != self.source_image_path:
            refresh_scene_media(self.current_scene, self.image_field, ".webp", self.source_image_path, True)
        if a != self.source_audio_path and self.workflow.ffmpeg_audio_processor.get_duration(a) != self.audio_duration:
            refresh_scene_media(self.current_scene, self.audio_field, ".wav", self.source_audio_path, True)
        if v != self.source_video_path and self.workflow.ffmpeg_processor.get_duration(v) != self.audio_duration:
            #v = self.workflow.ffmpeg_processor.add_audio_to_video(self.source_video_path, self.source_audio_path, True)
            refresh_scene_media(self.current_scene, self.video_field, ".mp4", self.source_video_path, True)

        # æ‰§è¡ŒéŸ³è§†é¢‘åˆ‡å‰²ï¼ˆåªæœ‰åœ¨è¾¹ç•Œè¢«æ‰‹åŠ¨è°ƒæ•´åæ‰æ‰§è¡Œï¼‰
        # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ‡å‰²ï¼š
        # 1. æœ‰å¤šä¸ªåœºæ™¯
        # 2. åœºæ™¯çš„è¾¹ç•Œä¸æ˜¯é»˜è®¤çš„ï¼ˆå³è¢«æ‰‹åŠ¨ç¼–è¾‘è¿‡ï¼‰

        if self.media_type != "clip" or len(self.audio_json) == 1:
            scene = self.audio_json[0]
            start = scene.get('start', 0)
            end = scene.get('end', self.audio_duration)
            # å¦‚æœæ˜¯å®Œæ•´éŸ³é¢‘ï¼ˆä»0åˆ°ç»“æŸï¼‰ï¼Œä¸éœ€è¦åˆ‡å‰²
            if abs(start) > 0.1 or abs(end - self.audio_duration) > 0.1:
                clip_wav = self.workflow.ffmpeg_audio_processor.audio_cut_fade(self.source_audio_path, start, end - start)
                refresh_scene_media(self.current_scene, self.SPEAKER_KEY+"_audio", ".wav", clip_wav, True)
                refresh_scene_media(self.current_scene, self.audio_field, ".wav", clip_wav, True)
                v = self.workflow.ffmpeg_processor.trim_video(self.source_video_path, start, end)
                refresh_scene_media(self.current_scene, self.video_field, ".mp4", v, True)
            else:
                v = self.source_video_path

            first_image = self.current_scene.get(self.image_field, None)
            if  not first_image or not os.path.exists(first_image):
                first_image = self.workflow.ffmpeg_processor.extract_frame(v, True)
                refresh_scene_media(self.current_scene, self.image_field, ".webp", first_image)
            last_image = self.workflow.ffmpeg_processor.extract_frame(v, False)
            refresh_scene_media(self.current_scene, self.image_field+"_last", ".webp", last_image, True)

        elif self.clip_multiple_audio_changed():
            print(f"âœ“ ç¡®è®¤å‰ªè¾‘åŒºé—´ï¼Œå…± {len(self.audio_json)} ä¸ªåœºæ™¯ï¼Œå¼€å§‹åˆ‡å‰²éŸ³è§†é¢‘...")
            for i, item in enumerate(self.audio_json):
                duration = item.get("duration", 0)
                if duration <= 0:
                    print(f"âš ï¸ åœºæ™¯ {i+1} duration={duration}ï¼Œè·³è¿‡")
                    continue
                clip_wav = self.workflow.ffmpeg_audio_processor.audio_cut_fade(self.source_audio_path, item["start"], item["duration"])
                olda, item["speaker_audio"] = refresh_scene_media(item, "clip_audio", ".wav", clip_wav)
                v = self.workflow.ffmpeg_processor.trim_video(self.source_video_path, item["start"], item["end"])
                refresh_scene_media(item, "clip", ".mp4", v)
                first_image = item.get(self.image_field, None)
                if  not first_image or not os.path.exists(first_image):
                    first_image = self.workflow.ffmpeg_processor.extract_frame(v, True)
                    refresh_scene_media(item, self.image_field, ".webp", first_image)
                last_image = self.workflow.ffmpeg_processor.extract_frame(v, False)
                refresh_scene_media(item, self.image_field+"_last", ".webp", last_image, True)

            print(f"âœ“ éŸ³è§†é¢‘åˆ‡å‰²å®Œæˆ")
        else:
            print(f"â„¹ï¸ è¾¹ç•Œæœªè¢«æ‰‹åŠ¨è°ƒæ•´ï¼Œæ— éœ€åˆ‡å‰²")
        
        self.result = {
            'audio_json': self.audio_json,
            'transcribe_way': self.transcribe_way
        }
        
        self.close_dialog()


    def clip_multiple_audio_changed(self):
        if self.media_type != "clip" or len(self.audio_json) == 1:
            return False
        
        # æ£€æŸ¥åœºæ™¯æ˜¯å¦å·²ç»æœ‰ç‹¬ç«‹çš„éŸ³é¢‘æ–‡ä»¶
        scenes_need_cutting = 0
        for i, scene in enumerate(self.audio_json):
            # æ£€æŸ¥æ˜¯å¦æœ‰ speaker_audio å­—æ®µ
            scene_audio = scene.get(self.SPEAKER_KEY+"_audio", None)
            if not scene_audio or not os.path.exists(scene_audio):
                # æ²¡æœ‰éŸ³é¢‘æ–‡ä»¶æˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦åˆ‡å‰²
                scenes_need_cutting += 1
            elif scene_audio == self.source_audio_path:
                # éŸ³é¢‘æ–‡ä»¶å°±æ˜¯æºæ–‡ä»¶ï¼Œè¯´æ˜æ²¡æœ‰è¢«åˆ‡å‰²è¿‡ï¼Œéœ€è¦åˆ‡å‰²
                scenes_need_cutting += 1
        
        # å¦‚æœè‡³å°‘æœ‰ä¸€ä¸ªåœºæ™¯éœ€è¦åˆ‡å‰²ï¼Œå°±è¿”å› True
        if scenes_need_cutting > 0:
            print(f"ğŸ“Š æ£€æµ‹åˆ° {scenes_need_cutting}/{len(self.audio_json)} ä¸ªåœºæ™¯éœ€è¦åˆ‡å‰²")
            return True
        
        print(f"ğŸ“Š æ‰€æœ‰ {len(self.audio_json)} ä¸ªåœºæ™¯éƒ½å·²æœ‰ç‹¬ç«‹éŸ³é¢‘ï¼Œæ— éœ€åˆ‡å‰²")
        return False



    def cancel(self):
        """Cancel the operation"""
        self.result = {'confirmed': False}
        self.close_dialog()
    


    def close_dialog(self):
        """Close the dialog and cleanup"""
        # Stop recording if in progress
        if self.recording:
            self.recording = False
            if hasattr(self, 'recording_dialog') and self.recording_dialog.winfo_exists():
                self.recording_dialog.destroy()
        
        # Stop all playback and reset states
        self.av_playing = False
        self.av_playing = False
        self.av_paused = False
        self.playback_start_time = None
        self.pause_accumulated_time = 0.0
        # Stop audio
        if self.source_audio_path:
            pygame.mixer.music.stop()
        
        # Cleanup video resources
        if self.source_video_path:
            # Cancel video frame updates
            if self.video_after_id:
                self.dialog.after_cancel(self.video_after_id)
            
            # Release video capture
            if self.video_cap:
                self.video_cap.release()
            
        self.image_references.clear()
        
        # Close dialog
        self.dialog.destroy()
    


    def start_time_update_thread(self):
        """Start a thread to update playback time"""
        def update_time():
            iteration = 0
            # ç¬¬ä¸€æ¬¡è¿­ä»£æ—¶ç­‰å¾…éŸ³é¢‘ç¨³å®šï¼Œç„¶åé‡ç½®æ—¶é—´åŸºå‡†
            first_iteration = True
            
            while self.av_playing and not self.av_paused:
                try:
                    # å†æ¬¡æ£€æŸ¥çŠ¶æ€ï¼Œé˜²æ­¢æš‚åœåä»ç„¶æ›´æ–°æ—¶é—´
                    if not self.av_playing or self.av_paused:
                        print(f"DEBUG: æ›´æ–°çº¿ç¨‹åœ¨è¿­ä»£ {iteration} æ—¶æ£€æµ‹åˆ°çŠ¶æ€æ”¹å˜ï¼Œé€€å‡º")
                        break
                    
                    # ç¬¬ä¸€æ¬¡è¿­ä»£ï¼šç­‰å¾…éŸ³é¢‘ç¨³å®šåé‡ç½®åŸºå‡†
                    if first_iteration:
                        print(f"DEBUG: é¦–æ¬¡è¿­ä»£ï¼Œç­‰å¾…éŸ³é¢‘ç¨³å®š...")
                        time.sleep(0.15)  # ç­‰å¾…150ms
                        # åŒæ­¥æ›´æ–°åŸºå‡†æ—¶é—´ï¼Œé¿å…ç´¯ç§¯è¯¯å·®
                        self.pause_accumulated_time = self.current_playback_time
                        self.playback_start_time = time.time()
                        first_iteration = False
                        print(f"DEBUG: æ—¶é—´åŸºå‡†å·²åŒæ­¥: pause_acc={self.pause_accumulated_time:.2f}s, start_time={self.playback_start_time:.3f}")
                        iteration += 1
                        continue
                        
                    if self.playback_start_time is not None:
                        elapsed_since_start = time.time() - self.playback_start_time
                        
                        # åªåœ¨æ’­æ”¾çŠ¶æ€ä¸‹æ›´æ–°æ—¶é—´
                        if self.av_playing and not self.av_paused:
                            calculated_time = self.pause_accumulated_time + elapsed_since_start
                            
                            # é˜²æ­¢æ—¶é—´å¼‚å¸¸è·³è·ƒ
                            time_diff = calculated_time - self.current_playback_time
                            if time_diff < -0.2:
                                # æ—¶é—´å‘åè·³è¶…è¿‡200msï¼Œå¯èƒ½æ˜¯bugï¼Œé‡ç½®åŸºå‡†
                                print(f"âš ï¸ æ—¶é—´å‘åè·³è·ƒ: {self.current_playback_time:.2f}s -> {calculated_time:.2f}sï¼Œé‡ç½®åŸºå‡†")
                                self.pause_accumulated_time = self.current_playback_time
                                self.playback_start_time = time.time()
                            elif time_diff > 0.3:
                                # æ—¶é—´å‘å‰è·³è¶…è¿‡300msï¼Œå¯èƒ½æ˜¯éŸ³é¢‘å»¶è¿Ÿï¼Œé‡ç½®åŸºå‡†
                                print(f"âš ï¸ æ—¶é—´å‘å‰è·³è·ƒ: {self.current_playback_time:.2f}s -> {calculated_time:.2f}sï¼Œé‡ç½®åŸºå‡†")
                                self.pause_accumulated_time = self.current_playback_time
                                self.playback_start_time = time.time()
                            else:
                                # æ­£å¸¸æ›´æ–°
                                self.current_playback_time = calculated_time
                            
                            # æ¯ç§’æ‰“å°ä¸€æ¬¡è°ƒè¯•ä¿¡æ¯
                            if iteration % 10 == 0:
                                print(f"DEBUG: æ›´æ–° {iteration}: pause_acc={self.pause_accumulated_time:.2f}s + elapsed={elapsed_since_start:.2f}s = {self.current_playback_time:.2f}s")
                            
                            # Update display in main thread
                            self.dialog.after(0, self.update_play_time_display)
                            
                            # Check if we've reached the end
                            if self.current_playback_time >= self.audio_duration + 0.5:
                                print(f"â¹ åˆ°è¾¾ç»“æŸ: {self.current_playback_time:.2f}s >= {self.audio_duration:.2f}s")
                                if self.av_playing and not self.av_paused:
                                    self.dialog.after(0, self.stop_playback)
                                break
                        else:
                            print(f"DEBUG: æ›´æ–°çº¿ç¨‹æ£€æµ‹åˆ°çŠ¶æ€æ”¹å˜ï¼Œé€€å‡º")
                            break
                    
                    iteration += 1
                    time.sleep(0.1)  # Update every 100ms
                except Exception as e:
                    print(f"ERROR: æ›´æ–°çº¿ç¨‹å¼‚å¸¸: {e}")
                    import traceback
                    traceback.print_exc()
                    break
            
            print(f"DEBUG: æ›´æ–°çº¿ç¨‹å·²é€€å‡ºï¼Œè¿­ä»£: {iteration}")
        
        # Start the update thread
        if self.av_playing:
            print(f"DEBUG: å¯åŠ¨æ›´æ–°çº¿ç¨‹ï¼Œpause_acc={self.pause_accumulated_time:.2f}s, current={self.current_playback_time:.2f}s")
            threading.Thread(target=update_time, daemon=True).start()

    

    def load_video_first_frame(self):
        """Load and display the first frame of the video in preview canvas"""
        if not self.source_video_path:
            return
            
        try:
            # Clear canvas first
            self.preview_canvas.delete("all")
            
            # Open video file
            cap = cv2.VideoCapture(self.source_video_path)
            
            if not cap.isOpened():
                cap.release()
                self.preview_canvas.create_text(
                    self.preview_canvas.winfo_width()//2, 
                    self.preview_canvas.winfo_height()//2,
                    text="æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶", fill="white", font=("Arial", 12)
                )
                return
            
            # Get video dimensions
            self.video_original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            self.video_original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            # Update crop controls max values
            if self.video_original_width and self.video_original_height:
                # Update spinbox max values
                self._update_crop_spinbox_max()
            
            # Read first frame
            ret, frame = cap.read()
            cap.release()
            
            if ret and frame is not None:
                # Convert BGR to RGB
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # Get canvas dimensions
                canvas_width = self.preview_canvas.winfo_width()
                canvas_height = self.preview_canvas.winfo_height()
                
                # If canvas is not ready, use default size
                if canvas_width <= 1 or canvas_height <= 1:
                    canvas_width, canvas_height = 640, 360
                
                # Calculate aspect ratio and resize
                height, width = frame_rgb.shape[:2]
                aspect_ratio = width / height
                
                if canvas_width / canvas_height > aspect_ratio:
                    new_height = canvas_height - 20  # Leave some margin
                    new_width = int(new_height * aspect_ratio)
                else:
                    new_width = canvas_width - 20  # Leave some margin
                    new_height = int(new_width / aspect_ratio)
                
                # Resize frame
                frame_resized = cv2.resize(frame_rgb, (new_width, new_height))
                
                # Convert to PIL Image and then to PhotoImage
                pil_image = Image.fromarray(frame_resized)
                self.first_frame_photo = ImageTk.PhotoImage(pil_image)
                
                # Add to image references to prevent garbage collection
                if not hasattr(self, 'image_references'):
                    self.image_references = []
                self.image_references.append(self.first_frame_photo)
                
                # Display the image in canvas center
                self.preview_canvas.create_image(
                    canvas_width//2, canvas_height//2, 
                    image=self.first_frame_photo, anchor=tk.CENTER
                )
                
                # Add helpful text below the frame
                self.preview_canvas.create_text(
                    canvas_width//2, canvas_height//2 + new_height//2 + 20,
                    text="ç‚¹å‡» 'â–¶ æ’­æ”¾' å¼€å§‹æ’­æ”¾è§†é¢‘", 
                    fill="white", font=("Arial", 12)
                )
                
                self.preview_canvas.create_text(
                    canvas_width//2, canvas_height//2 + new_height//2 + 40,
                    text="ğŸ’¡ è§†é¢‘ç¬¬ä¸€å¸§é¢„è§ˆ", 
                    fill="gray", font=("Arial", 10)
                )
                
            else:
                self.preview_canvas.create_text(
                    self.preview_canvas.winfo_width()//2, 
                    self.preview_canvas.winfo_height()//2,
                    text="æ— æ³•è¯»å–è§†é¢‘ç¬¬ä¸€å¸§", fill="white", font=("Arial", 12)
                )
                
        except Exception as e:
            print(f"âš ï¸ åŠ è½½è§†é¢‘ç¬¬ä¸€å¸§å¤±è´¥: {e}")
            self.preview_canvas.create_text(
                self.preview_canvas.winfo_width()//2, 
                self.preview_canvas.winfo_height()//2,
                text=f"åŠ è½½è§†é¢‘å¤±è´¥: {str(e)}", fill="red", font=("Arial", 10)
            )



    def update_video_frame(self):
        """Update video frame in preview canvas with audio sync"""
        if not self.av_playing or not self.video_cap:
            return
        
        try:
            # Calculate target time based on actual elapsed time (audio sync)
            if self.playback_start_time is not None:
                elapsed_since_start = time.time() - self.playback_start_time
                target_time = self.pause_accumulated_time + elapsed_since_start
                
                # å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœè®¡ç®—çš„æ—¶é—´æ˜æ˜¾ä¸åˆç†ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
                if target_time < 0 or target_time > self.audio_duration + 1.0:
                    print(f"âš ï¸ è§†é¢‘ç›®æ ‡æ—¶é—´å¼‚å¸¸: {target_time:.2f}s (pause_acc={self.pause_accumulated_time:.2f}s, elapsed={elapsed_since_start:.2f}s)")
                    target_time = self.current_playback_time
            else:
                target_time = self.current_playback_time
            
            fps = self.video_cap.get(cv2.CAP_PROP_FPS) or 30  # Default to 30 if FPS is unknown
            target_frame = int(target_time * fps)
            current_frame = int(self.video_cap.get(cv2.CAP_PROP_POS_FRAMES))
            
            # Calculate frame difference and skip frames if needed
            frame_diff = target_frame - current_frame
            
            if frame_diff > 1:
                # Video is behind audio, skip frames to catch up
                print(f"ğŸ¬ è§†é¢‘åŒæ­¥: è·³è¿‡ {frame_diff - 1} å¸§ (ç›®æ ‡å¸§: {target_frame}, å½“å‰å¸§: {current_frame})")
                self.video_cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)
                ret, frame = self.video_cap.read()
            elif frame_diff < -1:
                # Video is ahead of audio (unlikely but handle it)
                print(f"ğŸ¬ è§†é¢‘åŒæ­¥: è§†é¢‘è¶…å‰éŸ³é¢‘ {abs(frame_diff)} å¸§")
                self.video_cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)
                ret, frame = self.video_cap.read()
            else:
                # Normal playback - read next frame
                ret, frame = self.video_cap.read()
            
            if ret:
                # ä¸è¦åœ¨è¿™é‡Œæ›´æ–° current_playback_timeï¼Œå› ä¸ºæ›´æ–°çº¿ç¨‹å·²ç»åœ¨å¤„ç†
                # è¿™é‡Œåªè´Ÿè´£æ˜¾ç¤ºæ­£ç¡®çš„è§†é¢‘å¸§
                current_frame = self.video_cap.get(cv2.CAP_PROP_POS_FRAMES)
                # self.current_playback_time ç”±æ›´æ–°çº¿ç¨‹ç»´æŠ¤ï¼Œè¿™é‡Œä¸ä¿®æ”¹
                
                # ä¸éœ€è¦åœ¨è¿™é‡Œæ›´æ–°æ˜¾ç¤ºï¼Œæ›´æ–°çº¿ç¨‹ä¼šå¤„ç†
                # self.update_play_time_display()
                
                # Check if we're still in the selected time range
                try:
                    end_time = self._get_end_time()
                    # æ·»åŠ  0.5 ç§’å®¹å·®ï¼Œé¿å…åœ¨æ¥è¿‘ç»“æŸæ—¶è¯¯åˆ¤
                    if self.current_playback_time >= end_time + 0.5:
                        # Reached end of selected range
                        print(f"â¹ è§†é¢‘æ’­æ”¾åˆ°è¾¾ç»“æŸä½ç½®: {self.current_playback_time:.2f}s >= {end_time:.2f}s")
                        self.stop_playback()
                        return
                except:
                    pass
                
                # Convert frame to display in canvas
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                height, width = frame.shape[:2]
                
                # Resize frame to fit canvas
                canvas_width = self.preview_canvas.winfo_width()
                canvas_height = self.preview_canvas.winfo_height()
                if canvas_width > 1 and canvas_height > 1:
                    aspect_ratio = width / height
                    if canvas_width / canvas_height > aspect_ratio:
                        new_height = canvas_height
                        new_width = int(canvas_height * aspect_ratio)
                    else:
                        new_width = canvas_width
                        new_height = int(canvas_width / aspect_ratio)
                    
                    frame = cv2.resize(frame, (new_width, new_height))
                    
                    # Convert to PhotoImage and display
                    image = Image.fromarray(frame)
                    photo = ImageTk.PhotoImage(image)
                    
                    # Manage image references to prevent garbage collection
                    self.image_references.append(photo)
                    # Keep only the last few images to avoid memory buildup
                    if len(self.image_references) > 5:
                        self.image_references.pop(0)
                    
                    self.preview_canvas.delete("all")
                    self.preview_canvas.create_image(canvas_width//2, canvas_height//2, 
                                                   image=photo, anchor=tk.CENTER)
                
                # Calculate dynamic delay for next frame update
                # Use shorter delay for better sync accuracy
                next_frame_time = (current_frame + 1) / fps
                time_until_next_frame = next_frame_time - target_time
                
                if time_until_next_frame > 0:
                    delay = max(int(time_until_next_frame * 1000), 16)  # Minimum 16ms (~60 FPS max)
                else:
                    delay = 16  # Immediate update if we're behind
                
                self.video_after_id = self.dialog.after(delay, self.update_video_frame)
            else:
                # End of video
                self.stop_playback()
        except Exception as e:
            print(f"Video playback error: {e}")
            self.stop_playback()



    def display_image_on_canvas(self):
        """åœ¨canvasä¸Šæ˜¾ç¤ºå›¾ç‰‡"""
        try:
            img = Image.open(self.source_image_path)
            
            # æ¸…ç©º canvas
            self.image_canvas.delete("all")
            
            # è·å– canvas çš„å®é™…å¤§å°
            canvas_width = self.image_canvas.winfo_width()
            canvas_height = self.image_canvas.winfo_height()
            
            # å¦‚æœ canvas è¿˜æ²¡æœ‰å‡†å¤‡å¥½ï¼Œä½¿ç”¨é»˜è®¤å¤§å°
            if canvas_width <= 1 or canvas_height <= 1:
                canvas_width, canvas_height = 600, 600
            
            # è®¡ç®—å®½é«˜æ¯”å¹¶è°ƒæ•´å¤§å°ä»¥å¡«æ»¡ canvas
            img_width, img_height = img.size
            aspect_ratio = img_width / img_height
            
            # è®¡ç®—æ–°çš„å°ºå¯¸ï¼Œç•™ä¸€äº›è¾¹è·
            margin = 20
            available_width = canvas_width - margin
            available_height = canvas_height - margin
            
            if available_width / available_height > aspect_ratio:
                # canvas æ›´å®½ï¼Œä»¥é«˜åº¦ä¸ºåŸºå‡†
                new_height = available_height
                new_width = int(new_height * aspect_ratio)
            else:
                # canvas æ›´é«˜ï¼Œä»¥å®½åº¦ä¸ºåŸºå‡†
                new_width = available_width
                new_height = int(new_width / aspect_ratio)
            
            # è°ƒæ•´å›¾ç‰‡å¤§å°
            img_resized = img.resize((new_width, new_height), Image.LANCZOS)
            photo = ImageTk.PhotoImage(img_resized)
            
            # åœ¨ canvas ä¸­å¿ƒæ˜¾ç¤ºå›¾ç‰‡
            x = canvas_width // 2
            y = canvas_height // 2
            self.image_canvas.create_image(x, y, image=photo, anchor=tk.CENTER, tags="image")
            
            # ä¿æŒå¼•ç”¨ä»¥é˜²æ­¢è¢«åƒåœ¾å›æ”¶
            self.image_canvas.image = photo
            
        except Exception as e:
            print(f"æ˜¾ç¤ºå›¾ç‰‡å¤±è´¥: {e}")


    def _extract_speaker_from_json(self):
        """ä» fresh_json_text ä¸­æå– narrator å’Œ speaker çš„å€¼ï¼Œå¹¶è®¾ç½®åˆ°ä¸‹æ‹‰æ¡†"""
        if self._updating_from_json:
            return
        
        try:
            # è·å– fresh_json_text çš„å†…å®¹
            fresh_text = self.fresh_json_text.get(1.0, tk.END).strip()
            if not fresh_text:
                return
            
            # å°è¯•è§£æä¸º JSON
            fresh_json = parse_json_from_text(fresh_text)
            if fresh_json is None or not isinstance(fresh_json, list) or len(fresh_json) == 0:
                return
            
            # è®¾ç½®æ ‡å¿—ï¼Œé˜²æ­¢å¾ªç¯æ›´æ–°
            self._updating_from_json = True
            
            speaker_found = False
            for item in fresh_json:
                if not isinstance(item, dict):
                    continue
                if not speaker_found and self.SPEAKER_KEY in item:
                    speaker_value = str(item[self.SPEAKER_KEY]).strip()
                    if speaker_value:
                        for actor in self.ACTORS:
                            if actor and speaker_value.startswith(actor):
                                try:
                                    self.speaker.set(actor)
                                    speaker_found = True
                                    break
                                except:
                                    pass
                if speaker_found:
                    break
            
        except Exception as e:
            print(f"âš ï¸ ä» JSON æå– narrator/speaker å¤±è´¥: {e}")
        finally:
            # é‡ç½®æ ‡å¿—
            self._updating_from_json = False


    def _update_fresh_json_text(self):
        """æ›´æ–° fresh_json_text çš„å†…å®¹å¹¶è§¦å‘æå– narrator å’Œ speaker å€¼"""
        self.fresh_json_text.delete(1.0, tk.END)
        self.fresh_json_text.insert(1.0, json.dumps(self.audio_json, indent=2, ensure_ascii=False))
        # è§¦å‘æå– narrator å’Œ speaker å€¼
        self._extract_speaker_from_json()
        # åˆ·æ–°å‰ªè¾‘æ—¶é—´è½´
        self.refresh_edit_timeline()


    def on_speaker_changed(self, event=None):
        if self._updating_from_json:
            return
        
        try:
            person_value = self.speaker.get()
            if not person_value:
                return

            # è®¾ç½®æ ‡å¿—ï¼Œé¿å…è§¦å‘æ–‡æœ¬æ”¹å˜äº‹ä»¶
            self._updating_from_json = True
            
            for item in self.audio_json:
                if isinstance(item, dict):
                    person = item.get(self.SPEAKER_KEY, "")
                    for actor in self.ACTORS:
                        person = person.replace(actor, "")
                    item[self.SPEAKER_KEY] = person_value + person

            self._update_fresh_json_text()
        
        except Exception as e:
            print(f"âš ï¸ æ›´æ–° speaker å­—æ®µå¤±è´¥: {e}")
        finally:
            self._updating_from_json = False


    def on_animation_changed(self, *args):
        """å¤„ç†åŠ¨ç”»é€‰é¡¹å˜åŒ–"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡å’ŒéŸ³é¢‘
            if not self.source_image_path or not os.path.exists(self.source_image_path):
                print("âš ï¸ æ²¡æœ‰å›¾ç‰‡ï¼Œæ— æ³•é‡æ–°ç”Ÿæˆè§†é¢‘")
                return
            
            if not self.source_audio_path or not os.path.exists(self.source_audio_path):
                print("âš ï¸ æ²¡æœ‰éŸ³é¢‘ï¼Œæ— æ³•é‡æ–°ç”Ÿæˆè§†é¢‘")
                return
            
            # è·å–æ–°çš„åŠ¨ç”»é€‰æ‹©
            animation_choice = self.animation_var.get()
            self.animation_choice = animation_choice
            
            print(f"ğŸ¬ åŠ¨ç”»é€‰é¡¹å˜åŒ–: {animation_choice} ({'é™æ­¢' if animation_choice == 1 else 'å·¦' if animation_choice == 2 else 'å³' if animation_choice == 3 else 'åŠ¨ç”»'})")
            
            # åœæ­¢å½“å‰æ’­æ”¾
            if self.av_playing:
                self.stop_playback()
            
            # é‡Šæ”¾æ—§çš„è§†é¢‘æ•è·å¯¹è±¡
            if self.video_cap:
                self.video_cap.release()
                self.video_cap = None
                print("ğŸ”„ å·²é‡Šæ”¾æ—§è§†é¢‘èµ„æº")
            
            # é‡æ–°ç”Ÿæˆè§†é¢‘
            print(f"ğŸ”„ æ­£åœ¨é‡æ–°ç”Ÿæˆè§†é¢‘...")
            self.source_video_path = self.workflow.ffmpeg_processor.image_audio_to_video(
                self.source_image_path, 
                self.source_audio_path, 
                animation_choice
            )
            
            if self.source_video_path and os.path.exists(self.source_video_path):
                print(f"âœ“ è§†é¢‘é‡æ–°ç”ŸæˆæˆåŠŸ: {self.source_video_path}")
                
                # é‡ç½®æ’­æ”¾çŠ¶æ€
                self.current_playback_time = 0.0
                self.pause_accumulated_time = 0.0
                self.playback_start_time = None
                
                # åˆ·æ–°è§†é¢‘æ˜¾ç¤º
                self.dialog.after(100, self.load_video_first_frame)
            else:
                print(f"âŒ è§†é¢‘é‡æ–°ç”Ÿæˆå¤±è´¥")
                
        except Exception as e:
            print(f"âŒ å¤„ç†åŠ¨ç”»å˜åŒ–å¤±è´¥: {e}")



    def on_video_dnd_drop(self, event):
        """å¤„ç†è§†é¢‘æ‹–æ”¾"""
        file_path = event.data.strip('{}').strip('"')
        if is_video_file(file_path):
            self.handle_new_media(file_path)



    def on_image_dnd_drop(self, event):
        """å¤„ç†å›¾ç‰‡æ‹–æ”¾"""
        file_path = event.data.strip('{}').strip('"')
        if is_image_file(file_path):
            self.handle_new_media(file_path)


    def on_audio_dnd_drop(self, event):
        """å¤„ç†éŸ³é¢‘æ‹–æ”¾"""
        file_path = event.data.strip('{}').strip('"')
        if is_audio_file(file_path):
            self.handle_new_media(file_path)


    def process_new_media(self, av_path):
        if is_image_file(av_path):
            self.source_image_path = av_path
            self.source_video_path = self.workflow.ffmpeg_processor.image_audio_to_video(self.source_image_path, self.source_audio_path)
        elif is_audio_file(av_path):
            self.source_audio_path = self.workflow.ffmpeg_audio_processor.to_wav(av_path)
            self.source_video_path = self.workflow.ffmpeg_processor.add_audio_to_video(self.source_video_path, self.source_audio_path, True, True)
        elif is_video_file(av_path):
            self.source_video_path = av_path  # self.workflow.ffmpeg_processor.resize_video(av_path, width=self.workflow.ffmpeg_processor.width, height=None)
            if self.replace_media_audio=="keep":
                self.source_audio_path = self.workflow.ffmpeg_audio_processor.extract_audio_from_video(self.source_video_path)
            else:
                self.source_video_path = self.workflow.ffmpeg_processor.add_audio_to_video(self.source_video_path, self.source_audio_path, True, True)

        self.audio_duration = self.workflow.ffmpeg_audio_processor.get_duration(self.source_audio_path)
        self._set_end_time(self.audio_duration)
        self._set_start_time(0.0)


    def handle_new_media(self, av_path):
        if not av_path:
            return
        # åœæ­¢å½“å‰æ’­æ”¾
        if self.av_playing:
            self.stop_playback()
        # é‡Šæ”¾æ—§çš„è§†é¢‘æ•è·å¯¹è±¡
        if self.video_cap:
            self.video_cap.release()
            self.video_cap = None
        # æ¸…é™¤è£å‰ªé€‰æ‹©
        self.clear_crop_selection()
        # é‡ç½®è§†é¢‘å°ºå¯¸
        self.video_original_width = None
        self.video_original_height = None

        self.process_new_media(av_path)
        # é‡ç½®æ’­æ”¾çŠ¶æ€
        self.current_playback_time = 0.0
        self.pause_accumulated_time = 0.0
        self.playback_start_time = None

        self.update_play_time_display()
        self.update_duration_display()

        self.display_image_on_canvas()
        # åˆ·æ–°è§†é¢‘æ˜¾ç¤ºï¼ˆå»¶è¿ŸåŠ è½½ç¡®ä¿è§†é¢‘èµ„æºå‡†å¤‡å°±ç»ªï¼‰
        self.dialog.after(100, self.load_video_first_frame)

    
    def on_canvas_click(self, event):
        """Handle mouse click on preview canvas to start crop selection"""
        if not self.source_video_path:
            return
        # Get canvas coordinates
        canvas_x = event.x
        canvas_y = event.y
        # Convert canvas coordinates to video coordinates
        video_x, video_y = self.canvas_to_video_coords(canvas_x, canvas_y)
        if video_x is not None and video_y is not None:
            self.selecting = True
            self.selection_start_x = canvas_x
            self.selection_start_y = canvas_y
            # Clear previous selection
            if self.selection_rect:
                self.preview_canvas.delete(self.selection_rect)
            self.selection_rect = None

    
    def on_canvas_drag(self, event):
        """Handle mouse drag on preview canvas to update crop selection"""
        if not self.selecting:
            return
        # Get current canvas coordinates
        canvas_x = event.x
        canvas_y = event.y
        # Update selection rectangle
        if self.selection_rect:
            self.preview_canvas.delete(self.selection_rect)
        # Draw selection rectangle
        x1 = min(self.selection_start_x, canvas_x)
        y1 = min(self.selection_start_y, canvas_y)
        x2 = max(self.selection_start_x, canvas_x)
        y2 = max(self.selection_start_y, canvas_y)
        self.selection_rect = self.preview_canvas.create_rectangle(
            x1, y1, x2, y2,
            outline='yellow', width=2, dash=(5, 5)
        )

    
    def on_canvas_release(self, event):
        """Handle mouse release on preview canvas to finalize crop selection"""
        if not self.selecting:
            return
        
        self.selecting = False
        # Get canvas dimensions
        canvas_width = self.preview_canvas.winfo_width()
        canvas_height = self.preview_canvas.winfo_height()
        if canvas_width <= 1 or canvas_height <= 1:
            return
        # Get canvas coordinates
        canvas_x = event.x
        canvas_y = event.y
        # Calculate start and end canvas coordinates
        start_canvas_x = min(self.selection_start_x, canvas_x)
        start_canvas_y = min(self.selection_start_y, canvas_y)
        end_canvas_x = max(self.selection_start_x, canvas_x)
        end_canvas_y = max(self.selection_start_y, canvas_y)
        # Default canvas coordinates
        default_start_x = 0
        default_start_y = 0
        default_end_x = canvas_width
        default_end_y = canvas_height
        # Get image bounds on canvas
        image_bounds = self.get_image_bounds()
        if not image_bounds:
            return
        # Check if each coordinate is within canvas bounds (0 to canvas_width/height)
        start_x_in_bounds = 0 <= start_canvas_x <= canvas_width
        start_y_in_bounds = 0 <= start_canvas_y <= canvas_height
        end_x_in_bounds = 0 <= end_canvas_x <= canvas_width
        end_y_in_bounds = 0 <= end_canvas_y <= canvas_height
        # Convert each coordinate individually
        # Use actual coordinate if in canvas bounds, otherwise use default
        # Then clamp to image bounds
        start_x_canvas = start_canvas_x if start_x_in_bounds else default_start_x
        start_y_canvas = start_canvas_y if start_y_in_bounds else default_start_y
        # Clamp coordinates to image bounds
        start_x_canvas = max(image_bounds['left'], min(image_bounds['right'], start_x_canvas))
        start_y_canvas = max(image_bounds['top'], min(image_bounds['bottom'], start_y_canvas))
        
        start_x_video, start_y_video = self.canvas_to_video_coords(start_x_canvas, start_y_canvas)
        # If conversion failed, try with image bounds
        if start_x_video is None or start_y_video is None:
            start_x_video, start_y_video = self.canvas_to_video_coords(image_bounds['left'], image_bounds['top'])
        # For end_x and end_y: use end_canvas_x/y if in bounds, else defaults
        end_x_canvas = end_canvas_x if end_x_in_bounds else default_end_x
        end_y_canvas = end_canvas_y if end_y_in_bounds else default_end_y
        # Clamp coordinates to image bounds
        end_x_canvas = max(image_bounds['left'], min(image_bounds['right'], end_x_canvas))
        end_y_canvas = max(image_bounds['top'], min(image_bounds['bottom'], end_y_canvas))
        
        end_x_video, end_y_video = self.canvas_to_video_coords(end_x_canvas, end_y_canvas)
        # If conversion failed, try with image bounds
        if end_x_video is None or end_y_video is None:
            end_x_video, end_y_video = self.canvas_to_video_coords(image_bounds['right'], image_bounds['bottom'])
        
        # Final check - if any conversion still failed, return early
        if start_x_video is None or start_y_video is None or end_x_video is None or end_y_video is None:
            return
        if start_x_video > end_x_video:
            start_x_video, end_x_video = end_x_video, start_x_video
        if start_y_video > end_y_video:
            start_y_video, end_y_video = end_y_video, start_y_video
        # Update crop parameters
        self.crop_start_x = max(0, int(start_x_video))
        self.crop_start_y = max(0, int(start_y_video))
        crop_w = max(1, int(end_x_video - start_x_video))
        crop_h = max(1, int(end_y_video - start_y_video))
        # Store crop dimensions
        self.crop_width = crop_w
        self.crop_height = crop_h
        # Update UI controls
        self.crop_x_var.set(self.crop_start_x)
        self.crop_y_var.set(self.crop_start_y)
        self.crop_width_var.set(crop_w)
        print(f"âœ“ é€‰æ‹©è£å‰ªåŒºåŸŸ: ({self.crop_start_x}, {self.crop_start_y}), å°ºå¯¸: {crop_w}x{crop_h}")

    
    def get_image_bounds(self):
        """Get the actual image bounds on canvas"""
        try:
            # Get displayed image dimensions (from first frame)
            if not hasattr(self, 'first_frame_photo') or not self.first_frame_photo:
                return None
            # Find the image item on canvas
            items = self.preview_canvas.find_all()
            image_item = None
            for item in items:
                if self.preview_canvas.type(item) == 'image':
                    image_item = item
                    break
            if not image_item:
                return None
            # Get image coordinates and dimensions
            coords = self.preview_canvas.coords(image_item)
            img_x = coords[0]
            img_y = coords[1]
            # Get image dimensions from photo
            img_width = self.first_frame_photo.width()
            img_height = self.first_frame_photo.height()
            # Calculate image bounds
            img_left = img_x - img_width // 2
            img_right = img_x + img_width // 2
            img_top = img_y - img_height // 2
            img_bottom = img_y + img_height // 2
            return {
                'left': img_left,
                'right': img_right,
                'top': img_top,
                'bottom': img_bottom
            }
        except Exception as e:
            print(f"âš ï¸ è·å–å›¾åƒè¾¹ç•Œå¤±è´¥: {e}")
            return None


    def canvas_to_video_coords(self, canvas_x, canvas_y):
        """Convert canvas coordinates to video coordinates"""
        if not self.source_video_path or not self.video_original_width or not self.video_original_height:
            return None, None
        
        try:
            # Get canvas dimensions
            canvas_width = self.preview_canvas.winfo_width()
            canvas_height = self.preview_canvas.winfo_height()
            if canvas_width <= 1 or canvas_height <= 1:
                return None, None
            # Get displayed image dimensions (from first frame)
            if not hasattr(self, 'first_frame_photo') or not self.first_frame_photo:
                return None, None
            # Find the image item on canvas
            items = self.preview_canvas.find_all()
            image_item = None
            for item in items:
                if self.preview_canvas.type(item) == 'image':
                    image_item = item
                    break
            if not image_item:
                return None, None
            # Get image coordinates and dimensions
            coords = self.preview_canvas.coords(image_item)
            img_x = coords[0]
            img_y = coords[1]
            # Get image dimensions from photo
            img_width = self.first_frame_photo.width()
            img_height = self.first_frame_photo.height()
            # Calculate image bounds
            img_left = img_x - img_width // 2
            img_right = img_x + img_width // 2
            img_top = img_y - img_height // 2
            img_bottom = img_y + img_height // 2
            # Check if click is within image bounds
            if canvas_x < img_left or canvas_x > img_right or canvas_y < img_top or canvas_y > img_bottom:
                return None, None
            # Convert to relative coordinates (0.0 to 1.0)
            rel_x = (canvas_x - img_left) / img_width
            rel_y = (canvas_y - img_top) / img_height
            # Convert to video coordinates
            video_x = int(rel_x * self.video_original_width)
            video_y = int(rel_y * self.video_original_height)
            return video_x, video_y
            
        except Exception as e:
            print(f"âš ï¸ åæ ‡è½¬æ¢å¤±è´¥: {e}")
            return None, None

    
    def on_crop_params_changed(self, *args):
        """Handle changes to crop parameter controls"""
        try:
            self.crop_start_x = self.crop_x_var.get()
            self.crop_start_y = self.crop_y_var.get()
            
            width_val = self.crop_width_var.get()
            if width_val == 0:
                self.crop_width = None
            else:
                self.crop_width = width_val
                # Calculate height based on aspect ratio if not set
                if self.crop_width and self.video_original_width and self.video_original_height:
                    aspect_ratio = self.video_original_height / self.video_original_width
                    self.crop_height = int(self.crop_width * aspect_ratio)
            
            # Update selection rectangle display
            self.update_crop_selection_display()
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°è£å‰ªå‚æ•°å¤±è´¥: {e}")
    

    
    def update_crop_selection_display(self):
        """Update the visual selection rectangle on canvas"""
        if not self.source_video_path or self.crop_width is None or self.crop_width == 0:
            # Clear selection if no crop is set
            if self.selection_rect:
                self.preview_canvas.delete(self.selection_rect)
                self.selection_rect = None
            return
        
        try:
            # Clear previous selection
            if self.selection_rect:
                self.preview_canvas.delete(self.selection_rect)
            
            # Calculate crop height if not set
            crop_h = self.crop_height
            if not crop_h and self.video_original_width and self.video_original_height:
                # Calculate height based on aspect ratio
                aspect_ratio = self.video_original_height / self.video_original_width
                crop_h = int(self.crop_width * aspect_ratio)
            elif not crop_h:
                crop_h = self.crop_width  # Fallback to square
            
            # Convert video coordinates to canvas coordinates
            canvas_coords = self.video_to_canvas_coords(
                self.crop_start_x, self.crop_start_y,
                self.crop_start_x + self.crop_width,
                self.crop_start_y + crop_h
            )
            
            if canvas_coords:
                x1, y1, x2, y2 = canvas_coords
                self.selection_rect = self.preview_canvas.create_rectangle(
                    x1, y1, x2, y2,
                    outline='yellow', width=2, dash=(5, 5)
                )
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°é€‰æ‹©æ˜¾ç¤ºå¤±è´¥: {e}")
    

    
    def video_to_canvas_coords(self, video_x1, video_y1, video_x2, video_y2):
        """Convert video coordinates to canvas coordinates"""
        if not self.source_video_path or not self.video_original_width or not self.video_original_height:
            return None
        
        try:
            # Get canvas dimensions
            canvas_width = self.preview_canvas.winfo_width()
            canvas_height = self.preview_canvas.winfo_height()
            
            if canvas_width <= 1 or canvas_height <= 1:
                return None
            
            # Get displayed image dimensions
            if not hasattr(self, 'first_frame_photo') or not self.first_frame_photo:
                return None
            
            # Find the image item on canvas
            items = self.preview_canvas.find_all()
            image_item = None
            for item in items:
                if self.preview_canvas.type(item) == 'image':
                    image_item = item
                    break
            
            if not image_item:
                return None
            
            # Get image coordinates
            coords = self.preview_canvas.coords(image_item)
            img_x = coords[0]
            img_y = coords[1]
            
            # Get image dimensions
            img_width = self.first_frame_photo.width()
            img_height = self.first_frame_photo.height()
            
            # Calculate image bounds
            img_left = img_x - img_width // 2
            img_top = img_y - img_height // 2
            
            # Convert to relative coordinates
            rel_x1 = video_x1 / self.video_original_width
            rel_y1 = video_y1 / self.video_original_height
            rel_x2 = video_x2 / self.video_original_width
            rel_y2 = video_y2 / self.video_original_height
            
            # Convert to canvas coordinates
            canvas_x1 = img_left + rel_x1 * img_width
            canvas_y1 = img_top + rel_y1 * img_height
            canvas_x2 = img_left + rel_x2 * img_width
            canvas_y2 = img_top + rel_y2 * img_height
            
            return (canvas_x1, canvas_y1, canvas_x2, canvas_y2)
            
        except Exception as e:
            print(f"âš ï¸ åæ ‡è½¬æ¢å¤±è´¥: {e}")
            return None
    

    
    def clear_crop_selection(self):
        """Clear the crop selection"""
        self.crop_start_x = 0
        self.crop_start_y = 0
        self.crop_width = None
        self.crop_height = None
        
        if self.selection_rect:
            self.preview_canvas.delete(self.selection_rect)
            self.selection_rect = None
        
        self.crop_x_var.set(0)
        self.crop_y_var.set(0)
        self.crop_width_var.set(0)
        
        print("âœ“ å·²æ¸…é™¤è£å‰ªé€‰æ‹©")
    

    
    def _update_crop_spinbox_max(self):
        """Update max values for crop spinboxes based on video dimensions"""
        if not self.video_original_width or not self.video_original_height:
            return
        
        try:
            # Find and update crop spinboxes
            def update_widget(widget):
                if isinstance(widget, ttk.Spinbox):
                    var = widget.cget('textvariable')
                    if var:
                        var_obj = self.dialog.nametowidget(var) if isinstance(var, str) else var
                        if var_obj == self.crop_x_var:
                            widget.configure(to=self.video_original_width)
                        elif var_obj == self.crop_y_var:
                            widget.configure(to=self.video_original_height)
                        elif hasattr(self, 'crop_width_var') and var_obj == self.crop_width_var:
                            widget.configure(to=self.video_original_width)
                elif isinstance(widget, (ttk.Frame, tk.Frame)):
                    for child in widget.winfo_children():
                        update_widget(child)
            
            # Update all widgets
            for widget in self.dialog.winfo_children():
                update_widget(widget)
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°è£å‰ªæ§ä»¶æœ€å¤§å€¼å¤±è´¥: {e}")



    def try_update_scene_visual_fields(self, current_scene, scene_data):
        # æ˜¾ç¤ºå¯¹æ¯”å¯¹è¯æ¡†ï¼Œè®©ç”¨æˆ·æ¯”è¾ƒå’Œç¼–è¾‘
        updated_data = self._show_scene_comparison_dialog(current_scene, scene_data)
        if updated_data is None:
            return  # ç”¨æˆ·å–æ¶ˆ

        content = scene_data.get("speaking", "")
        if content:
            current_scene["speaking"] = content
        story = scene_data.get("visual", "")
        speaker = scene_data.get("speaker", "")
        if speaker:
            current_scene["speaker"] = speaker
        if story:
            current_scene["visual"] = story
        actions = scene_data.get("actions", "")
        if actions:
            current_scene["actions"] = actions
        #cinematography = scene_data.get("cinematography", "")
        #if cinematography:
        #    # è§„èŒƒåŒ– cinematography å­—æ®µï¼Œç¡®ä¿æ­£ç¡®å¤„ç† JSON å­—ç¬¦ä¸²
        #    current_scene["cinematography"] = self._normalize_json_string_field(cinematography)
        narrator = scene_data.get("narrator", "")
        if narrator:
            current_scene["narrator"] = narrator
        actions = scene_data.get("actions", "")
        if actions:
            current_scene["actions"] = actions
        kernel = scene_data.get("kernel", "")
        if kernel:
            current_scene["kernel"] = kernel

        self.save_scenes_to_json()



    def _show_scene_comparison_dialog(self, current_scene, new_scene_data):
        """æ˜¾ç¤ºåœºæ™¯æ•°æ®å¯¹æ¯”å¯¹è¯æ¡†ï¼Œè®©ç”¨æˆ·æ¯”è¾ƒå’Œç¼–è¾‘å­—æ®µå€¼"""
        import tkinter as tk
        import tkinter.ttk as ttk
        import tkinter.scrolledtext as scrolledtext
        
        # å­—æ®µåç§°æ˜ å°„
        field_labels = {
            "speaking": "å†…å®¹",
            "speaker": "ä¸»ä½“",
            "visual": "ç”»é¢",
            "action": "åŠ¨ä½œ",
            "voiceover": "æ—ç™½"
            #"cinematography": "ç”µå½±æ‘„å½±",
        }
        
        # éœ€è¦å¯¹æ¯”çš„å­—æ®µåˆ—è¡¨
        fields_to_compare = list(field_labels.keys())
        
        # åˆ›å»ºå¯¹è¯æ¡†
        try:
            root = tk._default_root
            if root is None:
                root = tk.Tk()
                root.withdraw()
        except:
            root = tk.Tk()
            root.withdraw()
        
        dialog = tk.Toplevel(root)
        dialog.title("å¯¹æ¯”å’Œç¼–è¾‘åœºæ™¯æ•°æ®")
        dialog.geometry("1200x800")
        dialog.transient(root)
        dialog.grab_set()
        
        # å±…ä¸­æ˜¾ç¤º
        dialog.update_idletasks()
        x = (dialog.winfo_screenwidth() - 1200) // 2
        y = (dialog.winfo_screenheight() - 800) // 2
        dialog.geometry(f"1200x800+{x}+{y}")
        
        # ä¸»æ¡†æ¶
        main_frame = ttk.Frame(dialog, padding=10)
        main_frame.pack(fill=tk.BOTH, expand=True)
        
        # æ ‡é¢˜
        title_label = ttk.Label(main_frame, text=f"æ–°æ—§åœºæ™¯æ•°æ®å¯¹æ¯”",  font=('TkDefaultFont', 11, 'bold'))
        title_label.pack(pady=(0, 10))
        
        # åˆ›å»ºæ»šåŠ¨æ¡†æ¶
        canvas = tk.Canvas(main_frame)
        scrollbar = ttk.Scrollbar(main_frame, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas)
        
        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # å­˜å‚¨ç¼–è¾‘æ§ä»¶å’Œå¤é€‰æ¡†
        comparison_widgets = {}
        
        # ä¸ºæ¯ä¸ªå­—æ®µåˆ›å»ºå¯¹æ¯”è¡Œ
        for field in fields_to_compare:
            field_frame = ttk.LabelFrame(scrollable_frame, text=field_labels[field], padding=5)
            field_frame.pack(fill=tk.X, pady=5, padx=5)
            
            # å¤é€‰æ¡†ï¼šæ˜¯å¦æ›´æ–°æ­¤å­—æ®µ
            checkbox_frame = ttk.Frame(field_frame)
            checkbox_frame.pack(fill=tk.X, pady=(0, 5))
            
            update_var = tk.BooleanVar(value=False)
            checkbox = ttk.Checkbutton(checkbox_frame, text="æ›´æ–°æ­¤å­—æ®µ", variable=update_var)
            checkbox.pack(side=tk.LEFT)
            
            # ä¸¤åˆ—å¸ƒå±€ï¼šå½“å‰å€¼ vs æ–°å€¼
            comparison_frame = ttk.Frame(field_frame)
            comparison_frame.pack(fill=tk.BOTH, expand=True)
            
            # å½“å‰å€¼åˆ—
            current_frame = ttk.Frame(comparison_frame)
            current_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(0, 5))
            ttk.Label(current_frame, text="å½“å‰å€¼:", font=('TkDefaultFont', 9, 'bold')).pack(anchor='w')
            current_text = scrolledtext.ScrolledText(current_frame, wrap=tk.WORD, height=3, width=50)
            current_text.pack(fill=tk.BOTH, expand=True)
            current_value = str(current_scene.get(field, ""))
            current_text.insert('1.0', current_value)
            current_text.config(state=tk.DISABLED)  # å½“å‰å€¼åªè¯»
            
            # æ–°å€¼åˆ—ï¼ˆå¯ç¼–è¾‘ï¼‰
            new_frame = ttk.Frame(comparison_frame)
            new_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(5, 0))
            ttk.Label(new_frame, text="æ–°å€¼ï¼ˆå¯ç¼–è¾‘ï¼‰:", font=('TkDefaultFont', 9, 'bold')).pack(anchor='w')
            new_text = scrolledtext.ScrolledText(new_frame, wrap=tk.WORD, height=3, width=50)
            new_text.pack(fill=tk.BOTH, expand=True)
            new_value = str(new_scene_data.get(field, ""))
            new_text.insert('1.0', new_value)
            
            if not current_value or current_value.strip() == "":
                update_var.set(True)
            else:
                update_var.set(False)
            # ä¿å­˜æ§ä»¶å¼•ç”¨
            comparison_widgets[field] = {
                'update_var': update_var,
                'new_text': new_text
            }
        
        canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        
        # æŒ‰é’®æ¡†æ¶
        button_frame = ttk.Frame(main_frame)
        button_frame.pack(fill=tk.X, pady=(10, 0))
        
        result = [None]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨é—­åŒ…ä¸­ä¿®æ”¹
        
        def on_ok():
            # æ”¶é›†ç”¨æˆ·é€‰æ‹©çš„æ•°æ®
            updated_data = {}
            for field, widgets in comparison_widgets.items():
                if widgets['update_var'].get():
                    # ç”¨æˆ·é€‰æ‹©äº†æ›´æ–°æ­¤å­—æ®µ
                    new_value = widgets['new_text'].get('1.0', tk.END).strip()
                    if new_value:  # åªæ·»åŠ éç©ºå€¼
                        updated_data[field] = new_value
            result[0] = updated_data if updated_data else None
            dialog.destroy()
        
        def on_cancel():
            result[0] = None
            dialog.destroy()
        
        ttk.Button(button_frame, text="å–æ¶ˆ", command=on_cancel).pack(side=tk.RIGHT, padx=5)
        ttk.Button(button_frame, text="ç¡®å®š", command=on_ok).pack(side=tk.RIGHT, padx=5)
        
        # ç­‰å¾…å¯¹è¯æ¡†å…³é—­
        dialog.wait_window()
        
        return result[0]
